@article{Alvin1998,
annote = {Uncertainty when we have different models. They use the same approach as Bernardo, Bayesian model to propagate de unsertainty.},
author = {Alvin, K F and Oberkampf, William L and Diegert, K V and Rutherford, B M},
file = {:Users/rio/Documents/Mendeley Desktop/Alvin et al/Society for Experimental Mechanics, Inc, 16 th International Modal Analysis Conference/Alvin et al. - 1998 - Uncertainty quantification in computational structural dynamics a new paradigm for model validation.pdf:pdf},
journal = {Society for Experimental Mechanics, Inc, 16 th International Modal Analysis Conference.},
keywords = {uncertainty - general},
pages = {1191--1198},
title = {{Uncertainty quantification in computational structural dynamics: a new paradigm for model validation}},
volume = {2},
year = {1998}
}
@article{Arnaut2008,
author = {Arnaut, L. R.},
file = {:Users/rio/Documents/Mendeley Desktop/Arnaut/NPL Technical Report TQE 2, 2nd. ed., sec. 4.1.2.2/Arnaut - 2008 - Measurement uncertainty in reverberation chambers - I. Sample statistics.pdf:pdf},
issn = {1754-2995},
journal = {NPL Technical Report TQE 2, 2nd. ed., sec. 4.1.2.2},
number = {2},
title = {{Measurement uncertainty in reverberation chambers - I. Sample statistics}},
volume = {TQE},
year = {2008}
}
@book{Banks2014,
author = {Banks, HT and Hu, S and Thompson, WC},
booktitle = {CRC Press},
file = {:Users/rio/Documents/Mendeley Desktop/Banks, Hu, Thompson/CRC Press/Banks, Hu, Thompson - 2014 - Modeling and Inverse Problems in the Presence of Uncertainty.pdf:pdf},
isbn = {9781482206432},
title = {{Modeling and Inverse Problems in the Presence of Uncertainty}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=jZA-AwAAQBAJ{\&}oi=fnd{\&}pg=PP1{\&}dq=Modeling+and+Inverse+Problems+in+the+Presence+of+Uncertainty{\&}ots=HsI{\_}aNVlHR{\&}sig=4pbvydoz4JeqqEXXEYEqfl7uTvY},
year = {2014}
}
@book{Bettencourt2012,
author = {Bettencourt, Ricardo and Bulska, Ewa and Godlewska-{\.{z}}y{\l}kiewicz, Beata and Papadakis, Ioannis and Patriarca, Marina and Vassileva, Emilia and Taylor, Philip},
doi = {10.2787/5825},
file = {:Users/rio/Documents/Mendeley Desktop/Bettencourt et al/Unknown/Bettencourt et al. - 2012 - Analytical measurement measurement uncertainty and statistics.pdf:pdf},
isbn = {9789279230714},
title = {{Analytical measurement : measurement uncertainty and statistics}},
year = {2012}
}
@article{Borgonovo2007,
abstract = {Uncertainty in parameters is present in many risk assessment problems and leads to uncertainty in model predictions. In this work, we introduce a global sensitivity indicator which looks at the influence of input uncertainty on the entire output distribution without reference to a specific moment of the output (moment independence) and which can be defined also in the presence of correlations among the parameters. We discuss its mathematical properties and highlight the differences between the present indicator, variance-based uncertainty importance measures and a moment independent sensitivity indicator previously introduced in the literature. Numerical results are discussed with application to the probabilistic risk assessment model on which Iman [A matrix-based approach to uncertainty and sensitivity analysis for fault trees. Risk Anal 1987;7(1):22-33] first introduced uncertainty importance measures. {\textcopyright} 2006 Elsevier Ltd. All rights reserved.},
annote = {They introduce a new global SA parameter to estimate the influence of any parameter in the output. They make a ranking of the influence of the parameters in the output. The method is independent of the correlations of the input parameters.},
author = {Borgonovo, E.},
doi = {10.1016/j.ress.2006.04.015},
file = {:Users/rio/Documents/Mendeley Desktop/Borgonovo/Reliability Engineering and System Safety/Borgonovo - 2007 - A new uncertainty importance measure.pdf:pdf},
isbn = {0951-8320},
issn = {09518320},
journal = {Reliability Engineering and System Safety},
keywords = {Global sensitivity analysis,Importance measures,Probabilistic risk assessment,Uncertainty analysis,Uncertainty importance measures,moment independence},
mendeley-tags = {moment independence},
number = {6},
pages = {771--784},
title = {{A new uncertainty importance measure}},
volume = {92},
year = {2007}
}
@article{Bretthorst1996,
abstract = {Probability theory as logic is founded on three simple desiderata: that degrees of belief should be represented by real numbers, that one should reason consistently, and that the theory should reduce to Aristotelian logic when the truth values of the hypotheses are known. Because this theory represents a probability as a state of knowledge, not a state of nature, hypotheses such as $\backslash$The frequency of oscillation of a sinusoidal signal had value ! when the data were taken," or $\backslash$Model x is a better description of the data than model y" make perfect sense. Problems of the first type are generally thought of as parameter estimation problems, while problems of the second type are thought of as model selection problems. However, in probability theory there is no essential distinction between these two types of problems. They are both solved by application of the sum and product rules of probability theory. Model selection problems are conceptually more difficult, because the models may have different functional forms. Consequently, conceptual difficulties enter the problem that are not present in parameter estimation. This paper is a tutorial on model selection. The conceptual problems that arise in model selection will be illustrated in such a way as to automatically avoid any difficulties. A simple example is worked in detail. This example,(radar target identification) illustrates all of the points of principle that must be faced in more complex model selection problems, including how to handle nuisance parameters, uninformative prior probabilities, and incomplete sets of models.},
author = {Bretthorst, G Larry},
file = {:Users/rio/Documents/Mendeley Desktop/Bretthorst/Maximum Entropy and Bayesian Methods/Bretthorst - 1996 - An Introduction to model selection using probability theory as logic.pdf:pdf},
journal = {Maximum Entropy and Bayesian Methods},
keywords = {Bayes},
pages = {1--42},
title = {{An Introduction to model selection using probability theory as logic}},
year = {1996}
}
@article{Ceylan2016,
author = {Ceylan, Ismail Ilkan and Darwiche, Adnan and Broeck, Guy Van Den},
file = {:Users/rio/Documents/Mendeley Desktop/Ceylan, Darwiche, Broeck/Proc. of KR'16/Ceylan, Darwiche, Broeck - 2016 - Open-World Probabilistic Databases.pdf:pdf},
journal = {Proc.$\backslash$ of KR'16},
title = {{Open-World Probabilistic Databases}},
year = {2016}
}
@article{Citac2000,
abstract = {This Guide gives detailed guidance for the evaluation and expression of uncertainty in quantitative chemical analysis, based on the approach taken in the ISO Guide to the Expression of Uncertainty in Measurement H.2. It is applicable at all levels of accuracy and in all fields - from routine analysis to basic research and to empirical and rational methods (see section 5.3.). Some common areas in which chemical measurements are needed, and in which the principles of this Guide may be applied, are: Quality control and quality assurance in manufacturing industries. Testing for regulatory compliance. Testing utilising an agreed method. Calibration of standards and equipment. Measurements associated with the development and certification of reference materials. Research and development.},
author = {Citac and Eurachem},
doi = {0 948926 15 5},
file = {:Users/rio/Documents/Mendeley Desktop/Citac, Eurachem/English/Citac, Eurachem - 2000 - Quantifying Uncertainty in Analytical Measurement.pdf:pdf},
isbn = {0948926155},
journal = {English},
pages = {126},
title = {{Quantifying Uncertainty in Analytical Measurement}},
url = {http://www.measurementuncertainty.org/mu/QUAM2000-1.pdf},
volume = {2nd},
year = {2000}
}
@article{Dalvi2009,
abstract = {A wide range of applications have recently emerged that need to manage large, imprecise data sets. The reasons for imprecision in data are as diverse as the applications them- selves: in sensor and RFID data, imprecision is due to mea- surement errors [28,66]; in information extraction, impreci- sion comes from the inherent ambiguity in natural-language text [32,40]; and in business intelligence, imprecision is used to reduce the cost of data cleaning [12]. In some applications, such as privacy, it is a requirement that the data be less pre- cise. For example, imprecision is purposely inserted to hide sensitive attributes of individuals so that the data may be published [29,55,62]. Imprecise data has no place in tradi- tional, precise database applications like payroll and inven- tory, and so, current database management systems are not prepared to deal with it. In contrast, these newly emerging applications offer value precisely because they query, search, and aggregate large volumes of imprecise data to find the“di- amonds in the dirt”. This wide-variety of applications points to the need for generic tools to manage imprecise data. In this paper, we survey the state of the art techniques to han- dle imprecise data which models imprecision as probabilistic data [4,8,11,14,21,28,45,51,71]. A probabilistic database management system, or Prob- DMS, is a system that stores large volumes of probabilis- tic data and supports complex queries. A ProbDMS may also need to perform some additional tasks, such as updates or recovery, but these do not differ from those in conven- tional database management systems and will not be dis- cussed here. The major challenge in a ProbDMS is that it needs both to scale to large data volumes, a core com- petence of database management systems, and to do prob- abilistic inference, which is a problem studied in AI. While many scalable data management systems exists, probabilis- tic inference is in general a hard problem [68], and current systems do not scale to the same extent as datamanagement systems do. To address this challenge, researchers have fo- cused on the specific nature of relational probabilistic data, and exploited the special form of probabilistic inference that occurs during query evaluation. A number of such results have emerged recently: lineage-based representations [11], safe plans [18], algorithms for top-k queries [63,82], and rep- resentations of views over probabilistic data [65,67]. What is common to all these results is that they apply and extend well known concepts that are fundamental to data manage- ment, such as the separation of query and data when analyz- ing complexity [75], incomplete databases [44], the threshold algorithm [31], and the use of materialized views to answer queries [42, 74]. In this paper, we briefly survey the key concepts in probabilistic database systems, and explain the intellectual roots of these concepts in data management.},
author = {Dalvi, Nilesh and R{\'{e}}, Christopher and Suciu, Dan},
doi = {10.1145/1538788.1538810},
file = {:Users/rio/Documents/Mendeley Desktop/Dalvi, R{\'{e}}, Suciu/Communications of the ACM/Dalvi, R{\'{e}}, Suciu - 2009 - Probabilistic databases diamonds in the dirt.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
number = {7},
pages = {86--94},
title = {{Probabilistic databases: diamonds in the dirt}},
url = {http://portal.acm.org/citation.cfm?doid=1538788.1538810$\backslash$nhttp://dl.acm.org/citation.cfm?id=1538810},
volume = {52},
year = {2009}
}
@article{Deshpande,
author = {Deshpande, Amol},
file = {:Users/rio/Documents/Mendeley Desktop/Deshpande/Unknown/Deshpande - Unknown - Probabilistic Databases.pdf:pdf},
keywords = {attribute-level uncertainty,cleaning of data,complexity class,conditional table,graphical model,inference algorithms,model counting problem,p,pos-,probabilistic conditional table},
title = {{Probabilistic Databases}}
}
@article{Leader2005,
abstract = {JASON.Quantificationsofmarginsanduncertainties(QMU).JSR-04-3330. McLean, VA:TheMitreCorporation;2005.},
author = {Eardley},
file = {:Users/rio/Documents/Mendeley Desktop/Eardley/JASON -The Mitre Corporation JASON report JSR-04-330/Eardley - 2005 - Quantifications of Margins and Uncertainties.pdf:pdf},
isbn = {9780309128537},
journal = {JASON -The Mitre Corporation JASON report JSR-04-330},
title = {{Quantifications of Margins and Uncertainties}},
year = {2005}
}
@article{Farrell2015a,
author = {Farrell, Kathryn Anne},
file = {:Users/rio/Documents/Mendeley Desktop/Farrell/Unknown/FARRELL-DISSERTATION-2015.pdf:pdf},
title = {{Selection , Calibration , and Validation of Coarse-Grained Models of Atomistic Systems}},
year = {2015}
}
@article{Farrell2015,
abstract = {A general adaptive modeling algorithm for selection and validation of coarse-grained models of atomistic systems is presented. A Bayesian framework is developed to address uncertainties in parameters, data, and model selection. Algorithms for computing output sensitivities to parameter variances, model evidence and posterior model plausibilities for given data, and for computing what are referred to as Occam Categories in reference to a rough measure of model simplicity, make up components of the overall approach. Computational results are provided for representative applications.},
author = {Farrell, Kathryn and Oden, J. Tinsley and Faghihi, Danial},
doi = {10.1016/j.jcp.2015.03.071},
file = {:Users/rio/Documents/Mendeley Desktop/Farrell, Oden, Faghihi/Journal of Computational Physics/oden-opal.pdf:pdf},
issn = {10902716},
journal = {Journal of Computational Physics},
keywords = {Bayesian inference,Coarse graining models,Model plausibility,Model validation,Output sensitivities},
pages = {189--208},
publisher = {Elsevier Inc.},
title = {{A Bayesian framework for adaptive selection, calibration, and validation of coarse-grained models of atomistic systems}},
url = {http://dx.doi.org/10.1016/j.jcp.2015.03.071},
volume = {295},
year = {2015}
}
@article{Ferretti2016,
abstract = {The majority of published sensitivity analyses (SAs) are either local or one factor-at-a-time (OAT) analyses, relying on unjustified assumptions of model linearity and additivity. Global approaches to sensitivity analyses (GSA) which would obviate these shortcomings, are applied by a minority of researchers. By reviewing the academic literature on SA, we here present a bibliometric analysis of the trends of different SA practices in last decade. The review has been conducted both on some top ranking journals (Nature and Science) and through an extended analysis in the Elsevier's Scopus database of scientific publications. After correcting for the global growth in publications, the amount of papers performing a generic SA has notably increased over the last decade. Even if OAT is still the most largely used technique in SA, there is a clear increase in the use of GSA with preference respectively for regression and variance-based techniques. Even after adjusting for the growth of publications in the sole modelling field, to which SA and GSA normally apply, the trend is confirmed. Data about regions of origin and discipline are also briefly discussed. The results above are confirmed when zooming on the sole articles published in chemical modelling, a field historically proficient in the use of SA methods.},
annote = {Solo es una revision bibliografica, habla del aumento del uso de los metodos de analisis de sensibilidad en los ultimos anos. De como se usa mucho el analisis de un solo parametro a la vez, lo que no funciona correctamente.},
author = {Ferretti, Federico and Saltelli, Andrea and Tarantola, Stefano},
doi = {10.1016/j.scitotenv.2016.02.133},
file = {:Users/rio/Documents/Mendeley Desktop/Ferretti, Saltelli, Tarantola/Science of the Total Environment/Ferretti, Saltelli, Tarantola - 2016 - Trends in Sensitivity Analysis practice in the last decade.pdf:pdf},
issn = {00489697},
journal = {Science of the Total Environment},
keywords = {Bibliometric analysis,Chemical modelling,Global sensitivity analysis,Sensitivity analysis,bibliometric analysis,global sensitivity analysis,sensitivity analysis},
pages = {2--6},
pmid = {26934843},
publisher = {Elsevier B.V.},
title = {{Trends in Sensitivity Analysis practice in the last decade}},
url = {http://dx.doi.org/10.1016/j.scitotenv.2016.02.133},
volume = {21027},
year = {2016}
}
@article{Gaganis2001,
author = {Gaganis, Petros and Smith, Leslie},
file = {:Users/rio/Documents/Mendeley Desktop/Gaganis, Smith/Water Resources/Gaganis, Smith - 2001 - A Bayesian approach to the quantification of the effect of model error on the predictions of groundwater models.pdf:pdf},
journal = {Water Resources},
keywords = {doi:10.102,http://dx.doi.org/10.1029/2000WR000001},
number = {9},
pages = {2309 --2322},
title = {{A Bayesian approach to the quantification of the effect of model error on the predictions of groundwater models}},
volume = {37},
year = {2001}
}
@book{Geris2016,
author = {Geris, Liesbet and Gomez-Cabrero, David},
doi = {10.1007/978-3-319-21296-8},
file = {:Users/rio/Documents/Mendeley Desktop/Geris, Gomez-Cabrero/Unknown/Geris, Gomez-Cabrero - 2016 - Uncertainty in Biology.pdf:pdf},
isbn = {978-3-319-21295-1},
pages = {471},
title = {{Uncertainty in Biology}},
url = {http://link.springer.com/10.1007/978-3-319-21296-8},
volume = {17},
year = {2016}
}
@article{Ghanem2013,
author = {Ghanem, Roger},
file = {:Users/rio/Documents/Mendeley Desktop/Ghanem/Unknown/Ghanem - 2013 - Uncertainty Quantification.pdf:pdf},
pages = {1--42},
title = {{Uncertainty Quantification}},
year = {2013}
}
@article{GharibShirangi2014,
abstract = {For large-scale history matching problems, applying the Gauss–Newton (GN) or the Levenberg–Marquardt (LM) algorithm is computationally expensive. However, these algorithms can be efficiently applied with parameterization based on a truncated singular value decomposition (SVD) of a dimensionless sensitivity matrix, where a truncated SVD is computed by using the Lanczos method. The SVD parameterization algorithm has been previously combined with randomized maximum likelihood (RML) to simultaneously generate multiple realizations of the reservoir model. The resulting algorithm, called SVD-EnRML, has been applied for simulation of permeability fields of 2D synthetic reservoirs. In this work, the SVD-EnRML algorithm is extended for the simulation of both porosity and permeability fields of 3D reservoirs. In the proposed extension, a dimensionless sensitivity matrix is defined for each set of correlated model parameters. A limitation of the original algorithm is due to the fact that a square root of the covariance matrix is required as a transformation from the original space to a dimensionless space. In this work, this limitation is resolved by introducing ensemble-based regularization based on utilizing an ensemble of unconditional realizations of the reservoir model. Although the proposed extension fits well within the original algorithm, a modified SVD-EnRML algorithm is introduced to mainly improve the computational efficiency. Computational results, composed of two different examples, show that the algorithm can be efficiently applied for the simulation of rock property fields and performance predictions of 3D reservoirs.},
author = {{Gharib Shirangi}, Mehrdad},
doi = {10.1016/j.petrol.2013.11.025},
file = {:Users/rio/Documents/Mendeley Desktop/Gharib Shirangi/Journal of Petroleum Science and Engineering/Gharib Shirangi - 2014 - History matching production data and uncertainty assessment with an efficient TSVD parameterization algorithm.pdf:pdf},
issn = {09204105},
journal = {Journal of Petroleum Science and Engineering},
pages = {54--71},
title = {{History matching production data and uncertainty assessment with an efficient TSVD parameterization algorithm}},
volume = {113},
year = {2014}
}
@article{Golias2012,
author = {Golia{\v{s}}, Marcel and Palen{\v{c}}{\'{a}}r, Rudolf},
file = {:Users/rio/Documents/Mendeley Desktop/Golia{\v{s}}, Palen{\v{c}}{\'{a}}r/Acta Polytechnica/1590-1422-1-PB.pdf:pdf},
issn = {12102709},
journal = {Acta Polytechnica},
keywords = {Correlation,Monte carlo method,Uncertainty of measurement},
number = {4},
pages = {57--61},
title = {{Determination of uncertainties for correlated input quantities by the Monte Carlo method}},
volume = {52},
year = {2012}
}
@inproceedings{Goncalves2013,
address = {New York, New York, USA},
author = {Gon{\c{c}}alves, Bernardo and Porto, Fabio},
booktitle = {Proceedings of the 25th International Conference on Scientific and Statistical Database Management},
doi = {10.1145/2484838.2484861},
file = {:Users/rio/Documents/Mendeley Desktop/Gon{\c{c}}alves, Porto/Proceedings of the 25th International Conference on Scientific and Statistical Database Management/Gon{\c{c}}alves, Porto - 2013 - Research lattices.pdf:pdf},
isbn = {978-1-4503-1921-8},
keywords = {large-scale science,lattice theory,research progress,scientific databases,scientific hypothesis},
pages = {41:1----41:4},
publisher = {ACM Press},
title = {{Research Lattices: Towards a Scientific Hypothesis Data Model}},
url = {http://dl.acm.org/citation.cfm?doid=2484838.2484861 http://doi.acm.org/10.1145/2484838.2484861},
year = {2013}
}
@article{Guerra2016,
author = {Guerra, Gabriel M. and Zio, Souleymane and Camata, Jose J. and Dias, Jonas and Elias, Renato N. and Mattoso, Marta and {B. Paraizo}, Paulo L. and {G. A. Coutinho}, Alvaro L. and Rochinha, Fernando A.},
doi = {10.1007/s10596-016-9563-6},
file = {:Users/rio/Documents/Mendeley Desktop/Guerra et al/Computational Geosciences/Guerra et al. - 2016 - Uncertainty quantification in numerical simulation of particle-laden flows.pdf:pdf},
issn = {1420-0597},
journal = {Computational Geosciences},
number = {1},
pages = {265--281},
title = {{Uncertainty quantification in numerical simulation of particle-laden flows}},
url = {http://link.springer.com/10.1007/s10596-016-9563-6},
volume = {20},
year = {2016}
}
@article{Hartmann2008,
abstract = {For the simulation of structural collapse using controlled explosives, the quantification of structural parameters has to be accomplished on the basis of only few data, which may additionally be characterized by vagueness, e.g. due to uncertain measurements or changing reproduction conditions. To ensure a reliable prediction of a structural collapse, next to a close to reality simulation of the complex dynamic process, this uncertainty has to be taken into account. With regard to very high computation associated with the simulation of collapses of real world structures based on conventional finite element models, this paper addresses an efficient approach for the simulation of structural collapse based on multibody models, that simultaneously allow for the investigation of uncertainty. ?? 2008 Elsevier Ltd. All rights reserved.},
author = {Hartmann, Dietrich and Breidt, Michael and Nguyen, van Vinh and Stangenberg, Friedhelm and H??hler, Sebastian and Schweizerhof, Karl and Mattern, Steffen and Blankenhorn, Gunther and M??ller, Bernd and Liebscher, Martin},
doi = {10.1016/j.compstruc.2008.03.004},
file = {:Users/rio/Documents/Mendeley Desktop/Hartmann et al/Computers and Structures/Hartmann et al. - 2008 - Structural collapse simulation under consideration of uncertainty-Fundamental concept and results.pdf:pdf},
issn = {00457949},
journal = {Computers and Structures},
keywords = {Demolition,Explosives,Fuzziness,Multi-level simulation,Multibody dynamics},
number = {21-22},
pages = {2064--2078},
publisher = {Elsevier Ltd},
title = {{Structural collapse simulation under consideration of uncertainty-Fundamental concept and results}},
url = {http://dx.doi.org/10.1016/j.compstruc.2008.03.004},
volume = {86},
year = {2008}
}
@article{Helton2009,
abstract = {In 2001, the National Nuclear Security Administration of the U.S. Department of Energy in conjunction with the national security laboratories (i.e, Los Alamos National Laboratory, Lawrence Livermore National Laboratory and Sandia National Laboratories) initiated development of a process designated Quantification of Margins and Uncer- tainty (QMU) for the use of risk assessment methodologies in the certification of the reliability and safety of the nation's nuclear weapons stockpile. This presentation discusses and illustrates the conceptual and computational basis of QMU in analyses that use computational models to predict the behavior of complex systems. Topics consid- ered include (i) the role of aleatory and epistemic uncertainty in QMU, (ii) the representation of uncertainty with probability, (iii) the probabilistic representation of uncertainty in QMU analyses involving only epistemic uncer- tainty, (iv) the probabilistic representation of uncertainty in QMU analyses involving aleatory and epistemic uncer- tainty, (v) procedures for sampling-based uncertainty and sensitivity analysis, (vi) the representation of uncertainty with alternatives to probability such as interval analysis, possibility theory and evidence theory, (vii) the representa- tion of uncertainty with alternatives to probability in QMU analyses involving only epistemic uncertainty, and (viii) the representation of uncertainty with alternatives to probability in QMU analyses involving aleatory and epistemic uncertainty. Concepts and computational procedures are illustrated with both notional examples and examples from reactor safety and radioactive waste disposal.},
annote = {Explica bastante bien todo el proceso, solo que habla de insertezas parametricas basicamente. Habla de las formas alternaticas de cuantificar la inserteza, como teoria de las posibilidades, etc.
Es algo viejo por lo que hay que buscar referencias mas recientes.},
author = {Helton, JC},
file = {:Users/rio/Documents/Mendeley Desktop/Helton/Unknown/Helton - 2009 - Conceptual and computational basis for the quantification of margins and uncertainty.pdf:pdf},
keywords = {Aleatory uncertainty,Epistemic uncertainty,Performance assessment,Quantification of margins and uncertainty,Risk assessment,Sensitivity analysis,Uncertainty analysis},
number = {June},
title = {{Conceptual and computational basis for the quantification of margins and uncertainty.}},
url = {http://www.osti.gov/energycitations/product.biblio.jsp?osti{\_}id=958189},
year = {2009}
}
@article{Helton2010,
abstract = {Procedures are described for the representation of results in analyses that involve both aleatory uncertainty and epistemic uncertainty, with aleatory uncertainty deriving from an inherent randomness in the behaviour of the system under study and epistemic uncertainty deriving from a lack of knowledge about the appropriate values to use for quantities that are assumed to have fixed but poorly known values in the context of a specific study. Aleatory uncertainty is usually represented with probability and leads to cumulative distribution functions (CDFs) or complementary CDFs (CCDFs) for analysis results of interest. Several mathematical structures are available for the representation of epistemic uncertainty, including interval analysis, possibility theory, evidence theory and probability theory. In the presence of epistemic uncertainty, there is not a single CDF or CCDF for a given analysis result. Rather, there is a family of CDFs and a corresponding family of CCDFs that derive from epistemic uncertainty and have an uncertainty structure that derives from the particular uncertainty structure (e.g. interval analysis, possibility theory, evidence theory or probability theory) used to represent epistemic uncertainty. Graphical formats for the representation of epistemic uncertainty in families of CDFs and CCDFs are investigated and presented for the indicated characterisations of epistemic uncertainty.$\backslash$nProcedures are described for the representation of results in analyses that involve both aleatory uncertainty and epistemic uncertainty, with aleatory uncertainty deriving from an inherent randomness in the behaviour of the system under study and epistemic uncertainty deriving from a lack of knowledge about the appropriate values to use for quantities that are assumed to have fixed but poorly known values in the context of a specific study. Aleatory uncertainty is usually represented with probability and leads to cumulative distribution functions (CDFs) or complementary CDFs (CCDFs) for analysis results of interest. Several mathematical structures are available for the representation of epistemic uncertainty, including interval analysis, possibility theory, evidence theory and probability theory. In the presence of epistemic uncertainty, there is not a single CDF or CCDF for a given analysis result. Rather, there is a family of CDFs and a corresponding family of CCDFs that derive from epistemic uncertainty and have an uncertainty structure that derives from the particular uncertainty structure (e.g. interval analysis, possibility theory, evidence theory or probability theory) used to represent epistemic uncertainty. Graphical formats for the representation of epistemic uncertainty in families of CDFs and CCDFs are investigated and presented for the indicated characterisations of epistemic uncertainty.},
author = {Helton, Jon C. and Johnson, Jay D. and Oberkampf, William L. and Sallaberry, C{\'{e}}dric J.},
doi = {10.1080/03081079.2010.486664},
file = {:Users/rio/Documents/Mendeley Desktop/Helton et al/International Journal of General Systems/Helton et al. - 2010 - Representation of analysis results involving aleatory and epistemic uncertainty.pdf:pdf},
issn = {0308-1079},
journal = {International Journal of General Systems},
keywords = {aleatory uncertainty,epistemic uncertainty,evidence theory,interval analysis,possibility theory,probability theory},
number = {6},
pages = {605--646},
publisher = {Taylor {\&} Francis Group},
title = {{Representation of analysis results involving aleatory and epistemic uncertainty}},
volume = {39},
year = {2010}
}
@article{Hoare2008,
abstract = {SaSAT (Sampling and Sensitivity Analysis Tools) is a user-friendly software package for applying uncertainty and sensitivity analyses to mathematical and computational models of arbitrary complexity and context. The toolbox is built in Matlab, a numerical mathematical software package, and utilises algorithms contained in the Matlab Statistics Toolbox. However, Matlab is not required to use SaSAT as the software package is provided as an executable file with all the necessary supplementary files. The SaSAT package is also designed to work seamlessly with Microsoft Excel but no functionality is forfeited if that software is not available. A comprehensive suite of tools is provided to enable the following tasks to be easily performed: efficient and equitable sampling of parameter space by various methodologies; calculation of correlation coefficients; regression analysis; factor prioritisation; and graphical output of results, including response surfaces, tornado plots, and scatterplots. Use of SaSAT is exemplified by application to a simple epidemic model. To our knowledge, a number of the methods available in SaSAT for performing sensitivity analyses have not previously been used in epidemiological modelling and their usefulness in this context is demonstrated.},
author = {Hoare, Alexander and Regan, David G and Wilson, David P},
doi = {10.1186/1742-4682-5-4},
file = {:Users/rio/Documents/Mendeley Desktop/Hoare, Regan, Wilson/Theoretical biology {\&} medical modelling/Hoare, Regan, Wilson - 2008 - Sampling and sensitivity analyses tools (SaSAT) for computational modelling.pdf:pdf},
isbn = {1742-4682 (Electronic)$\backslash$n1742-4682 (Linking)},
issn = {1742-4682},
journal = {Theoretical biology {\&} medical modelling},
pages = {4},
pmid = {18304361},
title = {{Sampling and sensitivity analyses tools (SaSAT) for computational modelling.}},
volume = {5},
year = {2008}
}
@article{Johnstone2015,
abstract = {Cardiac electrophysiology models have been developed for over 50. years, and now include detailed descriptions of individual ion currents and sub-cellular calcium handling. It is commonly accepted that there are many uncertainties in these systems, with quantities such as ion channel kinetics or expression levels being difficult to measure or variable between samples. Until recently, the original approach of describing model parameters using single values has been retained, and consequently the majority of mathematical models in use today provide point predictions, with no associated uncertainty.In recent years, statistical techniques have been developed and applied in many scientific areas to capture uncertainties in the quantities that determine model behaviour, and to provide a distribution of predictions which accounts for this uncertainty. In this paper we discuss this concept, which is termed uncertainty quantification, and consider how it might be applied to cardiac electrophysiology models.We present two case studies in which probability distributions, instead of individual numbers, are inferred from data to describe quantities such as maximal current densities. Then we show how these probabilistic representations of model parameters enable probabilities to be placed on predicted behaviours. We demonstrate how changes in these probability distributions across data sets offer insight into which currents cause beat-to-beat variability in canine APs. We conclude with a discussion of the challenges that this approach entails, and how it provides opportunities to improve our understanding of electrophysiology.},
author = {Johnstone, Ross H. and Chang, Eugene T Y and Bardenet, R??mi and de Boer, Teun P. and Gavaghan, David J. and Pathmanathan, Pras and Clayton, Richard H. and Mirams, Gary R.},
doi = {10.1016/j.yjmcc.2015.11.018},
file = {:Users/rio/Documents/Mendeley Desktop/Johnstone et al/Journal of Molecular and Cellular Cardiology/Johnstone et al. - 2015 - Uncertainty and variability in models of the cardiac action potential Can we build trustworthy models.pdf:pdf},
issn = {10958584},
journal = {Journal of Molecular and Cellular Cardiology},
keywords = {Cardiac electrophysiology,Mathematical model,Probability,Uncertainty quantification},
pages = {49--62},
pmid = {26611884},
publisher = {The Authors},
title = {{Uncertainty and variability in models of the cardiac action potential: Can we build trustworthy models?}},
url = {http://dx.doi.org/10.1016/j.yjmcc.2015.11.018},
volume = {96},
year = {2015}
}
@article{Kennedy2001,
author = {Kennedy, Marc C. and O'Hagan, Anthony},
doi = {10.1111/1467-9868.00294},
file = {:Users/rio/Documents/Mendeley Desktop/Kennedy, O'Hagan/Journal of the Royal Statistical Society Series B (Statistical Methodology)/Kennedy, O'Hagan - 2001 - Bayesian calibration of computer models.pdf:pdf},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {Calibration,Computer experiments,Deterministic models,Gaussian process,Interpolation,Model inadequacy,Sensitivity analysis,Uncertainty analysis},
month = {aug},
number = {3},
pages = {425--464},
publisher = {Blackwell Publishers Ltd.},
title = {{Bayesian calibration of computer models}},
url = {http://doi.wiley.com/10.1111/1467-9868.00294},
volume = {63},
year = {2001}
}
@article{Kidane2012,
abstract = {This work is concerned with establishing the feasibility of a data-on-demand (DoD) uncertainty quantification (UQ) protocol based on concentration-of-measure inequalities. Specific aims are to establish the feasibility of the protocol and its basic properties, including the tightness of the predictions afforded by the protocol. The assessment is based on an application to terminal ballistics and a specific system configuration consisting of 6061-T6 aluminum plates struck by spherical S-2 tool steel projectiles at ballistic impact speeds. The systems inputs are the plate thickness and impact velocity and the perforation area is chosen as the sole performance measure of the system. The objective of the UQ analysis is to certify the lethality of the projectile, i.e., that the projectile perforates the plate with high probability over a prespecified range of impact velocities and plate thicknesses. The net outcome of the UQ analysis is an M/U ratio, or confidence factor, of 2.93, indicative of a small probability of no perforation of the plate over its entire operating range. The high-confidence ({\textgreater}99.9{\%}) in the successful operation of the system afforded the analysis and the small number of tests (40) required for the determination of the modeling-error diameter, establishes the feasibility of the DoD UQ protocol as a rigorous yet practical approach for model-based certification of complex systems. ?? 2011 Elsevier Ltd. All rights reserved.},
author = {Kidane, A. and Lashgari, A. and Li, B. and McKerns, M. and Ortiz, M. and Owhadi, H. and Ravichandran, G. and Stalzer, M. and Sullivan, T. J.},
doi = {10.1016/j.jmps.2011.12.001},
file = {:Users/rio/Documents/Mendeley Desktop/Kidane et al/Journal of the Mechanics and Physics of Solids/Kidane et al. - 2012 - Rigorous model-based uncertainty quantification with application to terminal ballistics, part I Systems with cont.pdf:pdf},
isbn = {0022-5096},
issn = {00225096},
journal = {Journal of the Mechanics and Physics of Solids},
keywords = {Certification,Concentration of measure,Terminal ballistics,Uncertainty quantification},
number = {5},
pages = {983--1001},
title = {{Rigorous model-based uncertainty quantification with application to terminal ballistics, part I: Systems with controllable inputs and small scatter}},
volume = {60},
year = {2012}
}
@article{Kiureghian2009,
abstract = {The sources and characters of uncertainties in engineering modeling for risk and reliability analyses are discussed. While many sources of uncertainty may exist, they are generally categorized as either aleatory or epistemic. Uncertainties are characterized as epistemic, if the modeler sees a possibility to reduce them by gathering more data or by refining models. Uncertainties are categorized as aleatory if the modeler does not foresee the possibility of reducing them. From a pragmatic standpoint, it is useful to thus categorize the uncertainties within a model, since it then becomes clear as to which uncertainties have the potential of being reduced. More importantly, epistemic uncertainties may introduce dependence among random events, which may not be properly noted if the character of uncertainties is not correctly modeled. Influences of the two types of uncertainties in reliability assessment, codified design, performance-based engineering and risk-based decision-making are discussed. Two simple examples demonstrate the influence of statistical dependence arising from epistemic uncertainties on systems and time-variant reliability problems. ?? 2008 Elsevier Ltd. All rights reserved.},
author = {Kiureghian, Armen Der and Ditlevsen, Ove},
doi = {10.1016/j.strusafe.2008.06.020},
file = {:Users/rio/Documents/Mendeley Desktop/Kiureghian, Ditlevsen/Structural Safety/Kiureghian, Ditlevsen - 2009 - Aleatory or epistemic Does it matter.pdf:pdf},
isbn = {0167-4730},
issn = {01674730},
journal = {Structural Safety},
keywords = {Aleatory,Epistemic,Ergodicity,Parameter uncertainty,Predictive models,Probability distribution choice,Statistical dependence,Systems,Time-variant reliability,Uncertainty},
month = {mar},
number = {2},
pages = {105--112},
title = {{Aleatory or epistemic? Does it matter?}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167473008000556},
volume = {31},
year = {2009}
}
@article{Kumar2015d,
abstract = {Advanced analytics is a booming area in both industry and academia. Several projects aim to implement ML algorithms efficiently. But three key challenging and iterative practical tasks in using ML – feature engi-neering, algorithm selection, and parameter tuning, collectively called model selection – have largely been overlooked by the data management community even though these are often the most time-consuming tasks for analysts. To make the iterative process of model se-lection easier and faster, we envision a unifying abstract framework that acts a basis for a new class of analytics systems that we call model selection management sys-tems (MSMS). We discuss how time-tested ideas from database research offer new avenues to improve model selection, and outline how MSMS are a new frontier for interesting and impactful data management research.},
author = {Kumar, Arun and Mccann, Robert and Naughton, Jeffrey and Patel, Jignesh M.},
doi = {10.1145/2935694.2935698},
file = {:Users/rio/Documents/Mendeley Desktop/Kumar et al/ACM SIGMOD Record/Kumar et al. - 2016 - Model Selection Management Systems(2).pdf:pdf},
issn = {0163-5808},
journal = {ACM SIGMOD Record},
keywords = {Feature Engineering,Iterative Model Selection,Performance Optimization,Provenance for Machine Learning,Usability of Machine Learning},
month = {may},
number = {4},
pages = {17--22},
publisher = {ACM},
title = {{Model Selection Management Systems: The Next Frontier of Advanced Analytics}},
url = {http://dl.acm.org/citation.cfm?doid=2935694.2935698 http://pages.cs.wisc.edu/{~}arun/vision/},
volume = {44},
year = {2015}
}
@article{Lee2009,
abstract = {A wide variety of uncertainty propagation methods exist in literature; however, there is a lack of good understanding of their relative merits. In this paper, a comparative study on the performances of several representative uncertainty propagation methods, including a few newly developed methods that have received growing attention, is performed. The full factorial numerical integration, the univariate dimension reduction method, and the polynomial chaos expansion method are implemented and applied to several test problems. They are tested under different settings of the performance nonlinearity, distribution types of input random variables, and the magnitude of input uncertainty. The performances of those methods are compared in moment estimation, tail probability calculation, and the probability density function construction, corresponding to a wide variety of scenarios of design under uncertainty, such as robust design, and reliability-based design optimization. The insights gained are expected to direct designers for choosing the most applicable uncertainty propagation technique in design under uncertainty.},
author = {Lee, S. H. and Chen, W.},
doi = {10.1007/s00158-008-0234-7},
file = {:Users/rio/Documents/Mendeley Desktop/Lee, Chen/Structural and Multidisciplinary Optimization/Lee, Chen - 2009 - A comparative study of uncertainty propagation methods for black-box-type problems.pdf:pdf},
isbn = {1615-147X},
issn = {1615147X},
journal = {Structural and Multidisciplinary Optimization},
keywords = {Comparative study,Design under uncertainty,Dimension reduction method,Full factorial numerical integration,Polynomial chaos expansion,Uncertainty propagation},
number = {3},
pages = {239--253},
title = {{A comparative study of uncertainty propagation methods for black-box-type problems}},
volume = {37},
year = {2009}
}
@book{Loucks2005,
abstract = {The usefulness of any model depends in part on the accuracy and reliability of its output. Yet, because all models are imperfect abstractions of reality, and because precise input data are rarely if ever available, all output values are subject to imprecision. Input data errors and modelling uncertainties are not independent of each other – they can interact in various ways. The end result is imprecision and uncertainty associated with model output. This chapter focuses on ways of identifying, quantifying, and communicating the uncertainties in model outputs.},
author = {Loucks, Daniel P. and van Beek, Eelco and Stedinger, Jery R. and Dijkman, Jozef P.M. and Villars, Monique T.},
booktitle = {Water Resources Systems Planning and Management: An Introduction to Methods, Models and Applications},
doi = {ISBN: 92-3-103998-9},
file = {:Users/rio/Documents/Mendeley Desktop/Loucks et al/Water Resources Systems Planning and Management An Introduction to Methods, Models and Applications/Loucks et al. - 2005 - 9 Model Sensitivity and Uncertainty Analysis.pdf:pdf},
isbn = {9231039989},
pages = {254--290},
title = {{9 Model Sensitivity and Uncertainty Analysis}},
url = {https://www.utwente.nl/ctw/wem/education/afstuderen/Loucks{\_}VanBeek/09{\_}chapter09.pdf},
year = {2005}
}
@book{Marino2009,
abstract = {Accuracy of results from mathematical and computer models of biological systems is often complicated by the presence of uncertainties in experimental data that are used to estimate parameter values. Current mathematical modeling approaches typically use either single-parameter or local sensitivity analyses. However, these methods do not accurately assess uncertainty and sensitivity in the system as, by default they hold all other parameters fixed at baseline values. Using techniques described within we demonstrate how a multi-dimensional parameter space can be studied globally so all uncertainties can be identified. Further, uncertainty and sensitivity analysis techniques can help to identify and ultimately control uncertainties. In this work we develop methods for applying existing analytical tools to perform analyses on a variety of mathematical and computer models. We compare two specific types of global sensitivity analysis indexes that have proven to be among the most robust and efficient. Through familiar and new examples of mathematical and computer models, we provide a complete methodology for performing these analyses, both in deterministic and stochastic settings, and propose novel techniques to handle problems encountered during this type of analyses.},
author = {Marino, Simeone and Hogue, Ian B and Ray, Christian J and Kirschner, Denise E},
booktitle = {Journal of Theoretical Biology},
doi = {10.1016/j.jtbi.2008.04.011.A},
file = {:Users/rio/Documents/Mendeley Desktop/Marino et al/Journal of Theoretical Biology/Marino et al. - 2009 - A Methodology for Performing Global Ucertainty and Sensitivity Analysis in Systems Biology.pdf:pdf},
isbn = {7346477723},
issn = {1095-8541},
keywords = {abm,agent-based model,aleatory uncertainty,amplitude sensitivity test,efast,epistemic uncertainty,extended fourier,latin hypercube sampling,lhs,methods,monte carlo,partial rank correlation coefficient,prcc,sensitivity index},
number = {1},
pages = {178--196},
pmid = {18572196},
title = {{A Methodology for Performing Global Ucertainty and Sensitivity Analysis in Systems Biology}},
volume = {254},
year = {2009}
}
@incollection{Matthies2007,
address = {Dordrecht},
author = {Matthies, Hermann G.},
booktitle = {Extreme Man-Made and Natural Hazards in Dynamics of Structures},
doi = {10.1007/978-1-4020-5656-7_4},
file = {:Users/rio/Documents/Mendeley Desktop/Matthies/Extreme Man-Made and Natural Hazards in Dynamics of Structures/Matthies - 2007 - QUANTIFYING UNCERTAINTY MODERN COMPUTATIONAL REPRESENTATION OF PROBABILITY AND APPLICATIONS.pdf:pdf},
pages = {105--135},
publisher = {Springer Netherlands},
title = {{QUANTIFYING UNCERTAINTY: MODERN COMPUTATIONAL REPRESENTATION OF PROBABILITY AND APPLICATIONS}},
url = {http://link.springer.com/10.1007/978-1-4020-5656-7{\_}4},
year = {2007}
}
@phdthesis{Melorose2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Melorose, J. and Perroy, R. and Careas, S.},
booktitle = {Statewide Agricultural Land Use Baseline 2015},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:Users/rio/Documents/Mendeley Desktop/Melorose, Perroy, Careas/Statewide Agricultural Land Use Baseline 2015/Melorose, Perroy, Careas - 2015 - A PROBABILISTIC FRAMEWORK FOR UNCERTAINTY QUANTIFICATION IN LARGE-SCALE SIMULATIONS APPLICATION IN SEI.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
pmid = {25246403},
title = {{A PROBABILISTIC FRAMEWORK FOR UNCERTAINTY QUANTIFICATION IN LARGE-SCALE SIMULATIONS: APPLICATION IN SEISMIC IMAGING}},
volume = {1},
year = {2015}
}
@article{Mullins2016,
abstract = {This paper investigates model validation under a variety of different data scenarios and clarifies how different validation metrics may be appropriate for different scenarios. In the presence of multiple uncertainty sources, model validation metrics that compare the distributions of model prediction and observation are considered. Both ensemble validation and point-by-point approaches are discussed, and it is shown how applying the model reliability metric point-by-point enables the separation of contributions from aleatory and epistemic uncertainty sources. After individual validation assessments are made at different input conditions, it may be desirable to obtain an overall measure of model validity across the entire domain. This paper proposes an integration approach that assigns weights to the validation results according to the relevance of each validation test condition to the overall intended use of the model in prediction. Since uncertainty propagation for probabilistic validation is often unaffordable for complex computational models, surrogate models are often used; this paper proposes an approach to account for the additional uncertainty introduced in validation by the uncertain fit of the surrogate model. The proposed methods are demonstrated with a microelectromechanical system (MEMS) example.},
author = {Mullins, Joshua and Ling, You and Mahadevan, Sankaran and Sun, Lin and Strachan, Alejandro},
doi = {10.1016/j.ress.2015.10.003},
file = {:Users/rio/Documents/Mendeley Desktop/Mullins et al/Reliability Engineering and System Safety/Mullins et al. - 2016 - Separation of aleatory and epistemic uncertainty in probabilistic model validation.pdf:pdf},
issn = {09518320},
journal = {Reliability Engineering and System Safety},
keywords = {Epistemic uncertainty,Imprecise data,Model validation,Reliability,Validation metrics},
pages = {49--59},
title = {{Separation of aleatory and epistemic uncertainty in probabilistic model validation}},
volume = {147},
year = {2016}
}
@article{Noh2010,
abstract = {In RBDO, input uncertainty models such as marginal and joint cumulative$\backslash$ndistribution functions (CDFs) need to be used. However, only limited$\backslash$ndata exists in industry applications. Thus, identification of the$\backslash$ninput uncertainty model is challenging especially when input variables$\backslash$nare correlated. Since input random variables, such as fatigue material$\backslash$nproperties, are correlated in many industrial problems, the joint$\backslash$nCDF of correlated input variables needs to be correctly identified$\backslash$nfrom given data. In this paper, a Bayesian method is proposed to$\backslash$nidentify the marginal and joint CDFs from given data where a copula,$\backslash$nwhich only requires marginal CDFs and correlation parameters, is$\backslash$nused to model the joint CDF of input variables. Using simulated data$\backslash$nsets, performance of the Bayesian method is tested for different$\backslash$nnumbers of samples and is compared with the goodness-of-fit (GOF)$\backslash$ntest. Two examples are used to demonstrate how the Bayesian method$\backslash$nis used to identify correct marginal CDFs and copula.},
author = {Noh, Yoojeong and Choi, K. K. and Lee, Ikjin},
doi = {10.1007/s00158-009-0385-1},
file = {:Users/rio/Documents/Mendeley Desktop/Noh, Choi, Lee/Structural and Multidisciplinary Optimization/Noh, Choi, Lee - 2010 - Identification of marginal and joint CDFs using Bayesian method for RBDO.pdf:pdf},
isbn = {0015800903},
issn = {1615147X},
journal = {Structural and Multidisciplinary Optimization},
keywords = {Bayesian method,Copula,Correlated input variables,Goodness-of-fit test,Identification of marginal and joint CDFs,Input model uncertainty,Reliability-based design optimization},
number = {1-6},
pages = {35--51},
title = {{Identification of marginal and joint CDFs using Bayesian method for RBDO}},
volume = {40},
year = {2010}
}
@article{Ob2000,
author = {Oberkampf, W L and Deland, Sharon M and Rutherford, Brian M and Diegert, Kathleen V and Alvin, Kenneth F},
file = {:Users/rio/Documents/Mendeley Desktop/Oberkampf et al/Reliability Engineering and System Safety/Oberkampf et al. - 2002 - Estimation of total uncertainty in modeling and simulation.pdf:pdf},
journal = {Reliability Engineering and System Safety},
number = {April},
pages = {333--357},
title = {{Estimation of total uncertainty in modeling and simulation}},
volume = {75},
year = {2002}
}
@article{Oberkampf2004,
abstract = {Developers of computer codes, analysts who use the codes, and decision makers who rely on the results of the analyses face a critical question: How should confidence in modeling and simulation be critically assessed? Verification and validation (V{\&}V) of computational simulations are the primary methods for building and quantifying this confidence. Briefly, verification is the assessment of the accuracy of the solution to a computational model. Validation is the assessment of the accuracy of a computational simulation by comparison with experimental data. In verification, the relationship of the simulation to the real world is not an issue. In validation, the relationship between computation and the real world, ie, experimental data, is the issue. This paper presents our viewpoint of the state of the art in V{\&}V in computational physics. (In this paper we refer to all fields of computational engineering and physics, eg, computational fluid dynamics, computational solid mechanics, structural dynamics, shock wave physics, computational chemistry, etc, as computational physics.) We describe our view of the framework in which predictive capability relies on V{\&}V, as well as other factors that affect predictive capability. Our opinions about the research needs and management issues in V{\&}V are very practical: What methods and techniques need to be developed and what changes in the views of management need to occur to increase the usefulness, reliability, and impact of computational physics for decision making about engineering systems? We review the state of the art in V{\&}V over a wide range of topics, for example, prioritization of V{\&}V activities using the Phenomena Identification and Ranking Table (PIRT), code verification, software quality assurance (SQA), numerical error estimation, hierarchical experiments for validation, characteristics of validation experiments, the need to perform nondeterministic computational simulations in comparisons with experimental data, and validation metrics. We then provide an extensive discussion of V{\&}V research and implementation issues that we believe must be addressed for V{\&}V to be more effective in improving confidence in computational predictive capability. Some of the research topics addressed are development of improved procedures for the use of the PIRT for prioritizing V{\&}V activities, the method of manufactured solutions for code verification, development and use of hierarchical validation diagrams, and the construction and use of validation metrics incorporating statistical measures. Some of the implementation topics addressed are the needed management initiatives to better align and team computationalists and experimentalists in conducting validation activities, the perspective of commercial software companies, the key role of analysts and decision makers as code customers, obstacles to the improved effectiveness of V{\&}V, effects of cost and schedule constraints on practical applications in industrial settings, and the role of engineering standards committees in documenting best practices for V{\&}V. There are 207 references cited in this review article.},
author = {Oberkampf, William L and Trucano, Timothy G and Hirsch, Charles},
doi = {10.1115/1.1767847},
file = {:Users/rio/Documents/Mendeley Desktop/Oberkampf, Trucano, Hirsch/Applied Mechanics Reviews/Oberkampf, Trucano, Hirsch - 2004 - Verification, validation, and predictive capability in computational engineering and physics.pdf:pdf},
isbn = {0003-6900},
issn = {00036900},
journal = {Applied Mechanics Reviews},
number = {5},
pages = {345},
title = {{Verification, validation, and predictive capability in computational engineering and physics}},
volume = {57},
year = {2004}
}
@article{Oberkampf2002,
abstract = {This article develops a general framework for identifying error and uncertainty in computational simulations that deal with the numerical solution of a set of partial differential equations (PDEs). A comprehensive, new view of the general phases of modeling and simulation is proposed, consisting of the following phases: conceptual modeling of the physical system, mathematical modeling of the conceptual model, discretization and algorithm selection for the mathematical model, computer programming of the discrete model, numerical solution of the computer program model, and representation of the numerical solution. Our view incorporates the modeling and simulation phases that are recognized in the systems engineering and operations research communities, but it adds phases that are specific to the numerical solution of PDEs. In each of these phases, general sources of uncertainty, both aleatory and epistemic, and error are identified. Our general framework is applicable to any numerical discretization procedure for solving ODEs or PDEs. To demonstrate this framework, we describe a system-level example: the flight of an unguided, rocket-boosted, aircraft-launched missile. This example is discussed in detail at each of the six phases of modeling and simulation. Two alternative models of the flight dynamics are considered, along with aleatory uncertainty of the initial mass of the missile and epistemic uncertainty in the thrust of the rocket motor. We also investigate the interaction of modeling uncertainties and numerical integration error in the solution of the ordinary differential equations for the flight dynamics.},
author = {Oberkampf, William L. and DeLand, Sharon M. and Rutherford, Brian M. and Diegert, Kathleen V. and Alvin, Kenneth F.},
doi = {10.1016/S0951-8320(01)00120-X},
file = {:Users/rio/Documents/Mendeley Desktop/Oberkampf et al/Reliability Engineering {\&} System Safety/Oberkampf et al. - 2002 - Error and uncertainty in modeling and simulation.pdf:pdf},
isbn = {0951-8320},
issn = {09518320},
journal = {Reliability Engineering {\&} System Safety},
keywords = {aleatory uncertainty,epistemic uncertainty,modeling,nondeterministic features,simulation,stochastic uncertainty,subjective uncertainty},
number = {3},
pages = {333--357},
title = {{Error and uncertainty in modeling and simulation}},
volume = {75},
year = {2002}
}
@article{Oden2013,
abstract = {We address general approaches to the rational selection and validation of mathematical and computational models of tumor growth using methods of Bayesian inference. The model classes are derived from a general diffuse-interface, continuum mixture theory and focus on mass conservation of mixtures with up to four species. Synthetic data are generated using higher-order base models. We discuss general approaches to model cal- ibration, validation, plausibility, and selection based on Bayesian-based methods, infor- mation theory, and maximum information entropy.We also address computational issues and provide numerical experiments based on Markov chain Monte Carlo algorithms and high performance computing implementations.},
author = {Oden, J Tinsley and Prudencio, Ernesto E and Hawkins-Daarud, Andrea},
doi = {10.1142/S0218202513500103},
file = {:Users/rio/Documents/Mendeley Desktop/Oden, Prudencio, Hawkins-Daarud/Mathematical Models and Methods in Applied Sciences/Oden, Prudencio, Hawkins-Daarud - 2013 - SELECTION AND ASSESSMENT OF PHENOMENOLOGICAL MODELS OF TUMOR GROWTH.pdf:pdf},
issn = {0218-2025},
journal = {Mathematical Models and Methods in Applied Sciences},
keywords = {Bayesian statistics,Markov chain Monte Carlo methods.,diffuse-interface models,model selec- tion,model validation},
number = {7},
pages = {1309--1338},
title = {{Selection and Assessment of Phenomenological Models of Tumor Growth}},
url = {http://www.worldscientific.com/doi/abs/10.1142/S0218202513500103},
volume = {23},
year = {2013}
}
@article{Oden2010a,
abstract = {Having developed and calibrated a model to be used for a particular prediction (see Part I of this article, SIAM News, November 2010, page 1), we are ready to begin the validation process—that is, to assess the suitability of the calibrated model for the prediction.},
author = {Oden, J. Tinsley and Moser, Robert and Ghattas, Omar},
file = {:Users/rio/Documents/Mendeley Desktop/Oden, Moser, Ghattas/SIAM News/1857.pdf:pdf},
journal = {SIAM News},
number = {10},
pages = {2008--2011},
title = {{Computer predictions with quantified uncertainty, Part II}},
volume = {43},
year = {2010}
}
@article{Oden2010,
abstract = {Having developed and calibrated a model to be used for a particular prediction (see Part I of this article, SIAM News, November 2010, page 1), we are ready to begin the validation process—that is, to assess the suitability of the calibrated model for the prediction.},
author = {Oden, J. Tinsley and Moser, Robert and Ghattas, Omar},
file = {:Users/rio/Documents/Mendeley Desktop/Oden, Moser, Ghattas/SIAM News/1842.pdf:pdf},
journal = {SIAM News},
number = {10},
title = {{Computer predictions with quantified uncertainty, Part I}},
volume = {43},
year = {2010}
}
@article{Papadimitriou2014,
author = {Papadimitriou, Costas},
file = {:Users/rio/Documents/Mendeley Desktop/Papadimitriou/Proceedings of the 9th International Conference on Structural Dynamics, EURODYN 2014/Papadimitriou - 2014 - Bayesian Uncertainty Quantification and Propagation in Structural Dynamics.pdf:pdf},
isbn = {9789727521654},
journal = {Proceedings of the 9th International Conference on Structural Dynamics, EURODYN 2014},
keywords = {component mode synthesis,hpc,laplace asymptotics,large-order models,mcmc,surrogate models},
number = {July},
pages = {111--124},
title = {{Bayesian Uncertainty Quantification and Propagation in Structural Dynamics}},
year = {2014}
}
@article{Pianosi2016,
abstract = {Sensitivity Analysis (SA) investigates how the variation in the output of a numerical model can be attributed to variations of its input factors. SA is increasingly being used in environmental modelling for a variety of purposes, including uncertainty assessment, model calibration and diagnostic evaluation, dominant control analysis and robust decision-making. In this paper we review the SA literature with the goal of providing: (i) a comprehensive view of SA approaches also in relation to other methodologies for model identification and application; (ii) a systematic classification of the most commonly used SA methods; (iii) practical guidelines for the application of SA. The paper aims at delivering an introduction to SA for non-specialist readers, as well as practical advice with best practice examples from the literature; and at stimulating the discussion within the community of SA developers and users regarding the setting of good practices and on defining priorities for future research.},
author = {Pianosi, Francesca and Beven, Keith and Freer, Jim and Hall, Jim W. and Rougier, Jonathan and Stephenson, David B. and Wagener, Thorsten},
doi = {10.1016/j.envsoft.2016.02.008},
file = {:Users/rio/Documents/Mendeley Desktop/Pianosi et al/Environmental Modelling {\&} Software/Pianosi et al. - 2016 - Sensitivity analysis of environmental models A systematic review with practical workflow.pdf:pdf},
issn = {13648152},
journal = {Environmental Modelling {\&} Software},
keywords = {Calibration,Evaluation,Robust decision-making,Sensitivity Analysis,Spatio-tenporal Sensitivity Analysis,Uncertainty Analysis},
mendeley-tags = {Spatio-tenporal Sensitivity Analysis},
pages = {214--232},
publisher = {Elsevier Ltd},
title = {{Sensitivity analysis of environmental models: A systematic review with practical workflow}},
url = {http://www.sciencedirect.com/science/article/pii/S1364815216300287},
volume = {79},
year = {2016}
}
@techreport{Pilch2006,
author = {Pilch, Martin and Trucano, T.G.},
booktitle = {Sandia Report},
file = {:Users/rio/Documents/Mendeley Desktop/Pilch, Trucano/Sandia Report/Pilch, Trucano - 2006 - Ideas Underlying Quantification of Margins and Uncertainties (QMU) A White Paper.pdf:pdf},
number = {SAND2006-5001},
title = {{Ideas Underlying Quantification of Margins and Uncertainties (QMU): A White Paper}},
url = {http://www.stanford.edu/group/uq/docs/qmu{\_}ideas.pdf},
year = {2006}
}
@phdthesis{Sankararaman2012,
author = {Sankararaman, Shankar},
booktitle = {PhD Dissertation},
file = {:Users/rio/Documents/Mendeley Desktop/Sankararaman/PhD Dissertation/Sankararaman - 2012 - Uncertainty Quantification and Integration(3).pdf:pdf},
school = {Vanderbilt University},
title = {{Uncertainty Quantification and Integration}},
year = {2012}
}
@article{Santonja2012,
abstract = {Mathematical models based on ordinary differential equations are a useful tool to study the processes involved in epidemiology. Many models consider that the parameters are deterministic variables. But in practice, the transmission parameters present large variability and it is not possible to determine them exactly, and it is necessary to introduce randomness. In this paper, we present an application of the polynomial chaos approach to epidemiological mathematical models based on ordinary differential equations with random coefficients. Taking into account the variability of the transmission parameters of the model, this approach allows us to obtain an auxiliary system of differential equations, which is then integrated numerically to obtain the first-and the second-order moments of the output stochastic processes. A sensitivity analysis based on the polynomial chaos approach is also performed to determine which parameters have the greatest influence on the results. As an example, we will apply the approach to an obesity epidemic model.},
author = {Santonja, F. and Chen-Charpentier, B.},
doi = {10.1155/2012/742086},
file = {:Users/rio/Documents/Mendeley Desktop/Santonja, Chen-Charpentier/Computational and Mathematical Methods in Medicine/Santonja, Chen-Charpentier - 2012 - Uncertainty quantification in simulations of epidemics using polynomial chaos.pdf:pdf},
issn = {1748670X},
journal = {Computational and Mathematical Methods in Medicine},
pmid = {22927889},
title = {{Uncertainty quantification in simulations of epidemics using polynomial chaos}},
volume = {2012},
year = {2012}
}
@article{Shirangi2016,
abstract = {Parameterization based on truncated singular value decomposition (TSVD) of the dimensionless sensitivity matrix has been shown to be an efficient approach for history matching. With TSVD parameterization, the search direction is computed as a linear combination of a few principal right singular vectors. As the sensitivity matrix is not explicitly computed, this parameterization is appropriate for large-scale history-matching problems. Moreover, previous work presented theoretical evidence that TSVD of the dimensionless sensitivity matrix provides the optimal parameterization in terms of uncertainty reduction. TSVD has been used in the randomized maximum likelihood (RML) framework to generate multiple conditional realizations of reservoir models. In this work, we investigate the effect of TSVD in the search direction obtained by the application of the Gauss–Newton and the Levenberg–Marquardt (LM) methods. In particular, we show that the TSVD-based LM algorithm converges to appropriate estimates because it gradually resolves the important features of the true model. We also introduce an improved implementation of a TSVD-based LM algorithm for generating multiple realizations of reservoir models conditioned to production data. Our experiments indicate that the computational cost of the new implementation is on the order of 2/3 of the cost of the previous implementation.},
author = {Shirangi, Mehrdad G. and Emerick, Alexandre A.},
doi = {10.1016/j.petrol.2016.02.026},
file = {:Users/rio/Documents/Mendeley Desktop/Shirangi, Emerick/Journal of Petroleum Science and Engineering/Shirangi, Emerick - 2016 - An improved TSVD-based Levenberg-Marquardt algorithm for history matching and comparison with Gauss-Newton.pdf:pdf},
issn = {09204105},
journal = {Journal of Petroleum Science and Engineering},
pages = {258--271},
title = {{An improved TSVD-based Levenberg-Marquardt algorithm for history matching and comparison with Gauss-Newton}},
volume = {143},
year = {2016}
}
@article{Tobergte2013,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Tobergte, David R. and Curtis, Shirley},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:Users/rio/Documents/Mendeley Desktop/Tobergte, Curtis/Journal of Chemical Information and Modeling/Tobergte, Curtis - 2013 - Workshop on Quantification, Communication, and Interpretation of Uncertainty in Simulation and Data Science.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Journal of Chemical Information and Modeling},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{Workshop on Quantification, Communication, and Interpretation of Uncertainty in Simulation and Data Science}},
volume = {53},
year = {2013}
}
@article{Wang2016,
abstract = {Uncertainty quantification (UQ) refers to quantitative characterization and reduction of uncertainties present in computer model simulations. It is widely used in engineering and geophysics fields to assess and predict the likelihood of various outcomes. This paper describes a UQ platform called UQ-PyL (Uncertainty Quantification Python Laboratory), a flexible software platform designed to quantify uncertainty of complex dynamical models. UQ-PyL integrates different kinds of UQ methods, including experimental design, statistical analysis, sensitivity analysis, surrogate modeling and parameter optimization. It is written in Python language and runs on all common operating systems. UQ-PyL has a graphical user interface that allows users to enter commands via pull-down menus. It is equipped with a model driver generator that allows any computer model to be linked with the software. We illustrate the different functions of UQ-PyL by applying it to the uncertainty analysis of the Sacramento Soil Moisture Accounting Model. We will also demonstrate that UQ-PyL can be applied to a wide range of applications.},
author = {Wang, Chen and Duan, Qingyun and Tong, Charles H. and Di, Zhenhua and Gong, Wei},
doi = {10.1016/j.envsoft.2015.11.004},
file = {:Users/rio/Documents/Mendeley Desktop/Wang et al/Environmental Modelling and Software/Wang et al. - 2016 - A GUI platform for uncertainty quantification of complex dynamical models.pdf:pdf},
issn = {13648152},
journal = {Environmental Modelling and Software},
keywords = {Design of experiments,Parameter optimization,Sensitivity analysis,Surrogate modeling,UQ-PyL,Uncertainty Quantification},
pages = {1--12},
publisher = {Elsevier Ltd},
title = {{A GUI platform for uncertainty quantification of complex dynamical models}},
url = {http://dx.doi.org/10.1016/j.envsoft.2015.11.004},
volume = {76},
year = {2016}
}
@article{Wick2010,
abstract = {Incorporating probabilities into the semantics of incomplete databases has posed many challenges, forcing systems to sacrifice modeling power, scalability, or treatment of relational algebra operators. We propose an alternative approach where the underlying relational database always represents a single world, and an external factor graph encodes a distribution over possible worlds; Markov chain Monte Carlo (MCMC) inference is then used to recover this uncertainty to a desired level of fidelity. Our approach allows the efficient evaluation of arbitrary queries over probabilistic databases with arbitrary dependencies expressed by graphical models with structure that changes during inference. MCMC sampling provides efficiency by hypothesizing modifications to possible worlds rather than generating entire worlds from scratch. Queries are then run over the portions of the world that change, avoiding the onerous cost of running full queries over each sampled world. A significant innovation of this work is the connection between MCMC sampling and materialized view maintenance techniques: we find empirically that using view maintenance techniques is several orders of magnitude faster than naively querying each sampled world. We also demonstrate our system's ability to answer relational queries with aggregation, and demonstrate additional scalability through the use of parallelization on a real-world complex model of information extraction. This framework is sufficiently expressive to support probabilistic inference not only for answering queries, but also for inferring missing database content from raw evidence.},
archivePrefix = {arXiv},
arxivId = {1005.1934},
author = {Wick, Michael and McCallum, A and Miklau, Gerome},
doi = {10.14778/1920841.1920942},
eprint = {1005.1934},
file = {:Users/rio/Documents/Mendeley Desktop/Wick, McCallum, Miklau/Proceedings of the VLDB Endowment/Wick, McCallum, Miklau - 2010 - Scalable probabilistic databases with factor graphs and mcmc.1934v1:1934v1},
issn = {21508097},
journal = {Proceedings of the VLDB Endowment},
number = {1-2},
pages = {794--804},
title = {{Scalable probabilistic databases with factor graphs and mcmc}},
url = {papers2://publication/uuid/6327ACAE-8C67-4F02-80AD-ACD061F4E477},
volume = {3},
year = {2010}
}
@article{Williamson2015,
author = {Williamson, D.},
doi = {10.1002/env.2335},
file = {:Users/rio/Documents/Mendeley Desktop/Williamson/Environmetrics/Williamson - 2015 - Exploratory ensemble designs for environmental models using k-extended Latin Hypercubes.pdf:pdf},
issn = {11804009},
journal = {Environmetrics},
month = {jun},
number = {4},
pages = {268--283},
title = {{Exploratory ensemble designs for environmental models using k-extended Latin Hypercubes}},
url = {http://doi.wiley.com/10.1002/env.2335},
volume = {26},
year = {2015}
}
@article{Williamson2014a,
abstract = {We develop Bayesian dynamic linear model Gaussian processes for emulation of time series output for computer models that may exhibit chaotic behavior, but where this behavior retains some underlying structure. The statistical technology is particularly suited to emulating the time series output of large climate models that exhibit this feature and where we want samples from the posterior of the emulator to evolve in the same way as dynamic processes in the computer model do. The methodology combines key features of good uncertainty quantification (UQ) methods such as using complex mean functions to capture large-scale signals within parameter space, with dynamic linear models in a way that allows UQ to borrow strength from the Bayesian time series literature. We present an MCMC algorithm for sampling from the posterior of the emulator parameters when the roughness lengths of the Gaussian process are unknown. We discuss an interpretation of the results of this algorithm that allows us to use MCMC to fix th...},
author = {Williamson, Daniel and Blaker, Adam T.},
file = {:Users/rio/Documents/Mendeley Desktop/Williamson, Blaker/SIAMASA Journal on Uncertainty Quantification/Williamson, Blaker - 2014 - Evolving Bayesian Emulators for Structured Chaotic Time Series, with Application to Large Climate Models.pdf:pdf},
journal = {SIAM/ASA Journal on Uncertainty Quantification},
keywords = {37N10,60G15,60Gxx,Bayesian analysis,climate models,dynamic emulation,uncertainty quantification},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Evolving Bayesian Emulators for Structured Chaotic Time Series, with Application to Large Climate Models}},
year = {2014}
}
@article{Williamson2015a,
author = {Williamson, Daniel and Blaker, Adam T. and Hampton, Charlotte and Salter, James},
doi = {10.1007/s00382-014-2378-z},
file = {:Users/rio/Documents/Mendeley Desktop/Williamson et al/Climate Dynamics/Williamson et al. - 2015 - Identifying and removing structural biases in climate models with history matching.pdf:pdf},
issn = {0930-7575},
journal = {Climate Dynamics},
month = {sep},
number = {5-6},
pages = {1299--1324},
title = {{Identifying and removing structural biases in climate models with history matching}},
url = {http://link.springer.com/10.1007/s00382-014-2378-z},
volume = {45},
year = {2015}
}
@article{Weber2011,
author = {Wood-Schultz, David H. Sharp {\&} Merri M.},
file = {:Users/rio/Documents/Mendeley Desktop/Wood-Schultz/Los Alamos Science/Wood-Schultz - 2011 - QMU and Nuclear Weapons Certification-What's under the Hood.pdf:pdf},
journal = {Los Alamos Science},
number = {1},
pages = {55--82},
title = {{QMU and Nuclear Weapons Certification-What's under the Hood?}},
volume = {44},
year = {2011}
}
@article{Yi2016,
abstract = {Sensitivity analysis is a primary approach used in mathematical modeling to identify important factors that control the response dynamics in a model. In this paper, we applied the Morris sensitivity analysis method to identify the important factors governing the dynamics in a complex 3-dimensional water quality model. The water quality model was developed using the Environmental fluid dynamics code (EFDC) to simulate the fate and transport of nutrients and algal dynamics in Lake Dianchi, one of the most polluted large lakes in China. The analysis focused on the response of four water quality constituents, including chlorophyll-a, dissolved oxygen, total nitrogen, and total phosphorus, to 47 parameters and 7 external driving forces. We used Morris sensitivity analysis with different sample sizes and factor perturbation ranges to study the sensitivity with regard to different output metrics of the water quality model, and we analyzed the consistency between different sensitivity scenarios. In addition to the analysis with aggregate outputs, a spatiotemporal variability analysis was performed to understand the spatial heterogeneity and temporal distribution of sensitivities. Our results indicated that it is important to consider multiple characteristics in a sensitivity analysis, and we have identified a robust set of sensitive factors in the water quality model that will be useful for systematic model parameter identification and uncertainty analysis.},
author = {Yi, Xuan and Zou, Rui and Guo, Huaicheng},
doi = {10.1016/j.ecolmodel.2016.01.005},
file = {:Users/rio/Documents/Mendeley Desktop/Yi, Zou, Guo/Ecological Modelling/Yi, Zou, Guo - 2016 - Global sensitivity analysis of a three-dimensional nutrients-algae dynamic model for a large shallow lake.pdf:pdf},
issn = {03043800},
journal = {Ecological Modelling},
keywords = {EFDC model,Morris screening,Sensitivity analysis,Spatio-tenporal Sensitivity Analysis,Spatiotemporal sensitivity indices,Water quality model},
mendeley-tags = {Spatio-tenporal Sensitivity Analysis},
pages = {74--84},
publisher = {Elsevier B.V.},
title = {{Global sensitivity analysis of a three-dimensional nutrients-algae dynamic model for a large shallow lake}},
url = {http://dx.doi.org/10.1016/j.ecolmodel.2016.01.005},
volume = {327},
year = {2016}
}
@article{Zhang2008,
abstract = {Uncertain data are inherent in many important applications. Recently, considerable research efforts have been put into the field of managing uncertain data. In this paper, we summarize existing techniques to query and model uncertain data and systems that effectively manage uncertain data, mainly from a probabilistic point of view.},
author = {Zhang, Wenjie and Lin, Xuemin and Pei, Jian and Zhang, Ying},
doi = {10.1109/WAIM.2008.42},
file = {:Users/rio/Documents/Mendeley Desktop/Zhang et al/Proceedings - The 9th International Conference on Web-Age Information Management, WAIM 2008/Zhang et al. - 2008 - Managing uncertain data Probabilistic approaches.pdf:pdf},
isbn = {9780769531854},
journal = {Proceedings - The 9th International Conference on Web-Age Information Management, WAIM 2008},
pages = {405--412},
title = {{Managing uncertain data: Probabilistic approaches}},
year = {2008}
}
