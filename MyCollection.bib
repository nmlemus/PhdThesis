@book{Loucks2005,
abstract = {The usefulness of any model depends in part on the accuracy and reliability of its output. Yet, because all models are imperfect abstractions of reality, and because precise input data are rarely if ever available, all output values are subject to imprecision. Input data errors and modelling uncertainties are not independent of each other – they can interact in various ways. The end result is imprecision and uncertainty associated with model output. This chapter focuses on ways of identifying, quantifying, and communicating the uncertainties in model outputs.},
author = {Loucks, Daniel P. and van Beek, Eelco and Stedinger, Jery R. and Dijkman, Jozef P.M. and Villars, Monique T.},
booktitle = {Water Resources Systems Planning and Management: An Introduction to Methods, Models and Applications},
doi = {ISBN: 92-3-103998-9},
isbn = {9231039989},
pages = {254--290},
title = {{9 Model Sensitivity and Uncertainty Analysis}},
url = {https://www.utwente.nl/ctw/wem/education/afstuderen/Loucks{\_}VanBeek/09{\_}chapter09.pdf},
year = {2005}
}
@article{Gaganis2001,
author = {Gaganis, Petros and Smith, Leslie},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Gaganis, Smith - 2001 - A Bayesian approach to the quantification of the effect of model error on the predictions of groundwater models.pdf:pdf},
journal = {Water Resources},
keywords = {doi:10.102,http://dx.doi.org/10.1029/2000WR000001},
number = {9},
pages = {2309 --2322},
title = {{A Bayesian approach to the quantification of the effect of model error on the predictions of groundwater models}},
volume = {37},
year = {2001}
}
@article{Farrell2015,
abstract = {A general adaptive modeling algorithm for selection and validation of coarse-grained models of atomistic systems is presented. A Bayesian framework is developed to address uncertainties in parameters, data, and model selection. Algorithms for computing output sensitivities to parameter variances, model evidence and posterior model plausibilities for given data, and for computing what are referred to as Occam Categories in reference to a rough measure of model simplicity, make up components of the overall approach. Computational results are provided for representative applications.},
author = {Farrell, Kathryn and Oden, J. Tinsley and Faghihi, Danial},
doi = {10.1016/j.jcp.2015.03.071},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Farrell, Oden, Faghihi - 2015 - A Bayesian framework for adaptive selection, calibration, and validation of coarse-grained models of ato.pdf:pdf},
issn = {10902716},
journal = {Journal of Computational Physics},
keywords = {Bayesian inference,Coarse graining models,Model plausibility,Model validation,Output sensitivities},
pages = {189--208},
publisher = {Elsevier Inc.},
title = {{A Bayesian framework for adaptive selection, calibration, and validation of coarse-grained models of atomistic systems}},
url = {http://dx.doi.org/10.1016/j.jcp.2015.03.071},
volume = {295},
year = {2015}
}
@article{Zhang2016,
abstract = {Stochastic spectral methods have become a popular technique to quantify the uncertainties of nano-scale devices and circuits. They are much more efficient than Monte Carlo for certain design cases with a small number of random parameters. However, their computational cost significantly increases as the number of random parameters increases. This paper presents a big-data approach to solve high-dimensional uncertainty quantification problems. Specifically, we simulate integrated circuits and MEMS at only a small number of quadrature samples, then, a huge number of (e.g., {\$}1.5 \backslashtimes 10{\^{}}{\{}27{\}}{\$}) solution samples are estimated from the available small-size (e.g., {\$}500{\$}) solution samples via a low-rank and tensor-recovery method. Numerical results show that our algorithm can easily extend the applicability of tensor-product stochastic collocation to IC and MEMS problems with over 50 random parameters, whereas the traditional algorithm can only handle several random parameters.},
archivePrefix = {arXiv},
arxivId = {1603.06119},
author = {Zhang, Zheng and Weng, Tsui-Wei and Daniel, Luca},
eprint = {1603.06119},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Weng, Daniel - 2016 - A Big-Data Approach to Handle Process Variations Uncertainty Quantification by Tensor Recovery.pdf:pdf},
number = {2},
title = {{A Big-Data Approach to Handle Process Variations: Uncertainty Quantification by Tensor Recovery}},
url = {http://arxiv.org/abs/1603.06119},
year = {2016}
}
@article{Lee2009,
abstract = {A wide variety of uncertainty propagation methods exist in literature; however, there is a lack of good understanding of their relative merits. In this paper, a comparative study on the performances of several representative uncertainty propagation methods, including a few newly developed methods that have received growing attention, is performed. The full factorial numerical integration, the univariate dimension reduction method, and the polynomial chaos expansion method are implemented and applied to several test problems. They are tested under different settings of the performance nonlinearity, distribution types of input random variables, and the magnitude of input uncertainty. The performances of those methods are compared in moment estimation, tail probability calculation, and the probability density function construction, corresponding to a wide variety of scenarios of design under uncertainty, such as robust design, and reliability-based design optimization. The insights gained are expected to direct designers for choosing the most applicable uncertainty propagation technique in design under uncertainty.},
author = {Lee, S. H. and Chen, W.},
doi = {10.1007/s00158-008-0234-7},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Lee, Chen - 2009 - A comparative study of uncertainty propagation methods for black-box-type problems.pdf:pdf},
isbn = {1615-147X},
issn = {1615147X},
journal = {Structural and Multidisciplinary Optimization},
keywords = {Comparative study,Design under uncertainty,Dimension reduction method,Full factorial numerical integration,Polynomial chaos expansion,Uncertainty propagation},
number = {3},
pages = {239--253},
title = {{A comparative study of uncertainty propagation methods for black-box-type problems}},
volume = {37},
year = {2009}
}
@article{Lee2009a,
abstract = {A wide variety of uncertainty propagation methods exist in literature; however, there is a lack of good understanding of their relative merits. In this paper, a comparative study on the performances of several representative uncertainty propagation methods, including a few newly developed methods that have received growing attention, is performed. The full factorial numerical integration, the univariate dimension reduction method, and the polynomial chaos expansion method are implemented and applied to several test problems. They are tested under different settings of the performance nonlinearity, distribution types of input random variables, and the magnitude of input uncertainty. The performances of those methods are compared in moment estimation, tail probability calculation, and the probability density function construction, corresponding to a wide variety of scenarios of design under uncertainty, such as robust design, and reliability-based design optimization. The insights gained are expected to direct designers for choosing the most applicable uncertainty propagation technique in design under uncertainty.},
author = {Lee, S. H. and Chen, W.},
doi = {10.1007/s00158-008-0234-7},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Lee, Chen - 2009 - A comparative study of uncertainty propagation methods for black-box-type problems.pdf:pdf},
isbn = {1615-147X},
issn = {1615147X},
journal = {Structural and Multidisciplinary Optimization},
keywords = {Comparative study,Design under uncertainty,Dimension reduction method,Full factorial numerical integration,Polynomial chaos expansion,Uncertainty propagation},
number = {3},
pages = {239--253},
title = {{A comparative study of uncertainty propagation methods for black-box-type problems}},
volume = {37},
year = {2009}
}
@article{Roy2011,
abstract = {An overview of a comprehensive framework is given for estimating the predictive uncertainty of scientific computing applications. The framework is comprehensive in the sense that it treats both types of uncertainty (aleatory and epistemic), incorporates uncertainty due to the mathematical form of the model, and it provides a procedure for including estimates of numerical error in the predictive uncertainty. Aleatory (random) uncertainties in model inputs are treated as random variables, while epistemic (lack of knowledge) uncertainties are treated as intervals with no assumed probability distributions. Approaches for propagating both types of uncertainties through the model to the system response quantities of interest are briefly discussed. Numerical approximation errors (due to discretization, iteration, and computer round off) are estimated using verification techniques, and the conversion of these errors into epistemic uncertainties is discussed. Model form uncertainty is quantified using (a) model validation procedures, i.e., statistical comparisons of model predictions to available experimental data, and (b) extrapolation of this uncertainty structure to points in the application domain where experimental data do not exist. Finally, methods for conveying the total predictive uncertainty to decision makers are presented. The different steps in the predictive uncertainty framework are illustrated using a simple example in computational fluid dynamics applied to a hypersonic wind tunnel. ?? 2011 Elsevier B.V.},
annote = {All the concepts, definitions, source of uncertainty, types of uncertainty, a full workflow. Very usefull to reference all the steps of the UQ process.},
author = {Roy, Christopher J. and Oberkampf, William L.},
doi = {10.1016/j.cma.2011.03.016},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Roy, Oberkampf - 2011 - A comprehensive framework for verification, validation, and uncertainty quantification in scientific computing.pdf:pdf},
isbn = {0045-7825},
issn = {00457825},
journal = {Computer Methods in Applied Mechanics and Engineering},
keywords = {Computational simulation,Modeling,Uncertainty quantification,Validation,Verification},
number = {25-28},
pages = {2131--2144},
publisher = {Elsevier B.V.},
title = {{A comprehensive framework for verification, validation, and uncertainty quantification in scientific computing}},
volume = {200},
year = {2011}
}
@article{Baroni2014,
abstract = {The present study proposes a General Probabilistic Framework (GPF) for uncertainty and global sensitivity analysis of deterministic models in which, in addition to scalar inputs, non-scalar and correlated inputs can be considered as well. The analysis is conducted with the variance-based approach of Sobol/Saltelli where first and total sensitivity indices are estimated. The results of the framework can be used in a loop for model improvement, parameter estimation or model simplification. The framework is applied to SWAP, a 1D hydrological model for the transport of water, solutes and heat in unsaturated and saturated soils. The sources of uncertainty are grouped in five main classes: model structure (soil discretization), input (weather data), time-varying (crop) parameters, scalar parameters (soil properties) and observations (measured soil moisture). For each source of uncertainty, different realizations are created based on direct monitoring activities. Uncertainty of evapotranspiration, soil moisture in the root zone and bottom fluxes below the root zone are considered in the analysis. The results show that the sources of uncertainty are different for each output considered and it is necessary to consider multiple output variables for a proper assessment of the model. Improvements on the performance of the model can be achieved reducing the uncertainty in the observations, in the soil parameters and in the weather data. Overall, the study shows the capability of the GPF to quantify the relative contribution of the different sources of uncertainty and to identify the priorities required to improve the performance of the model. The proposed framework can be extended to a wide variety of modelling applications, also when direct measurements of model output are not available. {\textcopyright} 2013 Elsevier Ltd.},
annote = {From Duplicate 2 (A General Probabilistic Framework for uncertainty and global sensitivity analysis of deterministic models: A hydrological case study - Baroni, G.; Tarantola, S.)

From Duplicate 2 (A General Probabilistic Framework for uncertainty and global sensitivity analysis of deterministic models: A hydrological case study - Baroni, G.; Tarantola, S.)

Framework to UA and SA. Reference to UQ and SA.},
author = {Baroni, G. and Tarantola, S.},
doi = {10.1016/j.envsoft.2013.09.022},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Baroni, Tarantola - 2014 - A General Probabilistic Framework for uncertainty and global sensitivity analysis of deterministic models A h.pdf:pdf},
isbn = {3319772805},
issn = {13648152},
journal = {Environmental Modelling and Software},
keywords = {Global sensitivity analysis,Hydrological model,Multi-variables,Non-scalar input factors},
pages = {26--34},
publisher = {Elsevier Ltd},
title = {{A General Probabilistic Framework for uncertainty and global sensitivity analysis of deterministic models: A hydrological case study}},
url = {http://dx.doi.org/10.1016/j.envsoft.2013.09.022},
volume = {51},
year = {2014}
}
@article{Baroni2014a,
abstract = {The present study proposes a General Probabilistic Framework (GPF) for uncertainty and global sensitivity analysis of deterministic models in which, in addition to scalar inputs, non-scalar and correlated inputs can be considered as well. The analysis is conducted with the variance-based approach of Sobol/Saltelli where first and total sensitivity indices are estimated. The results of the framework can be used in a loop for model improvement, parameter estimation or model simplification. The framework is applied to SWAP, a 1D hydrological model for the transport of water, solutes and heat in unsaturated and saturated soils. The sources of uncertainty are grouped in five main classes: model structure (soil discretization), input (weather data), time-varying (crop) parameters, scalar parameters (soil properties) and observations (measured soil moisture). For each source of uncertainty, different realizations are created based on direct monitoring activities. Uncertainty of evapotranspiration, soil moisture in the root zone and bottom fluxes below the root zone are considered in the analysis. The results show that the sources of uncertainty are different for each output considered and it is necessary to consider multiple output variables for a proper assessment of the model. Improvements on the performance of the model can be achieved reducing the uncertainty in the observations, in the soil parameters and in the weather data. Overall, the study shows the capability of the GPF to quantify the relative contribution of the different sources of uncertainty and to identify the priorities required to improve the performance of the model. The proposed framework can be extended to a wide variety of modelling applications, also when direct measurements of model output are not available. {\textcopyright} 2013 Elsevier Ltd.},
author = {Baroni, G. and Tarantola, S.},
doi = {10.1016/j.envsoft.2013.09.022},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Baroni, Tarantola - 2014 - A General Probabilistic Framework for uncertainty and global sensitivity analysis of deterministic models A h.pdf:pdf},
isbn = {3319772805},
issn = {13648152},
journal = {Environmental Modelling and Software},
keywords = {,Global sensitivity analysis,Hydrological model,Multi-variables,Non-scalar input factors},
pages = {26--34},
publisher = {Elsevier Ltd},
title = {{A General Probabilistic Framework for uncertainty and global sensitivity analysis of deterministic models: A hydrological case study}},
url = {http://dx.doi.org/10.1016/j.envsoft.2013.09.022},
volume = {51},
year = {2014}
}
@article{Wang2016,
abstract = {Uncertainty quantification (UQ) refers to quantitative characterization and reduction of uncertainties present in computer model simulations. It is widely used in engineering and geophysics fields to assess and predict the likelihood of various outcomes. This paper describes a UQ platform called UQ-PyL (Uncertainty Quantification Python Laboratory), a flexible software platform designed to quantify uncertainty of complex dynamical models. UQ-PyL integrates different kinds of UQ methods, including experimental design, statistical analysis, sensitivity analysis, surrogate modeling and parameter optimization. It is written in Python language and runs on all common operating systems. UQ-PyL has a graphical user interface that allows users to enter commands via pull-down menus. It is equipped with a model driver generator that allows any computer model to be linked with the software. We illustrate the different functions of UQ-PyL by applying it to the uncertainty analysis of the Sacramento Soil Moisture Accounting Model. We will also demonstrate that UQ-PyL can be applied to a wide range of applications.},
author = {Wang, Chen and Duan, Qingyun and Tong, Charles H. and Di, Zhenhua and Gong, Wei},
doi = {10.1016/j.envsoft.2015.11.004},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2016 - A GUI platform for uncertainty quantification of complex dynamical models.pdf:pdf},
isbn = {1364-8152},
issn = {13648152},
journal = {Environmental Modelling and Software},
keywords = {Design of experiments,Parameter optimization,Sensitivity analysis,Surrogate modeling,UQ-PyL,Uncertainty Quantification},
pages = {1--12},
publisher = {Elsevier Ltd},
title = {{A GUI platform for uncertainty quantification of complex dynamical models}},
url = {http://dx.doi.org/10.1016/j.envsoft.2015.11.004},
volume = {76},
year = {2016}
}
@article{Wang2016,
abstract = {Uncertainty quantification (UQ) refers to quantitative characterization and reduction of uncertainties present in computer model simulations. It is widely used in engineering and geophysics fields to assess and predict the likelihood of various outcomes. This paper describes a UQ platform called UQ-PyL (Uncertainty Quantification Python Laboratory), a flexible software platform designed to quantify uncertainty of complex dynamical models. UQ-PyL integrates different kinds of UQ methods, including experimental design, statistical analysis, sensitivity analysis, surrogate modeling and parameter optimization. It is written in Python language and runs on all common operating systems. UQ-PyL has a graphical user interface that allows users to enter commands via pull-down menus. It is equipped with a model driver generator that allows any computer model to be linked with the software. We illustrate the different functions of UQ-PyL by applying it to the uncertainty analysis of the Sacramento Soil Moisture Accounting Model. We will also demonstrate that UQ-PyL can be applied to a wide range of applications.},
author = {Wang, Chen and Duan, Qingyun and Tong, Charles H. and Di, Zhenhua and Gong, Wei},
doi = {10.1016/j.envsoft.2015.11.004},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2016 - A GUI platform for uncertainty quantification of complex dynamical models.pdf:pdf},
isbn = {1364-8152},
issn = {13648152},
journal = {Environmental Modelling and Software},
keywords = {Design of experiments,Parameter optimization,Sensitivity analysis,Surrogate modeling,UQ-PyL,Uncertainty Quantification},
pages = {1--12},
publisher = {Elsevier Ltd},
title = {{A GUI platform for uncertainty quantification of complex dynamical models}},
url = {http://dx.doi.org/10.1016/j.envsoft.2015.11.004},
volume = {76},
year = {2016}
}
@article{Goncalves2013a,
author = {Gon{\c{c}}alves, Bernardo and Porto, Fabio},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Gon{\c{c}}alves, Porto - 2013 - A lattice-theoretic approach for representing and managing hypothesis-driven research.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {Data management,Large-scale science,Lattice theory,Scientific hypotheses},
title = {{A lattice-theoretic approach for representing and managing hypothesis-driven research}},
volume = {1087},
year = {2013}
}
@book{Marino2009,
abstract = {Accuracy of results from mathematical and computer models of biological systems is often complicated by the presence of uncertainties in experimental data that are used to estimate parameter values. Current mathematical modeling approaches typically use either single-parameter or local sensitivity analyses. However, these methods do not accurately assess uncertainty and sensitivity in the system as, by default they hold all other parameters fixed at baseline values. Using techniques described within we demonstrate how a multi-dimensional parameter space can be studied globally so all uncertainties can be identified. Further, uncertainty and sensitivity analysis techniques can help to identify and ultimately control uncertainties. In this work we develop methods for applying existing analytical tools to perform analyses on a variety of mathematical and computer models. We compare two specific types of global sensitivity analysis indexes that have proven to be among the most robust and efficient. Through familiar and new examples of mathematical and computer models, we provide a complete methodology for performing these analyses, both in deterministic and stochastic settings, and propose novel techniques to handle problems encountered during this type of analyses.},
author = {Marino, Simeone and Hogue, Ian B and Ray, Christian J and Kirschner, Denise E},
booktitle = {Journal of Theoretical Biology},
doi = {10.1016/j.jtbi.2008.04.011.A},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Marino et al. - 2008 - A Methodology for Performing Global Ucertainty and Sensitivity Analysis in Systems Biology.pdf:pdf},
isbn = {7346477723},
issn = {1095-8541},
keywords = {abm,agent-based model,aleatory uncertainty,amplitude sensitivity test,efast,epistemic uncertainty,extended fourier,latin hypercube sampling,lhs,methods,monte carlo,partial rank correlation coefficient,prcc,sensitivity index},
number = {1},
pages = {178--196},
pmid = {18572196},
title = {{A Methodology for Performing Global Ucertainty and Sensitivity Analysis in Systems Biology}},
volume = {254},
year = {2009}
}
@book{Marino2009,
abstract = {Accuracy of results from mathematical and computer models of biological systems is often complicated by the presence of uncertainties in experimental data that are used to estimate parameter values. Current mathematical modeling approaches typically use either single-parameter or local sensitivity analyses. However, these methods do not accurately assess uncertainty and sensitivity in the system as, by default they hold all other parameters fixed at baseline values. Using techniques described within we demonstrate how a multi-dimensional parameter space can be studied globally so all uncertainties can be identified. Further, uncertainty and sensitivity analysis techniques can help to identify and ultimately control uncertainties. In this work we develop methods for applying existing analytical tools to perform analyses on a variety of mathematical and computer models. We compare two specific types of global sensitivity analysis indexes that have proven to be among the most robust and efficient. Through familiar and new examples of mathematical and computer models, we provide a complete methodology for performing these analyses, both in deterministic and stochastic settings, and propose novel techniques to handle problems encountered during this type of analyses.},
author = {Marino, Simeone and Hogue, Ian B and Ray, Christian J and Kirschner, Denise E},
booktitle = {Journal of Theoretical Biology},
doi = {10.1016/j.jtbi.2008.04.011.A},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Marino et al. - 2008 - A Methodology for Performing Global Ucertainty and Sensitivity Analysis in Systems Biology.pdf:pdf},
isbn = {7346477723},
issn = {1095-8541},
keywords = {abm,agent-based model,aleatory uncertainty,amplitude sensitivity test,efast,epistemic uncertainty,extended fourier,latin hypercube sampling,lhs,methods,monte carlo,partial rank correlation coefficient,prcc,sensitivity index},
number = {1},
pages = {178--196},
pmid = {18572196},
title = {{A Methodology for Performing Global Ucertainty and Sensitivity Analysis in Systems Biology}},
volume = {254},
year = {2009}
}
@article{Borgonovo2007,
abstract = {Uncertainty in parameters is present in many risk assessment problems and leads to uncertainty in model predictions. In this work, we introduce a global sensitivity indicator which looks at the influence of input uncertainty on the entire output distribution without reference to a specific moment of the output (moment independence) and which can be defined also in the presence of correlations among the parameters. We discuss its mathematical properties and highlight the differences between the present indicator, variance-based uncertainty importance measures and a moment independent sensitivity indicator previously introduced in the literature. Numerical results are discussed with application to the probabilistic risk assessment model on which Iman [A matrix-based approach to uncertainty and sensitivity analysis for fault trees. Risk Anal 1987;7(1):22-33] first introduced uncertainty importance measures. {\textcopyright} 2006 Elsevier Ltd. All rights reserved.},
annote = {They introduce a new global SA parameter to estimate the influence of any parameter in the output. They make a ranking of the influence of the parameters in the output. The method is independent of the correlations of the input parameters.},
author = {Borgonovo, E.},
doi = {10.1016/j.ress.2006.04.015},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Borgonovo - 2007 - A new uncertainty importance measure.pdf:pdf},
isbn = {0951-8320},
issn = {09518320},
journal = {Reliability Engineering and System Safety},
keywords = {Global sensitivity analysis,Importance measures,Probabilistic risk assessment,Uncertainty analysis,Uncertainty importance measures,moment independence},
mendeley-tags = {moment independence},
number = {6},
pages = {771--784},
title = {{A new uncertainty importance measure}},
volume = {92},
year = {2007}
}
@phdthesis{Lavril2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Melorose, J. and Perroy, R. and Careas, S. and Lavril, Thibaut Jean Philippe},
booktitle = {Statewide Agricultural Land Use Baseline 2015},
doi = {10.1145/2064942.2064944.},
eprint = {arXiv:1011.1669v3},
isbn = {9781450311694},
issn = {1098-6596},
keywords = {icle},
pmid = {25246403},
school = {Universidade Federal do Rio de Janeiro},
title = {{A PROBABILISTIC FRAMEWORK FOR UNCERTAINTY QUANTIFICATION IN LARGE-SCALE SIMULATIONS: APPLICATION IN SEISMIC IMAGING}},
volume = {1},
year = {2015}
}
@article{Aggarwal2013,
author = {Aggarwal, Charu C.},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Aggarwal - 2013 - A Survey of Uncertain Data Clustering Algorithms.pdf:pdf},
journal = {Data Clustering: Algorithms and Applications},
number = {5},
pages = {455--480},
title = {{A Survey of Uncertain Data Clustering Algorithms}},
volume = {21},
year = {2013}
}
@article{Kiureghian2009,
abstract = {The sources and characters of uncertainties in engineering modeling for risk and reliability analyses are discussed. While many sources of uncertainty may exist, they are generally categorized as either aleatory or epistemic. Uncertainties are characterized as epistemic, if the modeler sees a possibility to reduce them by gathering more data or by refining models. Uncertainties are categorized as aleatory if the modeler does not foresee the possibility of reducing them. From a pragmatic standpoint, it is useful to thus categorize the uncertainties within a model, since it then becomes clear as to which uncertainties have the potential of being reduced. More importantly, epistemic uncertainties may introduce dependence among random events, which may not be properly noted if the character of uncertainties is not correctly modeled. Influences of the two types of uncertainties in reliability assessment, codified design, performance-based engineering and risk-based decision-making are discussed. Two simple examples demonstrate the influence of statistical dependence arising from epistemic uncertainties on systems and time-variant reliability problems. ?? 2008 Elsevier Ltd. All rights reserved.},
author = {Kiureghian, Armen Der and Ditlevsen, Ove},
doi = {10.1016/j.strusafe.2008.06.020},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Kiureghian, Ditlevsen - 2009 - Aleatory or epistemic Does it matter.pdf:pdf},
isbn = {0167-4730},
issn = {01674730},
journal = {Structural Safety},
keywords = {Aleatory,Epistemic,Ergodicity,Parameter uncertainty,Predictive models,Probability distribution choice,Statistical dependence,Systems,Time-variant reliability,Uncertainty},
month = {mar},
number = {2},
pages = {105--112},
title = {{Aleatory or epistemic? Does it matter?}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167473008000556},
volume = {31},
year = {2009}
}
@article{Bilkova2014,
author = {B{\'{i}}lkov{\'{a}}, Diana and Republic, Czech},
doi = {10.11648/j.pamj.20140302.11},
file = {:Users/nmlemus/Google Drive/Noel{\_}Uncertainty/Papers/32019714q2077.pdf:pdf},
issn = {2326-9790},
keywords = {and tl-moments,distribution function,income distribution,l-moments and tl-moments of,mikrocensus,order statistics,probability density function,probability distribution,quantile function,sample l-moments},
number = {2},
pages = {77--94},
title = {{Alternative Means of Statistical Data Analysis : L-Moments and TL-Moments of Probability Distributions}},
volume = {94},
year = {2014}
}
@article{Shirangi2016,
abstract = {Parameterization based on truncated singular value decomposition (TSVD) of the dimensionless sensitivity matrix has been shown to be an efficient approach for history matching. With TSVD parameterization, the search direction is computed as a linear combination of a few principal right singular vectors. As the sensitivity matrix is not explicitly computed, this parameterization is appropriate for large-scale history-matching problems. Moreover, previous work presented theoretical evidence that TSVD of the dimensionless sensitivity matrix provides the optimal parameterization in terms of uncertainty reduction. TSVD has been used in the randomized maximum likelihood (RML) framework to generate multiple conditional realizations of reservoir models. In this work, we investigate the effect of TSVD in the search direction obtained by the application of the Gauss–Newton and the Levenberg–Marquardt (LM) methods. In particular, we show that the TSVD-based LM algorithm converges to appropriate estimates because it gradually resolves the important features of the true model. We also introduce an improved implementation of a TSVD-based LM algorithm for generating multiple realizations of reservoir models conditioned to production data. Our experiments indicate that the computational cost of the new implementation is on the order of 2/3 of the cost of the previous implementation.},
author = {Shirangi, Mehrdad G. and Emerick, Alexandre A.},
doi = {10.1016/j.petrol.2016.02.026},
issn = {09204105},
journal = {Journal of Petroleum Science and Engineering},
pages = {258--271},
title = {{An improved TSVD-based Levenberg-Marquardt algorithm for history matching and comparison with Gauss-Newton}},
volume = {143},
year = {2016}
}
@article{Bretthorst1996,
abstract = {Probability theory as logic is founded on three simple desiderata: that degrees of belief should be represented by real numbers, that one should reason consistently, and that the theory should reduce to Aristotelian logic when the truth values of the hypotheses are known. Because this theory represents a probability as a state of knowledge, not a state of nature, hypotheses such as $\backslash$The frequency of oscillation of a sinusoidal signal had value ! when the data were taken," or $\backslash$Model x is a better description of the data than model y" make perfect sense. Problems of the first type are generally thought of as parameter estimation problems, while problems of the second type are thought of as model selection problems. However, in probability theory there is no essential distinction between these two types of problems. They are both solved by application of the sum and product rules of probability theory. Model selection problems are conceptually more difficult, because the models may have different functional forms. Consequently, conceptual difficulties enter the problem that are not present in parameter estimation. This paper is a tutorial on model selection. The conceptual problems that arise in model selection will be illustrated in such a way as to automatically avoid any difficulties. A simple example is worked in detail. This example,(radar target identification) illustrates all of the points of principle that must be faced in more complex model selection problems, including how to handle nuisance parameters, uninformative prior probabilities, and incomplete sets of models.},
author = {Bretthorst, G Larry},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Bretthorst - 1996 - An Introduction to model selection using probability theory as logic.pdf:pdf},
journal = {Maximum Entropy and Bayesian Methods},
keywords = {Bayes},
pages = {1--42},
title = {{An Introduction to model selection using probability theory as logic}},
year = {1996}
}
@book{Quak2007,
author = {Quak, Ewald},
booktitle = {Geometric Modelling, Numerical Simulation, and Optimization: Applied Mathematics at SINTEF},
doi = {10.1007/978-3-540-68783-2},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Quak - 2007 - An Introduction to the Numerics of Flow in Porous Media using Matlab.pdf:pdf},
isbn = {9783540687825},
number = {December},
pages = {5--10},
title = {{An Introduction to the Numerics of Flow in Porous Media using Matlab}},
year = {2007}
}
@book{Bettencourt2012,
author = {Bettencourt, Ricardo and Bulska, Ewa and Godlewska-{\.{z}}y{\l}kiewicz, Beata and Papadakis, Ioannis and Patriarca, Marina and Vassileva, Emilia and Taylor, Philip},
doi = {10.2787/5825},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Bettencourt et al. - 2012 - Analytical measurement measurement uncertainty and statistics.pdf:pdf},
isbn = {9789279230714},
pages = {240},
title = {{Analytical measurement : measurement uncertainty and statistics}},
url = {http://www.jrc.ec.europa.eu},
year = {2012}
}
@book{Bettencourt2012a,
author = {Bettencourt, Ricardo and Bulska, Ewa and Godlewska-{\.{z}}y{\l}kiewicz, Beata and Papadakis, Ioannis and Patriarca, Marina and Vassileva, Emilia and Taylor, Philip},
doi = {10.2787/5825},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Bettencourt et al. - 2012 - Analytical measurement measurement uncertainty and statistics.pdf:pdf},
isbn = {9789279230714},
pages = {240},
title = {{Analytical measurement : measurement uncertainty and statistics}},
url = {http://www.jrc.ec.europa.eu},
year = {2012}
}
@book{Sirovich1996,
abstract = {This is the first of three volumes on partial differential equations. It introduces basic examples of partial differential equations, arising in continuum mechanics, electromagnetism, complex analysis and other areas, and develops a number of tools for their solution, including particularly Fourier analysis, distribution theory, and Sobolev spaces. These tools are applied to the treatment of basic problems in linear PDE, including the Laplace equation, heat equation, and wave equation, as well as more general elliptic, parabolic, and hyperbolic equations. Volume I prepares the way for studies of more advanced topics in linear PDE, in Volume 2, and for studies of nonlinear PDE, in Volume 3. The book is addressed to graduate students in mathematics and to professional mathematicians, with an interest in partial differential equations, mathematical physics, differential geometry, harmonic analysis, and complex analysis.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Sirovich, Antman J E Marsden L and Holmes, Hale P and Keller, J Keener J and Mielke, B J Matkowsky A and Sreenivasan, C S Peskin K R S},
booktitle = {Applied Mathematical Sciences},
doi = {10.1007/978-1-4419-7055-8},
eprint = {arXiv:1011.1669v3},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Sirovich et al. - 1996 - Applied Mathematical Sciences.pdf:pdf},
isbn = {0387404376},
issn = {00255572},
number = {399},
pages = {80},
pmid = {25246403},
title = {{Applied Mathematical Sciences}},
url = {http://books.google.com/books?id=0xtSyLjsphkC{\&}pgis=1},
volume = {115},
year = {1996}
}
@article{Artijn2010,
author = {Artijn, I Mramsjah M and Om, J O H N V a N D E R B},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Artijn, Om - 2010 - ASSESSING SEISMIC UNCERTAINTY VIA GEOSTATISTICAL VELOCITY MODEL PERTURBATION AND IMAGE REGISTRATION AN APPLICATION T.pdf:pdf},
isbn = {9789039354513},
number = {3},
pages = {765--772},
title = {{ASSESSING SEISMIC UNCERTAINTY VIA GEOSTATISTICAL VELOCITY MODEL PERTURBATION AND IMAGE REGISTRATION: AN APPLICATION TO SUB-SALT IMAGING}},
volume = {7},
year = {2010}
}
@book{VVUQ2012,
author = {Council, National Research},
doi = {10.17226/13395},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Council - 2012 - Assessing the Reliability of Complex Models Mathematical and Statistical Foundations of Verification, Validation, and U.pdf:pdf},
isbn = {978-0-309-25634-6},
number = {February},
publisher = {National Academies Press},
title = {{Assessing the Reliability of Complex Models: Mathematical and Statistical Foundations of Verification, Validation, and Uncertainty Quantification}},
year = {2012}
}
@article{Kennedy2001a,
author = {Kennedy, Marc C. and O'Hagan, Anthony},
doi = {10.1111/1467-9868.00294},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Kennedy, O'Hagan - 2001 - Bayesian calibration of computer models.pdf:pdf},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {Calibration,Computer experiments,Deterministic models,Gaussian process,Interpolation,Model inadequacy,Sensitivity analysis,Uncertainty analysis},
month = {aug},
number = {3},
pages = {425--464},
publisher = {Blackwell Publishers Ltd.},
title = {{Bayesian calibration of computer models}},
url = {http://doi.wiley.com/10.1111/1467-9868.00294},
volume = {63},
year = {2001}
}
@article{Kennedy2001,
author = {Kennedy, Marc C. and O'Hagan, Anthony},
doi = {10.1111/1467-9868.00294},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Kennedy, O'Hagan - 2001 - Bayesian calibration of computer models.pdf:pdf},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {Calibration,Computer experiments,Deterministic models,Gaussian process,Interpolation,Model inadequacy,Sensitivity analysis,Uncertainty analysis},
month = {aug},
number = {3},
pages = {425--464},
publisher = {Blackwell Publishers Ltd.},
title = {{Bayesian calibration of computer models}},
url = {http://doi.wiley.com/10.1111/1467-9868.00294},
volume = {63},
year = {2001}
}
@article{NICHOLLS2004,
author = {NICHOLLS, GK},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/NICHOLLS - 2004 - Bayesian Inference and Markov Chain Monte Carlo by Example.pdf:pdf},
journal = {Kuopio, Finland: Workshop on Bayesian {\ldots}},
keywords = {and phrases,and ville kolehmainen for,bayesian inference,examples,i thank jari kaipio,inviting me to give,lecture notes,mcmc,these lectures},
title = {{Bayesian Inference and Markov Chain Monte Carlo by Example}},
url = {http://www.math.auckland.ac.nz/{~}nicholls/707/NichollsKuopio04.pdf},
year = {2004}
}
@article{Chkrebtii2016,
abstract = {We explore probability modelling of discretization uncertainty for system states defined implicitly by ordinary or partial differential equations. Accounting for this uncertainty can avoid posterior under-coverage when likelihoods are constructed from a coarsely discretized approximation to system equations. A formalism is proposed for inferring a fixed but a priori unknown model trajectory through Bayesian updating of a prior process conditional on model information. A one-step-ahead sampling scheme for interrogating the model is described, its consistency and first order convergence properties are proved, and its computational complexity is shown to be proportional to that of numerical explicit one-step solvers. Examples illustrate the flexibility of this framework to deal with a wide variety of complex and large-scale systems. Within the calibration problem, discretization uncertainty defines a layer in the Bayesian hierarchy, and a Markov chain Monte Carlo algorithm that targets this posterior distribution is presented. This formalism is used for inference on the JAK-STAT delay differential equation model of protein dynamics from indirectly observed measurements. The discussion outlines implications for the new field of probabilistic numerics.},
archivePrefix = {arXiv},
arxivId = {1306.2365},
author = {Chkrebtii, Oksana A. and Campbell, David A. and Calderhead, Ben and Girolami, Mark A.},
doi = {10.1214/16-BAXXX},
eprint = {1306.2365},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Chkrebtii et al. - 2016 - Bayesian Solution Uncertainty Quantification for Differential Equations.pdf:pdf},
issn = {1936-0975},
journal = {International Society for Bayesian Analysis},
keywords = {Bayesian numerical analysis,Gaussian processes,bayesian numerical analysis,differential equation models,gaussian,processes,uncertainty in computer models,uncertainty quantification},
pages = {1--29},
title = {{Bayesian Solution Uncertainty Quantification for Differential Equations}},
url = {http://arxiv.org/abs/1306.2365},
year = {2016}
}
@article{Papadimitriou2014,
author = {Papadimitriou, Costas},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Papadimitriou - 2014 - Bayesian Uncertainty Quantification and Propagation in Structural Dynamics.pdf:pdf},
isbn = {9789727521654},
journal = {Proceedings of the 9th International Conference on Structural Dynamics, EURODYN 2014},
keywords = {component mode synthesis,hpc,laplace asymptotics,large-order models,mcmc,surrogate models},
number = {July},
pages = {111--124},
title = {{Bayesian Uncertainty Quantification and Propagation in Structural Dynamics}},
year = {2014}
}
@article{Feinberg2015,
abstract = {The paper describes the philosophy, design, functionality, and usage of the Python software toolbox Chaospy for performing uncertainty quantification via polynomial chaos expansions and Monte Carlo simulation. The paper compares Chaospy to similar packages and demonstrates a stronger focus on defining reusable software building blocks that can easily be assembled to construct new, tailored algorithms for uncertainty quantification. For example, a Chaospy user can in a few lines of high-level computer code define custom distributions, polynomials, integration rules, sampling schemes, and statistical metrics for uncertainty analysis. In addition, the software introduces some novel methodological advances, like a framework for computing Rosenblatt transformations and a new approach for creating polynomial chaos expansions with dependent stochastic variables.},
author = {Feinberg, Jonathan and Langtangen, Hans Petter},
doi = {10.1016/j.jocs.2015.08.008},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Feinberg, Langtangen - 2015 - Chaospy An open source tool for designing methods of uncertainty quantification.pdf:pdf},
issn = {18777503},
journal = {Journal of Computational Science},
keywords = {Monte Carlo simulation,Polynomial chaos expansions,Python package,Rosenblatt transformations,Uncertainty quantification},
pages = {46--57},
publisher = {Elsevier B.V.},
title = {{Chaospy: An open source tool for designing methods of uncertainty quantification}},
volume = {11},
year = {2015}
}
@article{Oden2010,
abstract = {Having developed and calibrated a model to be used for a particular prediction (see Part I of this article, SIAM News, November 2010, page 1), we are ready to begin the validation process—that is, to assess the suitability of the calibrated model for the prediction.},
author = {Oden, J. Tinsley and Moser, Robert and Ghattas, Omar},
journal = {SIAM News},
number = {10},
pages = {2008--2011},
title = {{Computer predictions with quantified uncertainty, Part II}},
volume = {43},
year = {2010}
}
@article{Helton2009,
abstract = {In 2001, the National Nuclear Security Administration of the U.S. Department of Energy in conjunction with the national security laboratories (i.e, Los Alamos National Laboratory, Lawrence Livermore National Laboratory and Sandia National Laboratories) initiated development of a process designated Quantification of Margins and Uncer- tainty (QMU) for the use of risk assessment methodologies in the certification of the reliability and safety of the nation's nuclear weapons stockpile. This presentation discusses and illustrates the conceptual and computational basis of QMU in analyses that use computational models to predict the behavior of complex systems. Topics consid- ered include (i) the role of aleatory and epistemic uncertainty in QMU, (ii) the representation of uncertainty with probability, (iii) the probabilistic representation of uncertainty in QMU analyses involving only epistemic uncer- tainty, (iv) the probabilistic representation of uncertainty in QMU analyses involving aleatory and epistemic uncer- tainty, (v) procedures for sampling-based uncertainty and sensitivity analysis, (vi) the representation of uncertainty with alternatives to probability such as interval analysis, possibility theory and evidence theory, (vii) the representa- tion of uncertainty with alternatives to probability in QMU analyses involving only epistemic uncertainty, and (viii) the representation of uncertainty with alternatives to probability in QMU analyses involving aleatory and epistemic uncertainty. Concepts and computational procedures are illustrated with both notional examples and examples from reactor safety and radioactive waste disposal.},
annote = {Explica bastante bien todo el proceso, solo que habla de insertezas parametricas basicamente. Habla de las formas alternaticas de cuantificar la inserteza, como teoria de las posibilidades, etc.
Es algo viejo por lo que hay que buscar referencias mas recientes.},
author = {Helton, JC},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Helton - 2009 - Conceptual and computational basis for the quantification of margins and uncertainty.pdf:pdf},
keywords = {Aleatory uncertainty,Epistemic uncertainty,Performance assessment,Quantification of margins and uncertainty,Risk assessment,Sensitivity analysis,Uncertainty analysis},
number = {June},
title = {{Conceptual and computational basis for the quantification of margins and uncertainty.}},
url = {http://www.osti.gov/energycitations/product.biblio.jsp?osti{\_}id=958189},
year = {2009}
}
@inproceedings{Lavril2016,
author = {Lavril, Thibault and Mattoso, Marta and Costa, Danilo and Rochinha, Fernando A and Miras, Thomas},
booktitle = {Proceedings of the XXXVII Iberian Latin-American Congress on Computational Methods in Engineering},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Lavril et al. - 2016 - CONTROLLING PARALLEL ADAPTIVE SPARSE GRID STOCHASTIC COLLOCATION SIMULATIONS WITH.pdf:pdf},
title = {{CONTROLLING PARALLEL ADAPTIVE SPARSE GRID STOCHASTIC COLLOCATION SIMULATIONS WITH}},
year = {2016}
}
@inproceedings{Lavril2016,
author = {Lavril, Thibault and Mattoso, Marta and Costa, Danilo and Rochinha, Fernando A and Miras, Thomas},
booktitle = {Proceedings of the XXXVII Iberian Latin-American Congress on Computational Methods in Engineering},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Lavril et al. - 2016 - CONTROLLING PARALLEL ADAPTIVE SPARSE GRID STOCHASTIC COLLOCATION SIMULATIONS WITH.pdf:pdf},
title = {{CONTROLLING PARALLEL ADAPTIVE SPARSE GRID STOCHASTIC COLLOCATION SIMULATIONS WITH}},
year = {2016}
}
@article{Deshpande2017,
author = {Deshpande, S. and Watson, L. T. and Shu, J. and Kamke, F. A. and Ramakrishnan, N.},
doi = {10.1007/s00366-010-0192-8},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Deshpande et al. - 2011 - Data driven surrogate-based optimization in the problem solving environment WBCSim.pdf:pdf},
issn = {01770667},
journal = {Engineering with Computers},
keywords = {Experiment management,Optimization,Problem solving environment,Response surface approximation,Sequential approximate optimization,Surrogate,Trust region strategy,Visualization,Wood-based composite materials},
number = {3},
pages = {211--223},
title = {{Data driven surrogate-based optimization in the problem solving environment WBCSim}},
volume = {27},
year = {2011}
}
@article{Singh2007,
abstract = {The inherent uncertainty of data present in numerous applications such as sensor databases, text annotations, and information retrieval motivate the need to handle imprecise data at the database level. Uncertainty can be at the attribute or tuple level and is present in both continuous and discrete data domains. This paper presents a model for handling arbitrary probabilistic uncertain data (both discrete and continuous) natively at the database level. Our approach leads to a natural and efficient representation for probabilistic data. We develop a model that is consistent with possible worlds semantics and closed under basic relational operators. This is the first model that accurately and efficiently handles both continuous and discrete uncertainty. The model is implemented in a real database system (PostgreSQL) and the effectiveness and efficiency of our approach is validated experimentally.},
author = {Singh, Sarvjeet and Mayfield, Chris and Shah, Rahul and Prabhakar, Sunil and Hambrusch, Susanne and Neville, Jennifer and Cheng, Reynold},
doi = {10.1109/ICDE.2008.4497514},
isbn = {9781424418374},
issn = {10844627},
journal = {Proceedings - International Conference on Data Engineering},
pages = {1053--1061},
title = {{Database support for probabilistic attributes and tuples}},
year = {2008}
}
@article{Ji2014,
abstract = {An earth system model has been developed at Beijing Normal University (Beijing Normal University Earth System Model, BNU-ESM); the model is based on several widely evaluated climate model components and is used to study mechanisms of ocean-atmosphere interactions, natural climate variability and carbon-climate feedbacks at interannual to interdecadal time scales. In this paper, the model structure and individual components are described briefly. Further, results for the CMIP5 (Coupled Model Intercomparison Project phase 5) pre-industrial control and historical simulations are presented to demonstrate the model's performance in terms of the mean model state and the internal variability. It is illustrated that BNU-ESM can simulate many observed features of the earth climate system, such as the climatological annual cycle of surface-air temperature and precipitation, annual cycle of tropical Pacific sea surface temperature (SST), the overall patterns and positions of cells in global ocean meridional overturning circulation. For example, the El Ni{\~{n}}o-Southern Oscillation (ENSO) simulated in BNU-ESM exhibits an irregular oscillation between 2 and 5 years with the seasonal phase locking feature of ENSO. Important biases with regard to observations are presented and discussed, including warm SST discrepancies in the major upwelling regions, an equatorward drift of midlatitude westerly wind bands, and tropical precipitation bias over the ocean that is related to the double Intertropical Convergence Zone (ITCZ).},
author = {Ji, D. and Wang, L. and Feng, J. and Wu, Q. and Cheng, H. and Zhang, Q. and Yang, J. and Dong, W. and Dai, Y. and Gong, D. and Zhang, R. H. and Wang, X. and Liu, J. and Moore, J. C. and Chen, D. and Zhou, M.},
doi = {10.5194/gmd-7-2039-2014},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Ji et al. - 2014 - Description and basic evaluation of Beijing Normal University Earth System Model (BNU-ESM) version 1.pdf:pdf},
isbn = {1991-9603},
issn = {19919603},
journal = {Geoscientific Model Development},
number = {5},
pages = {2039--2064},
title = {{Description and basic evaluation of Beijing Normal University Earth System Model (BNU-ESM) version 1}},
volume = {7},
year = {2014}
}
@article{Golias2012,
author = {Golia{\v{s}}, Marcel and Palen{\v{c}}{\'{a}}r, Rudolf},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Golia{\v{s}}, Palen{\v{c}}{\'{a}}r - 2012 - Determination of uncertainties for correlated input quantities by the Monte Carlo method.pdf:pdf},
issn = {12102709},
journal = {Acta Polytechnica},
keywords = {Correlation,Monte carlo method,Uncertainty of measurement},
number = {4},
pages = {57--61},
title = {{Determination of uncertainties for correlated input quantities by the Monte Carlo method}},
volume = {52},
year = {2012}
}
@article{Beskales2008,
abstract = {Uncertainty pervades many domains in our lives. Current real-life applications, e.g., location tracking using GPS devices or cell phones, multimedia feature extraction, and sensor data management, deal with different kinds of uncertainty. Finding the nearest neighbor objects to a given query point is an important query type in these applications. In this paper, we study the problem of finding objects with the highest marginal probability of being the nearest neighbors to a query object. We adopt a general uncertainty model allowing for data and query uncertainty. Under this model, we define new query semantics, and provide several efficient evaluation algorithms. We analyze the cost factors involved in query evaluation, and present novel techniques to address the trade-offs among these factors. We give multiple extensions to our techniques including handling dependencies among data objects, and answering threshold queries. We conduct an extensive experimental study to evaluate our techniques on both real and synthetic data. {\textcopyright} 2008 VLDB Endowment.},
author = {Beskales, George and Soliman, M.a. Mohamed A and Ilyas, I.F. Ihab F},
doi = {10.14778/1453856.1453895},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Beskales, Soliman, Ilyas - 2008 - Efficient search for the Top-k probable nearest neighbors in uncertain databases.pdf:pdf},
isbn = {0000000000000},
issn = {21508097 (ISSN)},
journal = {Proceedings of the VLDB Endowment},
number = {1},
pages = {326--339},
title = {{Efficient search for the Top-k probable nearest neighbors in uncertain databases}},
url = {http://www.vldb.org/pvldb/1/1453895.pdf{\%}5Cnhttp://www.scopus.com/inward/record.url?eid=2-s2.0-84859170480{\&}partnerID=40{\&}md5=7e1d372e3a2771480ff9f393dd13314e},
volume = {1},
year = {2008}
}
@article{DeBaar2012,
abstract = {Techniques for Uncertainty Quantification (UQ) suffer from the 'curse of dimen-sionality': the number of required evaluations of the simulation code increases rapidly as the number of uncertain variables increases. Fluid-Structure Interaction (FSI) problems can in-volve complex physics as well as a large number of random input variables. The objective of the current work is to mitigate the curse of dimensionality by including adjoint-based gradient information from the FSI problem. For a FSI problem we increase the number of random struc-ture variables from 1 to 16. We apply a UQ response surface technique known as Kriging, and observe the computational effort that is required to obtain a certain target accuracy. When in-cluding gradient information – a technique known as Gradient-Enhanced Kriging (GEK) – we find a speedup that increases with the number of random variables. For example, for 4 random variables we observe a speedup of 3.0, while for 16 random variables we observe a speedup of 9.8. We conclude that including gradient information can lead to significant speedups.},
author = {{De Baar}, Jouke H S and Scholcz, Thomas P and Verhoosel, Clemens V and Dwight, Richard P and {Van Zuijlen}, Alexander H and Bijl, Hester},
file = {:Users/nmlemus/Google Drive/Noel{\_}Uncertainty/Papers/GEK{\_}UQ{\_}ECCOMAS{\_}submitted.pdf:pdf},
isbn = {9783950353709},
journal = {European Congress on Computational Methods in Applied Sciences and Engineering},
keywords = {Fluid-structure interac-tion,Gradient-Enhanced Kriging,Uncertainty Quantification,adjoint gradients,response surfaces},
number = {Eccomas},
pages = {1--12},
title = {{Efficient Uncertainty Quantification With Gradient-Enhanced Kriging: Applications in Fsi}},
year = {2012}
}
@article{Oberkampf2002,
abstract = {This article develops a general framework for identifying error and uncertainty in computational simulations that deal with the numerical solution of a set of partial differential equations (PDEs). A comprehensive, new view of the general phases of modeling and simulation is proposed, consisting of the following phases: conceptual modeling of the physical system, mathematical modeling of the conceptual model, discretization and algorithm selection for the mathematical model, computer programming of the discrete model, numerical solution of the computer program model, and representation of the numerical solution. Our view incorporates the modeling and simulation phases that are recognized in the systems engineering and operations research communities, but it adds phases that are specific to the numerical solution of PDEs. In each of these phases, general sources of uncertainty, both aleatory and epistemic, and error are identified. Our general framework is applicable to any numerical discretization procedure for solving ODEs or PDEs. To demonstrate this framework, we describe a system-level example: the flight of an unguided, rocket-boosted, aircraft-launched missile. This example is discussed in detail at each of the six phases of modeling and simulation. Two alternative models of the flight dynamics are considered, along with aleatory uncertainty of the initial mass of the missile and epistemic uncertainty in the thrust of the rocket motor. We also investigate the interaction of modeling uncertainties and numerical integration error in the solution of the ordinary differential equations for the flight dynamics.},
author = {Oberkampf, William L. and DeLand, Sharon M. and Rutherford, Brian M. and Diegert, Kathleen V. and Alvin, Kenneth F.},
doi = {10.1016/S0951-8320(01)00120-X},
isbn = {0951-8320},
issn = {09518320},
journal = {Reliability Engineering {\&} System Safety},
keywords = {aleatory uncertainty,epistemic uncertainty,modeling,nondeterministic features,simulation,stochastic uncertainty,subjective uncertainty},
number = {3},
pages = {333--357},
title = {{Error and uncertainty in modeling and simulation}},
volume = {75},
year = {2002}
}
@article{Thakur2010,
abstract = {The life of turbine blades is central to the integrity of an aircraft engine. Turbine blades, when manufactured, inevitably exhibit some deviations in shape from the desired design specifications due to the influence of manufacturing variability. This manufacturing variability may in turn lead to variations in the expected life and performance of these blades. It becomes important therefore to understand and model the effect of manufacturing variability on turbine blade life. The present work proposes a methodology which employs an existing geometry manipulation technique, namely Free Form Deformation (FFD), to generate 3-d models of the probable manufactured blade shapes. FFD is employed in conjunction with optimization for morphing the base geometry to generate different probable manufactured blade shapes in a case where a limited number of measurements are available per blade to characterize these differences. Lifing estimations on these perturbed geometries show that the presence of variability due to manufacturing processes may result in a reduction of around 1.6{\%} in mean life relative to the designed life, and, a maximum relative reduction of around 3.6{\%}, for turbine blades manufactured over a span of one year.},
author = {Thakur, Nikita and Keane, A.J. and Nair, P.B.},
doi = {10.3850/978-981-08-5118-7},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Thakur, Keane, Nair - 2010 - Estimating the effect of Manufacturing Variability on Turbine Blade Life.pdf:pdf},
isbn = {9789810851187},
journal = {4th International Workshop on Reliable Engineering Computing (REC 2010)},
keywords = {fuzzy analysis,fuzzy finite elements,interval analysis,interval fields},
number = {Rec},
pages = {978--981},
title = {{Estimating the effect of Manufacturing Variability on Turbine Blade Life}},
url = {http://eprints.soton.ac.uk/141630/},
year = {2010}
}
@article{Fournier2007,
abstract = {The method of moments is a popular technique for estimating the parameters of a generalized lambda distribution (GLD), but published results suggest that the percentile method gives superior results. However, the percentile method cannot be implemented in an automatic fashion, and automatic methods, like the starship method, can lead to prohibitive execution time with large sample sizes. A new estimation method is proposed that is automatic (it does not require the use of special tables or graphs), and it reduces the computational time. Based partly on the usual percentile method, this new method also requires choosing which quantile u to use when fitting a GLD to data. The choice for u is studied and it is found that the best choice depends on the final goal of the modeling process. The sampling distribution of the new estimator is studied and compared to the sampling distribution of estimators that have been proposed. Naturally, all estimators are biased and here it is found that the bias becomes negligible with sample sizes n ≥ 2 × 103. The .025 and .975 quantiles of the sampling distribution are investigated, and the difference between these quantiles is found to decrease proportionally to 1 / sqrt(n). The same results hold for the moment and percentile estimates. Finally, the influence of the sample size is studied when a normal distribution is modeled by a GLD. Both bounded and unbounded GLDs are used and the bounded GLD turns out to be the most accurate. Indeed it is shown that, up to n = 106, bounded GLD modeling cannot be rejected by usual goodness-of-fit tests. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
author = {Fournier, B. and Rupin, N. and Bigerelle, M. and Najjar, D. and Iost, A. and Wilcox, R.},
doi = {10.1016/j.csda.2006.09.043},
file = {:Users/nmlemus/Google Drive/Noel{\_}Uncertainty/Papers/MSMP{\_}CSDA{\_}IOST{\_}2007.pdf:pdf},
isbn = {0167-9473},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {Estimating distributions,GLD,Goodness-of-fit,Percentiles,Simplex},
number = {6},
pages = {2813--2835},
title = {{Estimating the parameters of a generalized lambda distribution}},
volume = {51},
year = {2007}
}
@article{Ob2000,
author = {Oberkampf, W L and Deland, Sharon M and Rutherford, Brian M and Diegert, Kathleen V and Alvin, Kenneth F},
journal = {Reliability Engineering and System Safety},
number = {April},
pages = {333--357},
title = {{Estimation of total uncertainty in modeling and simulation}},
volume = {75},
year = {2002}
}
@article{Williamson2014a,
abstract = {We develop Bayesian dynamic linear model Gaussian processes for emulation of time series output for computer models that may exhibit chaotic behavior, but where this behavior retains some underlying structure. The statistical technology is particularly suited to emulating the time series output of large climate models that exhibit this feature and where we want samples from the posterior of the emulator to evolve in the same way as dynamic processes in the computer model do. The methodology combines key features of good uncertainty quantification (UQ) methods such as using complex mean functions to capture large-scale signals within parameter space, with dynamic linear models in a way that allows UQ to borrow strength from the Bayesian time series literature. We present an MCMC algorithm for sampling from the posterior of the emulator parameters when the roughness lengths of the Gaussian process are unknown. We discuss an interpretation of the results of this algorithm that allows us to use MCMC to fix th...},
author = {Williamson, Daniel and Blaker, Adam T.},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Williamson, Blaker - 2014 - Evolving Bayesian Emulators for Structured Chaotic Time Series, with Application to Large Climate Models.pdf:pdf},
journal = {SIAM/ASA Journal on Uncertainty Quantification},
keywords = {37N10,60G15,60Gxx,Bayesian analysis,climate models,dynamic emulation,uncertainty quantification},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Evolving Bayesian Emulators for Structured Chaotic Time Series, with Application to Large Climate Models}},
year = {2014}
}
@article{Williamson2015,
author = {Williamson, D.},
doi = {10.1002/env.2335},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Williamson - 2015 - Exploratory ensemble designs for environmental models using k-extended Latin Hypercubes.pdf:pdf},
issn = {11804009},
journal = {Environmetrics},
month = {jun},
number = {4},
pages = {268--283},
title = {{Exploratory ensemble designs for environmental models using k-extended Latin Hypercubes}},
url = {http://doi.wiley.com/10.1002/env.2335},
volume = {26},
year = {2015}
}
@article{Zammit-Mangion2017,
abstract = {FRK is an R software package for spatial/spatio-temporal modelling and prediction with large datasets. It facilitates optimal spatial prediction (kriging) on the most commonly used manifolds (in Euclidean space and on the surface of the sphere), for both spatial and spatio-temporal fields. It differs from existing packages for spatial modelling and prediction by avoiding stationary and isotropic covariance and variogram models, instead constructing a spatial random effects (SRE) model on a discretised spatial domain. The discrete element is known as a basic areal unit (BAU), whose introduction in the software leads to several practical advantages. The software can be used to (i) integrate multiple observations with different supports with relative ease; (ii) obtain exact predictions at millions of prediction locations (without conditional simulation); and (iii) distinguish between measurement error and fine-scale variation at the resolution of the BAU, thereby allowing for improved uncertainty quantification when compared to related packages. The temporal component is included by adding another dimension. A key component of the SRE model is the specification of spatial or spatio-temporal basis functions; in the package, they can be generated automatically or by the user. The package also offers automatic BAU construction, an expectation maximisation (EM) algorithm for parameter estimation, and functionality for prediction over any user-specified polygons or BAUs. Use of the package is illustrated on several spatial and spatio-temporal datasets, and it is compared to two extensively used methods in spatial prediction and modelling, namely the package LatticeKrig and the stochastic partial differential equation tools in INLA.},
archivePrefix = {arXiv},
arxivId = {1705.08105},
author = {Zammit-Mangion, Andrew and Cressie, Noel},
eprint = {1705.08105},
file = {:Users/nmlemus/Google Drive/Noel{\_}Uncertainty/Papers/1705.08105.pdf:pdf},
keywords = {basic areal units,em algorithm,fixed rank kriging,spatial prediction,spatial ran-},
number = {2007},
title = {{FRK: An R Package for Spatial and Spatio-Temporal Prediction with Large Datasets}},
url = {http://arxiv.org/abs/1705.08105},
year = {2017}
}
@article{Yi2016a,
abstract = {Sensitivity analysis is a primary approach used in mathematical modeling to identify important factors that control the response dynamics in a model. In this paper, we applied the Morris sensitivity analysis method to identify the important factors governing the dynamics in a complex 3-dimensional water quality model. The water quality model was developed using the Environmental fluid dynamics code (EFDC) to simulate the fate and transport of nutrients and algal dynamics in Lake Dianchi, one of the most polluted large lakes in China. The analysis focused on the response of four water quality constituents, including chlorophyll-a, dissolved oxygen, total nitrogen, and total phosphorus, to 47 parameters and 7 external driving forces. We used Morris sensitivity analysis with different sample sizes and factor perturbation ranges to study the sensitivity with regard to different output metrics of the water quality model, and we analyzed the consistency between different sensitivity scenarios. In addition to the analysis with aggregate outputs, a spatiotemporal variability analysis was performed to understand the spatial heterogeneity and temporal distribution of sensitivities. Our results indicated that it is important to consider multiple characteristics in a sensitivity analysis, and we have identified a robust set of sensitive factors in the water quality model that will be useful for systematic model parameter identification and uncertainty analysis.},
author = {Yi, Xuan and Zou, Rui and Guo, Huaicheng},
doi = {10.1016/j.ecolmodel.2016.01.005},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Yi, Zou, Guo - 2016 - Global sensitivity analysis of a three-dimensional nutrients-algae dynamic model for a large shallow lake.pdf:pdf},
isbn = {03043800},
issn = {03043800},
journal = {Ecological Modelling},
keywords = {EFDC model,Morris screening,Sensitivity analysis,Spatiotemporal sensitivity indices,Water quality model},
pages = {74--84},
publisher = {Elsevier B.V.},
title = {{Global sensitivity analysis of a three-dimensional nutrients-algae dynamic model for a large shallow lake}},
url = {http://dx.doi.org/10.1016/j.ecolmodel.2016.01.005},
volume = {327},
year = {2016}
}
@article{Yi2016,
abstract = {Sensitivity analysis is a primary approach used in mathematical modeling to identify important factors that control the response dynamics in a model. In this paper, we applied the Morris sensitivity analysis method to identify the important factors governing the dynamics in a complex 3-dimensional water quality model. The water quality model was developed using the Environmental fluid dynamics code (EFDC) to simulate the fate and transport of nutrients and algal dynamics in Lake Dianchi, one of the most polluted large lakes in China. The analysis focused on the response of four water quality constituents, including chlorophyll-a, dissolved oxygen, total nitrogen, and total phosphorus, to 47 parameters and 7 external driving forces. We used Morris sensitivity analysis with different sample sizes and factor perturbation ranges to study the sensitivity with regard to different output metrics of the water quality model, and we analyzed the consistency between different sensitivity scenarios. In addition to the analysis with aggregate outputs, a spatiotemporal variability analysis was performed to understand the spatial heterogeneity and temporal distribution of sensitivities. Our results indicated that it is important to consider multiple characteristics in a sensitivity analysis, and we have identified a robust set of sensitive factors in the water quality model that will be useful for systematic model parameter identification and uncertainty analysis.},
author = {Yi, Xuan and Zou, Rui and Guo, Huaicheng},
doi = {10.1016/j.ecolmodel.2016.01.005},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Yi, Zou, Guo - 2016 - Global sensitivity analysis of a three-dimensional nutrients-algae dynamic model for a large shallow lake.pdf:pdf},
issn = {03043800},
journal = {Ecological Modelling},
keywords = {EFDC model,Morris screening,Sensitivity analysis,Spatio-tenporal Sensitivity Analysis,Spatiotemporal sensitivity indices,Water quality model},
mendeley-tags = {Spatio-tenporal Sensitivity Analysis},
pages = {74--84},
publisher = {Elsevier B.V.},
title = {{Global sensitivity analysis of a three-dimensional nutrients-algae dynamic model for a large shallow lake}},
url = {http://dx.doi.org/10.1016/j.ecolmodel.2016.01.005},
volume = {327},
year = {2016}
}
@book{Tobergte2013,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Tobergte, David R. and Curtis, Shirley},
booktitle = {Journal of Chemical Information and Modeling},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:Users/nmlemus/Google Drive/Noel{\_}Uncertainty/Books/Zaven A Karian{\_} Edward J Dudewicz-Handbook of fitting statistical distributions with R-CRC Press (2011).pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{Handbook of fitting statistical distributions with R}},
volume = {53},
year = {2013}
}
@book{Higdon2017,
abstract = {These lecture notes highlight the mathematical and computational structure relating to the formulation of, and development of algorithms for, the Bayesian approach to inverse problems in differential equations. This approach is fundamental in the quantification of uncertainty within applications in volving the blending of mathematical models with data. The finite dimensional situation is described first, along with some motivational examples. Then the development of probability measures on separable Banach space is undertaken, using a random series over an infinite set of functions to construct draws; these probability measures are used as priors in the Bayesian approach to inverse problems. Regularity of draws from the priors is studied in the natural Sobolev or Besov spaces implied by the choice of functions in the random series construction, and the Kolmogorov continuity theorem is used to extend regularity considerations to the space of H{\"{o}}lder continuous functions. Bayes' theorem is de rived in this prior setting, and here interpreted as finding conditions under which the posterior is absolutely continuous with respect to the prior, and determining a formula for the Radon-Nikodym derivative in terms of the likelihood of the data. Having established the form of the posterior, we then describe various properties common to it in the infinite dimensional setting. These properties include well-posedness, approximation theory, and the existence of maximum a posteriori estimators. We then describe measure-preserving dynamics, again on the infinite dimensional space, including Markov chain-Monte C arlo and sequential Monte Carlo methods, and measure-preserving reversible stochastic differential equations. By formulating the theory and algorithms on the underlying infinite dimensional space, we obtain a framework suitable for rigorous analysis of the accuracy of reconstructions, of computational complexity, as well as naturally constructing algorithms which perform well under mesh refinement, since they are inherently well-defined in infinite dimensions.},
archivePrefix = {arXiv},
arxivId = {1507.00398},
author = {Higdon, David},
doi = {10.1007/978-3-319-12385-1},
eprint = {1507.00398},
file = {:Users/nmlemus/Google Drive/Noel{\_}Uncertainty/Books/Roger Ghanem, David Higdon, Houman Owhadi (eds.)-Handbook of Uncertainty Quantification-Springer International Publishing (2017).pdf:pdf},
isbn = {978-3-319-12384-4},
title = {{Handbook of Uncertainty Quantification}},
url = {http://link.springer.com/10.1007/978-3-319-12385-1},
year = {2017}
}
@article{Stratos2011,
abstract = {Database management systems (DBMS) provide incredible flexibility and performance when it comes to query processing, scalability and accuracy. To fully exploit DBMS features, however, the user must define a schema, load the data, tune the system for the expected workload, and answer several questions. Should the database use a column-store, a row-store or some hybrid format? What indices should be created? All these questions make for a formidable and time-consuming hurdle, often deterring new applications or imposing high cost to existing ones. A characteristic example is that of scientific databases with huge data sets. The prohibitive initialization cost and complexity still forces scientists to rely on "ancient" tools for their data management tasks, delaying scientific understanding and progress. Users and applications collect their data in flat files, which have traditionally been considered to be "outside" a DBMS. A DBMS wants control: always bring all data "inside", replicate it and format it in its own "secret" way. The problem has been recognized and current efforts extend existing systems with abilities such as reading information from flat files and gracefully incorporating it into the processing engine. This paper proposes a new generation of systems where the only requirement from the user is a link to the raw data files. Queries can then immediately be fired without preparation steps in between. Internally and in an abstract way, the system takes care of selectively, adaptively and incrementally providing the proper environment given the queries at hand. Only part of the data is loaded at any given time and it is being stored and accessed in the format suitable for the current workload.},
author = {{Idreos Ioannis Alagiannis Ryan Johnson Anastasia Ailamaki CWI}, Stratos and Idreos, Stratos and Alagiannis, Ioannis and Johnson, Ryan and Ailamaki, Anastasia},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Idreos Ioannis Alagiannis Ryan Johnson Anastasia Ailamaki CWI et al. - 2011 - Here are my Data Files. Here are my Queries.Where are my R.pdf:pdf},
journal = {CIDR '11: Fifth Biennial Conference on Innovative Data Systems Research},
pages = {57--68},
title = {{Here are my Data Files. Here are my Queries.Where are my Results?}},
year = {2011}
}
@article{GharibShirangi2014,
abstract = {For large-scale history matching problems, applying the Gauss–Newton (GN) or the Levenberg–Marquardt (LM) algorithm is computationally expensive. However, these algorithms can be efficiently applied with parameterization based on a truncated singular value decomposition (SVD) of a dimensionless sensitivity matrix, where a truncated SVD is computed by using the Lanczos method. The SVD parameterization algorithm has been previously combined with randomized maximum likelihood (RML) to simultaneously generate multiple realizations of the reservoir model. The resulting algorithm, called SVD-EnRML, has been applied for simulation of permeability fields of 2D synthetic reservoirs. In this work, the SVD-EnRML algorithm is extended for the simulation of both porosity and permeability fields of 3D reservoirs. In the proposed extension, a dimensionless sensitivity matrix is defined for each set of correlated model parameters. A limitation of the original algorithm is due to the fact that a square root of the covariance matrix is required as a transformation from the original space to a dimensionless space. In this work, this limitation is resolved by introducing ensemble-based regularization based on utilizing an ensemble of unconditional realizations of the reservoir model. Although the proposed extension fits well within the original algorithm, a modified SVD-EnRML algorithm is introduced to mainly improve the computational efficiency. Computational results, composed of two different examples, show that the algorithm can be efficiently applied for the simulation of rock property fields and performance predictions of 3D reservoirs.},
author = {{Gharib Shirangi}, Mehrdad},
doi = {10.1016/j.petrol.2013.11.025},
issn = {09204105},
journal = {Journal of Petroleum Science and Engineering},
pages = {54--71},
title = {{History matching production data and uncertainty assessment with an efficient TSVD parameterization algorithm}},
volume = {113},
year = {2014}
}
@techreport{Pilch2006,
author = {Pilch, Martin and Trucano, T.G.},
booktitle = {Sandia Report},
number = {SAND2006-5001},
title = {{Ideas Underlying Quantification of Margins and Uncertainties (QMU): A White Paper}},
url = {http://www.stanford.edu/group/uq/docs/qmu{\_}ideas.pdf},
year = {2006}
}
@article{Noh2010,
abstract = {In RBDO, input uncertainty models such as marginal and joint cumulative$\backslash$ndistribution functions (CDFs) need to be used. However, only limited$\backslash$ndata exists in industry applications. Thus, identification of the$\backslash$ninput uncertainty model is challenging especially when input variables$\backslash$nare correlated. Since input random variables, such as fatigue material$\backslash$nproperties, are correlated in many industrial problems, the joint$\backslash$nCDF of correlated input variables needs to be correctly identified$\backslash$nfrom given data. In this paper, a Bayesian method is proposed to$\backslash$nidentify the marginal and joint CDFs from given data where a copula,$\backslash$nwhich only requires marginal CDFs and correlation parameters, is$\backslash$nused to model the joint CDF of input variables. Using simulated data$\backslash$nsets, performance of the Bayesian method is tested for different$\backslash$nnumbers of samples and is compared with the goodness-of-fit (GOF)$\backslash$ntest. Two examples are used to demonstrate how the Bayesian method$\backslash$nis used to identify correct marginal CDFs and copula.},
author = {Noh, Yoojeong and Choi, K. K. and Lee, Ikjin},
doi = {10.1007/s00158-009-0385-1},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Noh, Choi, Lee - 2010 - Identification of marginal and joint CDFs using Bayesian method for RBDO.pdf:pdf},
isbn = {0015800903},
issn = {1615147X},
journal = {Structural and Multidisciplinary Optimization},
keywords = {Bayesian method,Copula,Correlated input variables,Goodness-of-fit test,Identification of marginal and joint CDFs,Input model uncertainty,Reliability-based design optimization},
number = {1-6},
pages = {35--51},
title = {{Identification of marginal and joint CDFs using Bayesian method for RBDO}},
volume = {40},
year = {2010}
}
@article{Williamson2015a,
author = {Williamson, Daniel and Blaker, Adam T. and Hampton, Charlotte and Salter, James},
doi = {10.1007/s00382-014-2378-z},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Williamson et al. - 2015 - Identifying and removing structural biases in climate models with history matching.pdf:pdf},
issn = {0930-7575},
journal = {Climate Dynamics},
month = {sep},
number = {5-6},
pages = {1299--1324},
title = {{Identifying and removing structural biases in climate models with history matching}},
url = {http://link.springer.com/10.1007/s00382-014-2378-z},
volume = {45},
year = {2015}
}
@book{Sullivan2015,
author = {Sullivan, T. J.},
editor = {Springer},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Sullivan - 2015 - Introduction to Uncertainty Quantification.pdf:pdf},
isbn = {9783319233949},
publisher = {Springer},
title = {{Introduction to Uncertainty Quantification}},
url = {http://www.springer.com/series/1214},
year = {2015}
}
@article{Iglesias2014,
abstract = {In a Bayesian setting, inverse problems and uncertainty quantification (UQ) - the propagation of uncertainty through a computational (forward) model - are strongly connected. In the form of conditional expectation the Bayesian update becomes computationally attractive. This is especially the case as together with a functional or spectral approach for the forward UQ there is no need for time-consuming and slowly convergent Monte Carlo sampling. The developed sampling-free non-linear Bayesian update is derived from the variational problem associated with conditional expectation. This formulation in general calls for further discretisation to make the computation possible, and we choose a polynomial approximation. After giving details on the actual computation in the framework of functional or spectral approximations, we demonstrate the workings of the algorithm on a number of examples of increasing complexity. At last, we compare the linear and quadratic Bayesian update on the small but taxing example of the chaotic Lorenz 84 model, where we experiment with the influence of different observation or measurement operators on the update.},
archivePrefix = {arXiv},
arxivId = {1312.5048},
author = {Iglesias, Marco A and Stuart, Andrew M},
eprint = {1312.5048},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Iglesias, Stuart - 2014 - Inverse problems and uncertainty quantification.pdf:pdf},
journal = {SIAM News},
keywords = {60h15,60h25,62f15,62p30,65n21,74g75,80a23,classification,identification,inverse problem,msc2010,uncertainty quantification},
pages = {2--3},
title = {{Inverse problems and uncertainty quantification}},
url = {http://arxiv.org/abs/1312.5048},
year = {2014}
}
@article{Iglesias2014,
abstract = {In a Bayesian setting, inverse problems and uncertainty quantification (UQ) - the propagation of uncertainty through a computational (forward) model - are strongly connected. In the form of conditional expectation the Bayesian update becomes computationally attractive. This is especially the case as together with a functional or spectral approach for the forward UQ there is no need for time-consuming and slowly convergent Monte Carlo sampling. The developed sampling-free non-linear Bayesian update is derived from the variational problem associated with conditional expectation. This formulation in general calls for further discretisation to make the computation possible, and we choose a polynomial approximation. After giving details on the actual computation in the framework of functional or spectral approximations, we demonstrate the workings of the algorithm on a number of examples of increasing complexity. At last, we compare the linear and quadratic Bayesian update on the small but taxing example of the chaotic Lorenz 84 model, where we experiment with the influence of different observation or measurement operators on the update.},
archivePrefix = {arXiv},
arxivId = {1312.5048},
author = {Litvinenko, Alexander and Matthies, Hermann G. and Iglesias, Marco A and Stuart, Andrew M},
eprint = {1312.5048},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Iglesias, Stuart - 2014 - Inverse problems and uncertainty quantification.pdf:pdf},
journal = {SIAM News},
keywords = {60h15,60h25,62f15,62p30,65n21,74g75,80a23,classification,identification,inverse problem,msc2010,uncertainty quantification},
pages = {2--3},
title = {{Inverse problems and uncertainty quantification}},
url = {http://arxiv.org/abs/1312.5048},
year = {2013}
}
@article{Kawai2014,
author = {Kawai, Soshi and Shimoyama, Koji},
doi = {10.2514/6.2014-2737},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Kawai, Shimoyama - 2014 - Kriging-model-based uncertainty quantification in computational fluid dynamics.pdf:pdf},
isbn = {978-1-62410-288-2},
journal = {32nd AIAA Applied Aerodynamics Conference},
number = {June},
pages = {1--16},
title = {{Kriging-model-based uncertainty quantification in computational fluid dynamics}},
url = {http://arc.aiaa.org/doi/10.2514/6.2014-2737},
year = {2014}
}
@book{Lorenz2011,
abstract = {This book focuses on computational methods for large-scale statistical inverse problems and provides an introduction to statistical Bayesian and frequentist methodologies. Recent research advances for approximation methods are discussed, along with Kalman filtering methods and optimization-based approaches to solving inverse problems. The aim is to cross-fertilize the perspectives of researchers in the areas of data assimilation, statistics, large-scale optimization, applied and computational mathematics, high performance computing, and cutting-edge applications. The solution to large-scale inverse problems critically depends on methods to reduce computational cost. Recent research approaches tackle this challenge in a variety of different ways. Many of the computational frameworks highlighted in this book build upon state-of-the-art methods for simulation of the forward problem, such as, fast Partial Differential Equation (PDE) solvers, reduced-order models and emulators of the forward problem, stochastic spectral approximations, and ensemble-based approximations, as well as exploiting the machinery for large-scale deterministic optimization through adjoint and other sensitivity analysis methods. Key Features: • Brings together the perspectives of researchers in areas of inverse problems and data assimilation. • Assesses the current state-of-the-art and identify needs and opportunities for future research. • Focuses on the computational methods used to analyze and simulate inverse problems. • Written by leading experts of inverse problems and uncertainty quantification. Graduate students and researchers working in statistics, mathematics and engineering will benefit from this book.},
author = {{Lorenz Biegler, George Biros}, Omar Ghattas},
doi = {10.1002/9780470685853},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Lorenz Biegler, George Biros - 2011 - Large-Scale Inverse Problems and Quantification of Uncertainty.pdf:pdf},
isbn = {1119957583},
pages = {388},
title = {{Large-Scale Inverse Problems and Quantification of Uncertainty}},
url = {https://books.google.com/books?hl=en{\&}lr={\&}id=YPPKipz9qccC{\&}pgis=1},
year = {2011}
}
@book{Schlick2006,
abstract = {The development of high performance, massively parallel computers and the increasing demands of computationally challenging applications have ne-cessitated the development of scalable solvers and preconditioners. One of the most effective ways to achieve scalability is the use of multigrid or multilevel techniques. Algebraic multigrid (AMG) is a very efficient algorithm for solving large problems on unstructured grids. While much of it can be parallelized in a straightforward way, some components of the classical algorithm, particularly the coarsening process and some of the most efficient smoothers, are highly sequential, and require new par-allel approaches. This chapter presents the basic principles of AMG and gives an overview of various parallel implementations of AMG, including descriptions of par-allel coarsening schemes and smoothers, some numerical results as well as references to existing software packages.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Schlick, Timothy J. Barth Michael Griebel David E. Keyes RistoM. Nieminen Dirk Roose Tamar},
booktitle = {Numer. Solu. Partial Differ. Equ. Parallel Comput.},
doi = {10.1007/3-540-31619-1},
eprint = {arXiv:1011.1669v3},
file = {:Users/nmlemus/Google Drive/Noel{\_}Uncertainty/Books/(Lecture Notes in Computational Science and Engineering 92) Timothy Barth (auth.), Hester Bijl, Didier Lucor, Siddhartha Mishra, Christoph Schwab (eds.)-Uncertainty Quantification in Computational Flu.pdf:pdf},
isbn = {3-540-29076-1},
issn = {1439-7358},
pages = {209--236},
pmid = {25246403},
title = {{Lecture Notes in Computational Science and Engineering}},
volume = {51},
year = {2006}
}
@article{Zio2013,
author = {Zio, Enrico and Pedroni, Nicola},
doi = {10.3406/mcm.1987.945},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Zio, Pedroni - 2013 - Literature review of methods for representing uncertainty.pdf:pdf},
issn = {2100-3874},
journal = {Fondation pour une Culture de S{\'{e}}curit{\'{e}} Industrielle, Toulouse, France},
number = {1},
pages = {77--88},
title = {{Literature review of methods for representing uncertainty}},
url = {http://www.persee.fr/web/revues/home/prescript/article/mcm{\_}0755-8287{\_}1987{\_}num{\_}5{\_}1{\_}945},
volume = {5},
year = {2013}
}
@phdthesis{Goncalves2015,
author = {Gon{\c{c}}alves, Bernardo},
title = {{Managing large-scale scientific hypotheses as uncertain and probabilistic data}},
year = {2015}
}
@article{Ailamaki2010,
abstract = {Needed are generic, rather than one-off, DBMS solutions automating storage and analysis of data from scientific collaborations.},
author = {Ailamaki, Anastasia and Kantere, Verena and Dash, Debabrata},
doi = {10.1145/1743546.1743568},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Ailamaki, Kantere, Dash - 2010 - Managing scientific data.pdf:pdf},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {archiving},
number = {6},
pages = {68},
title = {{Managing scientific data}},
volume = {53},
year = {2010}
}
@article{Zhang2008,
abstract = {Uncertain data are inherent in many important applications. Recently, considerable research efforts have been put into the field of managing uncertain data. In this paper, we summarize existing techniques to query and model uncertain data and systems that effectively manage uncertain data, mainly from a probabilistic point of view.},
author = {Zhang, Wenjie and Lin, Xuemin and Pei, Jian and Zhang, Ying},
doi = {10.1109/WAIM.2008.42},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2008 - Managing uncertain data Probabilistic approaches.pdf:pdf},
isbn = {9780769531854},
journal = {Proceedings - The 9th International Conference on Web-Age Information Management, WAIM 2008},
pages = {405--412},
title = {{Managing uncertain data: Probabilistic approaches}},
year = {2008}
}
@inproceedings{Bernecker2011a,
abstract = {Many spatial query problems defined on uncertain data are computationally expensive, in particular, if in addition to spatial attributes, a time component is added. Although there exists a wide range of applications dealing with uncertain spatio-temporal data, there is no solution for efficient management of such data available yet. This paper is the first work to propose general models for spatio-temporal uncertain data that have the potential to allow efficient processing on a wide range of queries. The main challenge here is to unfold this potential by developing new algorithms based on these models. In addition, we give examples of interesting spatio-temporal queries on uncertain data.},
address = {New York, New York, USA},
author = {Bernecker, Thomas and Emrich, Tobias and Kriegel, Hans-Peter and Zuefle, Andreas and Chen, Lei and Lian, Xiang and Mamoulis, Nikos},
booktitle = {Proceedings of the 2nd ACM SIGSPATIAL International Workshop on Querying and Mining Uncertain Spatio-Temporal Data - QUeST '11},
doi = {10.1145/2064969.2064972},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Bernecker et al. - 2011 - Managing uncertain spatio-temporal data.pdf:pdf},
isbn = {9781450310376},
number = {c},
pages = {16--20},
publisher = {ACM Press},
title = {{Managing uncertain spatio-temporal data}},
url = {http://dl.acm.org/citation.cfm?doid=2064969.2064972 http://dl.acm.org/citation.cfm?doid=2064969.2064972{\%}5Cnhttp://dl.acm.org/citation.cfm?doid=2064969.2064972{\%}5Cnhttp://dl.acm.org/citation.cfm?id=2064972},
year = {2011}
}
@inproceedings{Bernecker2011,
abstract = {Many spatial query problems defined on uncertain data are computationally expensive, in particular, if in addition to spatial attributes, a time component is added. Although there exists a wide range of applications dealing with uncertain spatio-temporal data, there is no solution for efficient management of such data available yet. This paper is the first work to propose general models for spatio-temporal uncertain data that have the potential to allow efficient processing on a wide range of queries. The main challenge here is to unfold this potential by developing new algorithms based on these models. In addition, we give examples of interesting spatio-temporal queries on uncertain data.},
address = {New York, New York, USA},
author = {Bernecker, Thomas and Emrich, Tobias and Kriegel, Hans-Peter and Zuefle, Andreas and Chen, Lei and Lian, Xiang and Mamoulis, Nikos},
booktitle = {Proceedings of the 2nd ACM SIGSPATIAL International Workshop on Querying and Mining Uncertain Spatio-Temporal Data - QUeST '11},
doi = {10.1145/2064969.2064972},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Bernecker et al. - 2011 - Managing uncertain spatio-temporal data.pdf:pdf},
isbn = {9781450310376},
number = {c},
pages = {16--20},
publisher = {ACM Press},
title = {{Managing uncertain spatio-temporal data}},
url = {http://dl.acm.org/citation.cfm?doid=2064969.2064972 http://dl.acm.org/citation.cfm?doid=2064969.2064972{\%}5Cnhttp://dl.acm.org/citation.cfm?id=2064972},
year = {2011}
}
@article{Haas,
author = {Haas, Peter J and Jermaine, Chris},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Haas, Jermaine - 2014 - MCDB and SimSQL Scalable Stochastic Analytics within the Database.pdf:pdf},
title = {{MCDB and SimSQL : Scalable Stochastic Analytics within the Database}},
year = {2014}
}
@book{Salicone2000,
author = {Salicone, Simona},
booktitle = {Methods},
file = {:Users/nmlemus/Google Drive/Noel{\_}Uncertainty/Books/(Springer Series in Reliability Engineering) Simona Salicone (auth.)-Measurement Uncertainty{\_} An Approach via the Mathematical Theory of Evidence-Springer US (2007).pdf:pdf},
isbn = {9780387306551},
title = {{Measurement Uncertainty An Approach via the Mathematical Theory of Evidence}},
year = {2000}
}
@article{Arnaut2008,
author = {Arnaut, L. R.},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Arnaut - 2008 - Measurement uncertainty in reverberation chambers - I. Sample statistics.pdf:pdf},
issn = {1754-2995},
journal = {NPL Technical Report TQE 2, 2nd. ed., sec. 4.1.2.2},
number = {2},
title = {{Measurement uncertainty in reverberation chambers - I. Sample statistics}},
volume = {TQE},
year = {2008}
}
@article{Kumar2015d,
abstract = {Advanced analytics is a booming area in both industry and academia. Several projects aim to implement ML algorithms efficiently. But three key challenging and iterative practical tasks in using ML – feature engi-neering, algorithm selection, and parameter tuning, collectively called model selection – have largely been overlooked by the data management community even though these are often the most time-consuming tasks for analysts. To make the iterative process of model se-lection easier and faster, we envision a unifying abstract framework that acts a basis for a new class of analytics systems that we call model selection management sys-tems (MSMS). We discuss how time-tested ideas from database research offer new avenues to improve model selection, and outline how MSMS are a new frontier for interesting and impactful data management research.},
author = {Kumar, Arun and Mccann, Robert and Naughton, Jeffrey and Patel, Jignesh M.},
doi = {10.1145/2935694.2935698},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Kumar et al. - 2015 - Model Selection Management Systems The Next Frontier of Advanced Analytics.pdf:pdf},
issn = {0163-5808},
journal = {ACM SIGMOD Record},
keywords = {Feature Engineering,Iterative Model Selection,Performance Optimization,Provenance for Machine Learning,Usability of Machine Learning},
month = {may},
number = {4},
pages = {17--22},
publisher = {ACM},
title = {{Model Selection Management Systems: The Next Frontier of Advanced Analytics}},
url = {http://dl.acm.org/citation.cfm?doid=2935694.2935698 http://pages.cs.wisc.edu/{~}arun/vision/ http://dl.acm.org/citation.cfm?doid=2935694.2935698{\%}5Cnhttp://pages.cs.wisc.edu/{~}arun/vision/},
volume = {44},
year = {2015}
}
@article{Kumar2015d,
abstract = {Advanced analytics is a booming area in both industry and academia. Several projects aim to implement ML algorithms efficiently. But three key challenging and iterative practical tasks in using ML – feature engi-neering, algorithm selection, and parameter tuning, collectively called model selection – have largely been overlooked by the data management community even though these are often the most time-consuming tasks for analysts. To make the iterative process of model se-lection easier and faster, we envision a unifying abstract framework that acts a basis for a new class of analytics systems that we call model selection management sys-tems (MSMS). We discuss how time-tested ideas from database research offer new avenues to improve model selection, and outline how MSMS are a new frontier for interesting and impactful data management research.},
author = {Kumar, Arun and Mccann, Robert and Naughton, Jeffrey and Patel, Jignesh M.},
doi = {10.1145/2935694.2935698},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Kumar et al. - 2015 - Model Selection Management Systems The Next Frontier of Advanced Analytics.pdf:pdf},
issn = {0163-5808},
journal = {ACM SIGMOD Record},
keywords = {Feature Engineering,Iterative Model Selection,Performance Optimization,Provenance for Machine Learning,Usability of Machine Learning},
month = {may},
number = {4},
pages = {17--22},
publisher = {ACM},
title = {{Model Selection Management Systems: The Next Frontier of Advanced Analytics}},
url = {http://dl.acm.org/citation.cfm?doid=2935694.2935698 http://pages.cs.wisc.edu/{~}arun/vision/},
volume = {44},
year = {2015}
}
@article{Porto2011,
author = {Porto, Fabio and Moura, Ana Maria De C and Gon{\c{c}}alves, Bernardo and Costa, Ramon and Lustosa, Hermano and Corr{\^{e}}a, Frederico},
doi = {10.4018/JDM.2015040101},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Porto et al. - 2011 - Modeling and Implementing Scientific Hypotheses.pdf:pdf},
issn = {15338010},
keywords = {conceptual model,escience,scientific hypothesis},
title = {{Modeling and Implementing Scientific Hypotheses}},
year = {2011}
}
@book{Banks2014a,
author = {Banks, HT and Hu, S and Thompson, WC},
booktitle = {CRC Press},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Banks, Hu, Thompson - 2014 - Modeling and Inverse Problems in the Presence of Uncertainty.pdf:pdf},
isbn = {9781482206432},
title = {{Modeling and Inverse Problems in the Presence of Uncertainty}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=jZA-AwAAQBAJ{\&}oi=fnd{\&}pg=PP1{\&}dq=Modeling+and+Inverse+Problems+in+the+Presence+of+Uncertainty{\&}ots=HsI{\_}aNVlHR{\&}sig=4pbvydoz4JeqqEXXEYEqfl7uTvY},
year = {2014}
}
@book{Banks2014,
author = {Banks, HT and Hu, S and Thompson, WC},
booktitle = {CRC Press},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Banks, Hu, Thompson - 2014 - Modeling and Inverse Problems in the Presence of Uncertainty.pdf:pdf},
isbn = {9781482206432},
title = {{Modeling and Inverse Problems in the Presence of Uncertainty}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=jZA-AwAAQBAJ{\&}oi=fnd{\&}pg=PP1{\&}dq=Modeling+and+Inverse+Problems+in+the+Presence+of+Uncertainty{\&}ots=HsI{\_}aNVlHR{\&}sig=4pbvydoz4JeqqEXXEYEqfl7uTvY},
year = {2014}
}
@phdthesis{Dallachiesa2014,
author = {Dallachiesa, Michele},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Dallachiesa - 2014 - Modeling and Querying Data Series and Data Streams with Uncertainty.pdf:pdf},
number = {March},
school = {University of Trento},
title = {{Modeling and Querying Data Series and Data Streams with Uncertainty}},
url = {http://eprints-phd.biblio.unitn.it/1223/1/FinalSubmissionPhD.pdf},
year = {2014}
}
@article{Ceylan2016,
author = {Ceylan, Ismail Ilkan and Darwiche, Adnan and Broeck, Guy Van Den},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Ceylan, Darwiche, Broeck - 2016 - Open-World Probabilistic Databases.pdf:pdf},
journal = {Proc.$\backslash$ of KR'16},
title = {{Open-World Probabilistic Databases}},
year = {2016}
}
@article{Singh2008,
abstract = {Orion is a state-of-the-art uncertain database management system with built-in support for probabilistic data as first class data types. In contrast to other uncertain databases, Orion supports both attribute and tuple uncertainty with arbitrary correlations. This enables the database engine to handle both discrete and continuous pdfs in a natural and accurate manner. The underlying model is closed under the basic relational operators and is consistent with Possible Worlds Semantics. We demonstrate how Orion simplifies the design and enhances the capabilities of two example applications: managing sensor data (continuous uncertainty) and inferring missing values (discrete uncertainty).},
author = {Singh, Sarvjeet and Mayfield, Chris and Mittal, Sagar},
doi = {10.1145/1376616.1376744},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Singh, Mayfield, Mittal - 2008 - Orion 2.0 native support for uncertain data.pdf:pdf},
isbn = {9781605581026},
issn = {07308078},
journal = {In Proc. of the ACM Special Interest Group on Management of Data (SIGMOD 2008)},
pages = {1239--1241},
title = {{Orion 2.0: native support for uncertain data}},
url = {http://dl.acm.org/citation.cfm?id=1376744},
year = {2008}
}
@article{Kennedy2010,
abstract = {Estimation via sampling out of highly selective join queries is well known to be problematic, most notably in online aggregation. Without goal-directed sampling strategies, samples falling outside of the selection constraints lower estimation efficiency at best, and cause inaccurate estimates at worst This problem appears in general probabilistic database systems, where query processing is tightly coupled with sampling. By committing to a set of samples before evaluating the query, the engine wastes effort on samples that will be discarded, query processing that may need to be repeated, or unnecessarily large numbers of samples. We describe PIP, a general probabilistic database system that uses symbolic representations of probabilistic data to defer computation of expectations, moments, and other statistical measures until the expression to be measured is fully known. This approach is sufficiently general to admit both continuous and discrete distributions. Moreover, deferring sampling enables a broad range of goal-oriented sampling-based (as well as exact) integration techniques for computing expectations, allows the selection of the integration strategy most appropriate to the expression being measured, and can reduce the amount of sampling work required. We demonstrate the effectiveness of this approach by showing that even straightforward algorithms can make use of the added information. These algorithms have a profoundly positive impact on the efficiency and accuracy of expectation computations, particularly in the case of highly selective join queries.},
author = {Kennedy, Oliver and Koch, Christoph},
doi = {10.1109/ICDE.2010.5447879},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Kennedy, Koch - 2010 - PIP A database system for great and small expectations.pdf:pdf},
isbn = {9781424454440},
issn = {10844627},
journal = {Proceedings - International Conference on Data Engineering},
pages = {157--168},
title = {{PIP: A database system for great and small expectations}},
year = {2010}
}
@article{Deshpande,
author = {Deshpande, Amol},
file = {:Users/nmlemus/Dropbox/Noel{\_}Uncertainty/Papers/News/termpaper{\_}probabilistic.pdf:pdf},
keywords = {attribute-level uncertainty,cleaning of data,complexity class,conditional table,graphical model,inference algorithms,model counting problem,p,pos-,probabilistic conditional table},
title = {{Probabilistic Databases}}
}
@article{Dalvi2009,
abstract = {A wide range of applications have recently emerged that need to manage large, imprecise data sets. The reasons for imprecision in data are as diverse as the applications them- selves: in sensor and RFID data, imprecision is due to mea- surement errors [28,66]; in information extraction, impreci- sion comes from the inherent ambiguity in natural-language text [32,40]; and in business intelligence, imprecision is used to reduce the cost of data cleaning [12]. In some applications, such as privacy, it is a requirement that the data be less pre- cise. For example, imprecision is purposely inserted to hide sensitive attributes of individuals so that the data may be published [29,55,62]. Imprecise data has no place in tradi- tional, precise database applications like payroll and inven- tory, and so, current database management systems are not prepared to deal with it. In contrast, these newly emerging applications offer value precisely because they query, search, and aggregate large volumes of imprecise data to find the“di- amonds in the dirt”. This wide-variety of applications points to the need for generic tools to manage imprecise data. In this paper, we survey the state of the art techniques to han- dle imprecise data which models imprecision as probabilistic data [4,8,11,14,21,28,45,51,71]. A probabilistic database management system, or Prob- DMS, is a system that stores large volumes of probabilis- tic data and supports complex queries. A ProbDMS may also need to perform some additional tasks, such as updates or recovery, but these do not differ from those in conven- tional database management systems and will not be dis- cussed here. The major challenge in a ProbDMS is that it needs both to scale to large data volumes, a core com- petence of database management systems, and to do prob- abilistic inference, which is a problem studied in AI. While many scalable data management systems exists, probabilis- tic inference is in general a hard problem [68], and current systems do not scale to the same extent as datamanagement systems do. To address this challenge, researchers have fo- cused on the specific nature of relational probabilistic data, and exploited the special form of probabilistic inference that occurs during query evaluation. A number of such results have emerged recently: lineage-based representations [11], safe plans [18], algorithms for top-k queries [63,82], and rep- resentations of views over probabilistic data [65,67]. What is common to all these results is that they apply and extend well known concepts that are fundamental to data manage- ment, such as the separation of query and data when analyz- ing complexity [75], incomplete databases [44], the threshold algorithm [31], and the use of materialized views to answer queries [42, 74]. In this paper, we briefly survey the key concepts in probabilistic database systems, and explain the intellectual roots of these concepts in data management.},
author = {Dalvi, Nilesh and R{\'{e}}, Christopher and Suciu, Dan},
doi = {10.1145/1538788.1538810},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Dalvi, R{\'{e}}, Suciu - 2009 - Probabilistic databases diamonds in the dirt.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
number = {7},
pages = {86--94},
title = {{Probabilistic databases: diamonds in the dirt}},
url = {http://portal.acm.org/citation.cfm?doid=1538788.1538810{\%}5Cnhttp://dl.acm.org/citation.cfm?id=1538810},
volume = {52},
year = {2009}
}
@article{Do2015,
author = {Do, Overno and Do, Stado and De, I O and De, Ecretaria and De, Stado and De, Entro and Em, Rofissional},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Do et al. - 2015 - Proposta para um ambiente de gerenciamento de dados de predi{\c{c}}{\~{a}}o.pdf:pdf},
title = {{Proposta para um ambiente de gerenciamento de dados de predi{\c{c}}{\~{a}}o}},
year = {2015}
}
@article{Ericson2010,
author = {Ericson, Douglas},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Ericson - 2010 - QEF - Manual do Usu{\'{a}}rio Douglas Ericson Fabio Porto.pdf:pdf},
journal = {Scenario},
title = {{QEF - Manual do Usu{\'{a}}rio Douglas Ericson Fabio Porto}},
year = {2010}
}
@article{Wood-Schultz2011,
author = {Wood-Schultz, David H. Sharp and Merri M.},
number = {1},
pages = {55--82},
title = {{QMU and Nuclear Weapons Certification What's under the hood?}},
volume = {44},
year = {2011}
}
@article{Weber2011,
author = {Wood-Schultz, David H. Sharp {\&} Merri M.},
journal = {Los Alamos Science},
number = {1},
pages = {55--82},
title = {{QMU and Nuclear Weapons Certification-What's under the Hood?}},
volume = {44},
year = {2011}
}
@book{Andrade2003,
author = {Andrade, Eliana X L De},
booktitle = {Algoritmos Evolutivos},
doi = {10.5540/001.2012.0066.01},
file = {:Users/nmlemus/Google Drive/Noel{\_}Uncertainty/Books/livro{\_}81.pdf:pdf},
isbn = {9788582150245},
pages = {103},
title = {{Quantificacao De Incertezas E Estimacao De Parametros Em Dinamica Estrutural: Uma Introducao a Partir De Exemplos Computacionais}},
volume = {4},
year = {2003}
}
@article{Leader2005,
abstract = {JASON.Quantificationsofmarginsanduncertainties(QMU).JSR-04-3330. McLean, VA:TheMitreCorporation;2005.},
author = {Eardley},
isbn = {9780309128537},
journal = {JASON -The Mitre Corporation JASON report JSR-04-330},
title = {{Quantifications of Margins and Uncertainties}},
year = {2005}
}
@article{Li2016,
abstract = {Stratigraphic (or lithological) uncertainty refers to the uncertainty of boundaries between different soil layers and lithological units, which has received increasing attention in geotechnical engineering. In this paper, an effective stochastic geological modeling framework is proposed based on Markov random field theory, which is conditional on site investigation data, such as observations of soil types from ground surface, borehole logs, and strata orientation from geophysical tests. The proposed modeling method is capable of accounting for the inherent heterogeneous and anisotropic characteristics of geological structure. In this method, two modeling approaches are introduced to simulate subsurface geological structures to accommodate different confidence levels on geological structure type (i.e., layered vs others). The sensitivity analysis for two modeling approaches is conducted to reveal the influence of mesh density and the model parameter on the simulation results. Illustrative examples using borehole data are presented to elucidate the ability to quantify the geological structure uncertainty. Furthermore, the applicability of two modeling approaches and the behavior of the proposed model under different model parameters are discussed in detail. Finally, Bayesian inferential framework is introduced to allow for the estimation of the posterior distribution of model parameter, when additional or subsequent borehole information becomes available. Practical guidance of using the proposed stochastic geological modeling technique for engineering practice is given.},
annote = {The paper present two different models ICM and MCMC. The authors evaluate the results and compare the models based in the information entropy.

They use Bayes inference too, to update the probability of some parameters. To do this they use information of boreholes.

This paper could be used to exemplify the needs of Bayesian Inference (Inverse Problem) into our framework.},
author = {Li, Zhao and Wang, Xiangrong and Wang, Hui and Liang, Robert Y},
doi = {10.1016/j.enggeo.2015.12.017},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2016 - Quantifying stratigraphic uncertainties by stochastic simulation techniques based on Markov random field.pdf:pdf},
issn = {00137952},
journal = {Engineering Geology},
keywords = {Geological modeling,Markov random field,Soil heterogeneity,Stratigraphic uncertainty,Uncertainty quantification},
pages = {106--122},
publisher = {Elsevier B.V.},
title = {{Quantifying stratigraphic uncertainties by stochastic simulation techniques based on Markov random field}},
url = {http://dx.doi.org/10.1016/j.enggeo.2015.12.017},
volume = {201},
year = {2016}
}
@article{Citac2000,
abstract = {This Guide gives detailed guidance for the evaluation and expression of uncertainty in quantitative chemical analysis, based on the approach taken in the ISO Guide to the Expression of Uncertainty in Measurement H.2. It is applicable at all levels of accuracy and in all fields - from routine analysis to basic research and to empirical and rational methods (see section 5.3.). Some common areas in which chemical measurements are needed, and in which the principles of this Guide may be applied, are: Quality control and quality assurance in manufacturing industries. Testing for regulatory compliance. Testing utilising an agreed method. Calibration of standards and equipment. Measurements associated with the development and certification of reference materials. Research and development.},
author = {Citac and Eurachem},
doi = {0 948926 15 5},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Citac, Eurachem - 2000 - Quantifying Uncertainty in Analytical Measurement.pdf:pdf},
isbn = {0948926155},
journal = {English},
pages = {126},
title = {{Quantifying Uncertainty in Analytical Measurement}},
url = {http://www.measurementuncertainty.org/mu/QUAM2000-1.pdf},
volume = {2nd},
year = {2000}
}
@article{Citac2000a,
abstract = {This Guide gives detailed guidance for the evaluation and expression of uncertainty in quantitative chemical analysis, based on the approach taken in the ISO Guide to the Expression of Uncertainty in Measurement H.2. It is applicable at all levels of accuracy and in all fields - from routine analysis to basic research and to empirical and rational methods (see section 5.3.). Some common areas in which chemical measurements are needed, and in which the principles of this Guide may be applied, are: Quality control and quality assurance in manufacturing industries. Testing for regulatory compliance. Testing utilising an agreed method. Calibration of standards and equipment. Measurements associated with the development and certification of reference materials. Research and development.},
author = {Citac and Eurachem},
doi = {0 948926 15 5},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Citac, Eurachem - 2000 - Quantifying Uncertainty in Analytical Measurement.pdf:pdf},
isbn = {0948926155},
journal = {English},
pages = {126},
title = {{Quantifying Uncertainty in Analytical Measurement}},
url = {http://www.measurementuncertainty.org/mu/QUAM2000-1.pdf},
volume = {2nd},
year = {2000}
}
@incollection{Matthies2007,
address = {Dordrecht},
author = {Matthies, Hermann G.},
booktitle = {Extreme Man-Made and Natural Hazards in Dynamics of Structures},
doi = {10.1007/978-1-4020-5656-7_4},
pages = {105--135},
publisher = {Springer Netherlands},
title = {{QUANTIFYING UNCERTAINTY: MODERN COMPUTATIONAL REPRESENTATION OF PROBABILITY AND APPLICATIONS}},
url = {http://link.springer.com/10.1007/978-1-4020-5656-7{\_}4},
year = {2007}
}
@article{Partner2016,
author = {Partner, Lead and Revision, W U},
file = {:Users/nmlemus/Google Drive/Noel{\_}Uncertainty/Books/QUICS{\_}Deliverable{\_}2.1{\_}v4Final.pdf:pdf},
pages = {1--38},
title = {{QUICS : Quantifying Uncertainty in Integrated Catchment Studies 2 . 1 Software tools for quantifying uncertainty across different scales}},
year = {2016}
}
@article{Baxter2016,
abstract = {A simple, simulation-based model of temporal uncertainty is presented that embraces other approaches recently proposed in the literature, including those more usually involving mathematical calculation rather than simulation. More specifically, it is shown how the random generation of dates for events, conditioned by uncertain temporal knowledge of the true date, can be adapted to what has been called the chronological apportioning of artefact assemblages and aoristic analysis (as a temporal rather than spatio-temporal method). The methodology is in the same spirit - though there are differences - as that underpinning the use of summed radiocarbon dates. A possibly novel approach to representing temporal change is suggested. Ideas are illustrated using data extracted from a large corpus of late Iron Age and Roman brooches, where the focus of interest was on their temporal distribution over a period of about 450 years.},
author = {Baxter, M J and Cool, H E M},
doi = {10.1016/j.jas.2015.12.007},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Baxter, Cool - 2016 - Reinventing the wheel Modelling temporal uncertainty with applications to brooch distributions in Roman Britain.pdf:pdf},
issn = {10959238},
journal = {Journal of Archaeological Science},
keywords = {Aoristic,Brooches,Modelling,Roman,Simulation,Temporal change,Temporal uncertainty},
pages = {120--127},
publisher = {Elsevier Ltd},
title = {{Reinventing the wheel? Modelling temporal uncertainty with applications to brooch distributions in Roman Britain}},
volume = {66},
year = {2016}
}
@article{Helton2010,
abstract = {Procedures are described for the representation of results in analyses that involve both aleatory uncertainty and epistemic uncertainty, with aleatory uncertainty deriving from an inherent randomness in the behaviour of the system under study and epistemic uncertainty deriving from a lack of knowledge about the appropriate values to use for quantities that are assumed to have fixed but poorly known values in the context of a specific study. Aleatory uncertainty is usually represented with probability and leads to cumulative distribution functions (CDFs) or complementary CDFs (CCDFs) for analysis results of interest. Several mathematical structures are available for the representation of epistemic uncertainty, including interval analysis, possibility theory, evidence theory and probability theory. In the presence of epistemic uncertainty, there is not a single CDF or CCDF for a given analysis result. Rather, there is a family of CDFs and a corresponding family of CCDFs that derive from epistemic uncertainty and have an uncertainty structure that derives from the particular uncertainty structure (e.g. interval analysis, possibility theory, evidence theory or probability theory) used to represent epistemic uncertainty. Graphical formats for the representation of epistemic uncertainty in families of CDFs and CCDFs are investigated and presented for the indicated characterisations of epistemic uncertainty.$\backslash$nProcedures are described for the representation of results in analyses that involve both aleatory uncertainty and epistemic uncertainty, with aleatory uncertainty deriving from an inherent randomness in the behaviour of the system under study and epistemic uncertainty deriving from a lack of knowledge about the appropriate values to use for quantities that are assumed to have fixed but poorly known values in the context of a specific study. Aleatory uncertainty is usually represented with probability and leads to cumulative distribution functions (CDFs) or complementary CDFs (CCDFs) for analysis results of interest. Several mathematical structures are available for the representation of epistemic uncertainty, including interval analysis, possibility theory, evidence theory and probability theory. In the presence of epistemic uncertainty, there is not a single CDF or CCDF for a given analysis result. Rather, there is a family of CDFs and a corresponding family of CCDFs that derive from epistemic uncertainty and have an uncertainty structure that derives from the particular uncertainty structure (e.g. interval analysis, possibility theory, evidence theory or probability theory) used to represent epistemic uncertainty. Graphical formats for the representation of epistemic uncertainty in families of CDFs and CCDFs are investigated and presented for the indicated characterisations of epistemic uncertainty.},
author = {Helton, Jon C. and Johnson, Jay D. and Oberkampf, William L. and Sallaberry, C{\'{e}}dric J.},
doi = {10.1080/03081079.2010.486664},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Helton et al. - 2010 - Representation of analysis results involving aleatory and epistemic uncertainty.pdf:pdf},
issn = {0308-1079},
journal = {International Journal of General Systems},
number = {6},
pages = {605--646},
title = {{Representation of analysis results involving aleatory and epistemic uncertainty}},
volume = {39},
year = {2010}
}
@article{Helton2010,
abstract = {Procedures are described for the representation of results in analyses that involve both aleatory uncertainty and epistemic uncertainty, with aleatory uncertainty deriving from an inherent randomness in the behaviour of the system under study and epistemic uncertainty deriving from a lack of knowledge about the appropriate values to use for quantities that are assumed to have fixed but poorly known values in the context of a specific study. Aleatory uncertainty is usually represented with probability and leads to cumulative distribution functions (CDFs) or complementary CDFs (CCDFs) for analysis results of interest. Several mathematical structures are available for the representation of epistemic uncertainty, including interval analysis, possibility theory, evidence theory and probability theory. In the presence of epistemic uncertainty, there is not a single CDF or CCDF for a given analysis result. Rather, there is a family of CDFs and a corresponding family of CCDFs that derive from epistemic uncertainty and have an uncertainty structure that derives from the particular uncertainty structure (e.g. interval analysis, possibility theory, evidence theory or probability theory) used to represent epistemic uncertainty. Graphical formats for the representation of epistemic uncertainty in families of CDFs and CCDFs are investigated and presented for the indicated characterisations of epistemic uncertainty.$\backslash$nProcedures are described for the representation of results in analyses that involve both aleatory uncertainty and epistemic uncertainty, with aleatory uncertainty deriving from an inherent randomness in the behaviour of the system under study and epistemic uncertainty deriving from a lack of knowledge about the appropriate values to use for quantities that are assumed to have fixed but poorly known values in the context of a specific study. Aleatory uncertainty is usually represented with probability and leads to cumulative distribution functions (CDFs) or complementary CDFs (CCDFs) for analysis results of interest. Several mathematical structures are available for the representation of epistemic uncertainty, including interval analysis, possibility theory, evidence theory and probability theory. In the presence of epistemic uncertainty, there is not a single CDF or CCDF for a given analysis result. Rather, there is a family of CDFs and a corresponding family of CCDFs that derive from epistemic uncertainty and have an uncertainty structure that derives from the particular uncertainty structure (e.g. interval analysis, possibility theory, evidence theory or probability theory) used to represent epistemic uncertainty. Graphical formats for the representation of epistemic uncertainty in families of CDFs and CCDFs are investigated and presented for the indicated characterisations of epistemic uncertainty.},
author = {Helton, Jon C. and Johnson, Jay D. and Oberkampf, William L. and Sallaberry, C{\'{e}}dric J.},
doi = {10.1080/03081079.2010.486664},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Helton et al. - 2010 - Representation of analysis results involving aleatory and epistemic uncertainty.pdf:pdf},
issn = {0308-1079},
journal = {International Journal of General Systems},
keywords = {aleatory uncertainty,epistemic uncertainty,evidence theory,interval analysis,possibility theory,probability theory},
number = {6},
pages = {605--646},
publisher = {Taylor {\&} Francis Group},
title = {{Representation of analysis results involving aleatory and epistemic uncertainty}},
volume = {39},
year = {2010}
}
@article{Helton2010a,
abstract = {Procedures are described for the representation of results in analyses that involve both aleatory uncertainty and epistemic uncertainty, with aleatory uncertainty deriving from an inherent randomness in the behaviour of the system under study and epistemic uncertainty deriving from a lack of knowledge about the appropriate values to use for quantities that are assumed to have fixed but poorly known values in the context of a specific study. Aleatory uncertainty is usually represented with probability and leads to cumulative distribution functions (CDFs) or complementary CDFs (CCDFs) for analysis results of interest. Several mathematical structures are available for the representation of epistemic uncertainty, including interval analysis, possibility theory, evidence theory and probability theory. In the presence of epistemic uncertainty, there is not a single CDF or CCDF for a given analysis result. Rather, there is a family of CDFs and a corresponding family of CCDFs that derive from epistemic uncertainty and have an uncertainty structure that derives from the particular uncertainty structure (e.g. interval analysis, possibility theory, evidence theory or probability theory) used to represent epistemic uncertainty. Graphical formats for the representation of epistemic uncertainty in families of CDFs and CCDFs are investigated and presented for the indicated characterisations of epistemic uncertainty.$\backslash$nProcedures are described for the representation of results in analyses that involve both aleatory uncertainty and epistemic uncertainty, with aleatory uncertainty deriving from an inherent randomness in the behaviour of the system under study and epistemic uncertainty deriving from a lack of knowledge about the appropriate values to use for quantities that are assumed to have fixed but poorly known values in the context of a specific study. Aleatory uncertainty is usually represented with probability and leads to cumulative distribution functions (CDFs) or complementary CDFs (CCDFs) for analysis results of interest. Several mathematical structures are available for the representation of epistemic uncertainty, including interval analysis, possibility theory, evidence theory and probability theory. In the presence of epistemic uncertainty, there is not a single CDF or CCDF for a given analysis result. Rather, there is a family of CDFs and a corresponding family of CCDFs that derive from epistemic uncertainty and have an uncertainty structure that derives from the particular uncertainty structure (e.g. interval analysis, possibility theory, evidence theory or probability theory) used to represent epistemic uncertainty. Graphical formats for the representation of epistemic uncertainty in families of CDFs and CCDFs are investigated and presented for the indicated characterisations of epistemic uncertainty.},
author = {Helton, Jon C. and Johnson, Jay D. and Oberkampf, William L. and Sallaberry, C{\'{e}}dric J.},
doi = {10.1080/03081079.2010.486664},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Helton et al. - 2010 - Representation of analysis results involving aleatory and epistemic uncertainty.pdf:pdf},
issn = {0308-1079},
journal = {International Journal of General Systems},
keywords = {aleatory uncertainty,epistemic uncertainty,evidence theory,interval analysis,possibility theory,probability theory},
number = {6},
pages = {605--646},
publisher = {Taylor {\&} Francis Group},
title = {{Representation of analysis results involving aleatory and epistemic uncertainty}},
volume = {39},
year = {2010}
}
@article{Scheidt2009,
abstract = {Assessing uncertainty of a spatial phenomenon requires the analysis$\backslash$nof a large number of parameters which must be processed by a transfer$\backslash$nfunction. To capture the possibly of a wide range of uncertainty$\backslash$nin the transfer function response, a large set of geostatistical$\backslash$nmodel realizations needs to be processed. Stochastic spatial simulation$\backslash$ncan rapidly provide multiple, equally probable realizations. However,$\backslash$nsince the transfer function is often computationally demanding, only$\backslash$na small number of models can be evaluated in practice, and are usually$\backslash$nselected through a ranking procedure. Traditional ranking techniques$\backslash$nfor selection of probabilistic ranges of response (P10, P50 and P90)$\backslash$nare highly dependent on the static property used. In this paper,$\backslash$nwe propose to parameterize the spatial uncertainty represented by$\backslash$na large set of geostatistical realizations through a distance function$\backslash$nmeasuring dissimilarity between any two geostatistical realizations.$\backslash$nThe distance function allows a mapping of the space of uncertainty.$\backslash$nThe distance can be tailored to the particular problem. The multi-dimensional$\backslash$nspace of uncertainty can be modeled using kernel techniques, such$\backslash$nas kernel principal component analysis (KPCA) or kernel clustering.$\backslash$nThese tools allow for the selection of a subset of representative$\backslash$nrealizations containing similar properties to the larger set. Without$\backslash$nlosing accuracy, decisions and strategies can then be performed applying$\backslash$na transfer function on the subset without the need to exhaustively$\backslash$nevaluate each realization. This method is applied to a synthetic$\backslash$noil reservoir, where spatial uncertainty of channel facies is modeled$\backslash$nthrough multiple realizations generated using a multi-point geostatistical$\backslash$nalgorithm and several training images.},
annote = {In general they porpouse a method to select a set of representative realizations. This is, using a distance function we can clusterize all the realizations and select just one representative realization by cluster.},
author = {Scheidt, C{\'{e}}line and Caers, Jef},
doi = {10.1007/s11004-008-9186-0},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Scheidt, Caers - 2009 - Representing spatial uncertainty using distances and kernels.pdf:pdf},
isbn = {1100400891860},
issn = {18748961},
journal = {Mathematical Geosciences},
keywords = {Distance,Geostatistics,Kernel methods,Ranking,Uncertainty quantification},
number = {4},
pages = {397--419},
title = {{Representing spatial uncertainty using distances and kernels}},
volume = {41},
year = {2009}
}
@article{Paper2016,
author = {Paper, Conference and Gon, Bernardo and Machado, Fabio and Laborat, Porto},
isbn = {9781450319218},
number = {March},
title = {{Research Lattice: Towards a Data Model for Scientific Hypotheses}},
year = {2016}
}
@inproceedings{Goncalves2013,
address = {New York, New York, USA},
author = {Gon{\c{c}}alves, Bernardo and Porto, Fabio},
booktitle = {Proceedings of the 25th International Conference on Scientific and Statistical Database Management},
doi = {10.1145/2484838.2484861},
isbn = {978-1-4503-1921-8},
keywords = {large-scale science,lattice theory,research progress,scientific databases,scientific hypothesis},
pages = {41:1----41:4},
publisher = {ACM Press},
title = {{Research Lattices: Towards a Scientific Hypothesis Data Model}},
url = {http://dl.acm.org/citation.cfm?doid=2484838.2484861 http://doi.acm.org/10.1145/2484838.2484861},
year = {2013}
}
@article{Kidane2012,
abstract = {This work is concerned with establishing the feasibility of a data-on-demand (DoD) uncertainty quantification (UQ) protocol based on concentration-of-measure inequalities. Specific aims are to establish the feasibility of the protocol and its basic properties, including the tightness of the predictions afforded by the protocol. The assessment is based on an application to terminal ballistics and a specific system configuration consisting of 6061-T6 aluminum plates struck by spherical S-2 tool steel projectiles at ballistic impact speeds. The systems inputs are the plate thickness and impact velocity and the perforation area is chosen as the sole performance measure of the system. The objective of the UQ analysis is to certify the lethality of the projectile, i.e., that the projectile perforates the plate with high probability over a prespecified range of impact velocities and plate thicknesses. The net outcome of the UQ analysis is an M/U ratio, or confidence factor, of 2.93, indicative of a small probability of no perforation of the plate over its entire operating range. The high-confidence ({\textgreater}99.9{\%}) in the successful operation of the system afforded the analysis and the small number of tests (40) required for the determination of the modeling-error diameter, establishes the feasibility of the DoD UQ protocol as a rigorous yet practical approach for model-based certification of complex systems. ?? 2011 Elsevier Ltd. All rights reserved.},
author = {Kidane, A. and Lashgari, A. and Li, B. and McKerns, M. and Ortiz, M. and Owhadi, H. and Ravichandran, G. and Stalzer, M. and Sullivan, T. J.},
doi = {10.1016/j.jmps.2011.12.001},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Kidane et al. - 2012 - Rigorous model-based uncertainty quantification with application to terminal ballistics, part I Systems with cont.pdf:pdf},
isbn = {0022-5096},
issn = {00225096},
journal = {Journal of the Mechanics and Physics of Solids},
keywords = {Certification,Concentration of measure,Terminal ballistics,Uncertainty quantification},
number = {5},
pages = {983--1001},
title = {{Rigorous model-based uncertainty quantification with application to terminal ballistics, part I: Systems with controllable inputs and small scatter}},
volume = {60},
year = {2012}
}
@article{Kidane2012a,
abstract = {This work is concerned with establishing the feasibility of a data-on-demand (DoD) uncertainty quantification (UQ) protocol based on concentration-of-measure inequalities. Specific aims are to establish the feasibility of the protocol and its basic properties, including the tightness of the predictions afforded by the protocol. The assessment is based on an application to terminal ballistics and a specific system configuration consisting of 6061-T6 aluminum plates struck by spherical S-2 tool steel projectiles at ballistic impact speeds. The systems inputs are the plate thickness and impact velocity and the perforation area is chosen as the sole performance measure of the system. The objective of the UQ analysis is to certify the lethality of the projectile, i.e., that the projectile perforates the plate with high probability over a prespecified range of impact velocities and plate thicknesses. The net outcome of the UQ analysis is an M/U ratio, or confidence factor, of 2.93, indicative of a small probability of no perforation of the plate over its entire operating range. The high-confidence ({\textgreater}99.9{\%}) in the successful operation of the system afforded the analysis and the small number of tests (40) required for the determination of the modeling-error diameter, establishes the feasibility of the DoD UQ protocol as a rigorous yet practical approach for model-based certification of complex systems. ?? 2011 Elsevier Ltd. All rights reserved.},
author = {Kidane, A. and Lashgari, A. and Li, B. and McKerns, M. and Ortiz, M. and Owhadi, H. and Ravichandran, G. and Stalzer, M. and Sullivan, T. J.},
doi = {10.1016/j.jmps.2011.12.001},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Kidane et al. - 2012 - Rigorous model-based uncertainty quantification with application to terminal ballistics, part I Systems with cont.pdf:pdf},
isbn = {0022-5096},
issn = {00225096},
journal = {Journal of the Mechanics and Physics of Solids},
keywords = {Certification,Concentration of measure,Terminal ballistics,Uncertainty quantification},
number = {5},
pages = {983--1001},
title = {{Rigorous model-based uncertainty quantification with application to terminal ballistics, part I: Systems with controllable inputs and small scatter}},
volume = {60},
year = {2012}
}
@article{Hoare2008,
abstract = {SaSAT (Sampling and Sensitivity Analysis Tools) is a user-friendly software package for applying uncertainty and sensitivity analyses to mathematical and computational models of arbitrary complexity and context. The toolbox is built in Matlab, a numerical mathematical software package, and utilises algorithms contained in the Matlab Statistics Toolbox. However, Matlab is not required to use SaSAT as the software package is provided as an executable file with all the necessary supplementary files. The SaSAT package is also designed to work seamlessly with Microsoft Excel but no functionality is forfeited if that software is not available. A comprehensive suite of tools is provided to enable the following tasks to be easily performed: efficient and equitable sampling of parameter space by various methodologies; calculation of correlation coefficients; regression analysis; factor prioritisation; and graphical output of results, including response surfaces, tornado plots, and scatterplots. Use of SaSAT is exemplified by application to a simple epidemic model. To our knowledge, a number of the methods available in SaSAT for performing sensitivity analyses have not previously been used in epidemiological modelling and their usefulness in this context is demonstrated.},
author = {Hoare, Alexander and Regan, David G and Wilson, David P},
doi = {10.1186/1742-4682-5-4},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Hoare, Regan, Wilson - 2008 - Sampling and sensitivity analyses tools (SaSAT) for computational modelling.pdf:pdf},
isbn = {1742-4682 (Electronic)$\backslash$n1742-4682 (Linking)},
issn = {1742-4682},
journal = {Theoretical biology {\&} medical modelling},
pages = {4},
pmid = {18304361},
title = {{Sampling and sensitivity analyses tools (SaSAT) for computational modelling.}},
volume = {5},
year = {2008}
}
@article{Hoare2008a,
abstract = {SaSAT (Sampling and Sensitivity Analysis Tools) is a user-friendly software package for applying uncertainty and sensitivity analyses to mathematical and computational models of arbitrary complexity and context. The toolbox is built in Matlab, a numerical mathematical software package, and utilises algorithms contained in the Matlab Statistics Toolbox. However, Matlab is not required to use SaSAT as the software package is provided as an executable file with all the necessary supplementary files. The SaSAT package is also designed to work seamlessly with Microsoft Excel but no functionality is forfeited if that software is not available. A comprehensive suite of tools is provided to enable the following tasks to be easily performed: efficient and equitable sampling of parameter space by various methodologies; calculation of correlation coefficients; regression analysis; factor prioritisation; and graphical output of results, including response surfaces, tornado plots, and scatterplots. Use of SaSAT is exemplified by application to a simple epidemic model. To our knowledge, a number of the methods available in SaSAT for performing sensitivity analyses have not previously been used in epidemiological modelling and their usefulness in this context is demonstrated.},
author = {Hoare, Alexander and Regan, David G and Wilson, David P},
doi = {10.1186/1742-4682-5-4},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Hoare, Regan, Wilson - 2008 - Sampling and sensitivity analyses tools (SaSAT) for computational modelling.pdf:pdf},
isbn = {1742-4682 (Electronic)$\backslash$n1742-4682 (Linking)},
issn = {1742-4682},
journal = {Theoretical biology {\&} medical modelling},
pages = {4},
pmid = {18304361},
title = {{Sampling and sensitivity analyses tools (SaSAT) for computational modelling.}},
volume = {5},
year = {2008}
}
@article{Wick2010,
abstract = {Incorporating probabilities into the semantics of incomplete databases has posed many challenges, forcing systems to sacrifice modeling power, scalability, or treatment of relational algebra operators. We propose an alternative approach where the underlying relational database always represents a single world, and an external factor graph encodes a distribution over possible worlds; Markov chain Monte Carlo (MCMC) inference is then used to recover this uncertainty to a desired level of fidelity. Our approach allows the efficient evaluation of arbitrary queries over probabilistic databases with arbitrary dependencies expressed by graphical models with structure that changes during inference. MCMC sampling provides efficiency by hypothesizing modifications to possible worlds rather than generating entire worlds from scratch. Queries are then run over the portions of the world that change, avoiding the onerous cost of running full queries over each sampled world. A significant innovation of this work is the connection between MCMC sampling and materialized view maintenance techniques: we find empirically that using view maintenance techniques is several orders of magnitude faster than naively querying each sampled world. We also demonstrate our system's ability to answer relational queries with aggregation, and demonstrate additional scalability through the use of parallelization on a real-world complex model of information extraction. This framework is sufficiently expressive to support probabilistic inference not only for answering queries, but also for inferring missing database content from raw evidence.},
archivePrefix = {arXiv},
arxivId = {1005.1934},
author = {Wick, Michael and McCallum, A and Miklau, Gerome},
doi = {10.14778/1920841.1920942},
eprint = {1005.1934},
issn = {21508097},
journal = {Proceedings of the VLDB Endowment},
number = {1-2},
pages = {794--804},
title = {{Scalable probabilistic databases with factor graphs and mcmc}},
url = {papers2://publication/uuid/6327ACAE-8C67-4F02-80AD-ACD061F4E477},
volume = {3},
year = {2010}
}
@techreport{Energy2009,
author = {{U.S. Department of Energy}},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/U.S. Department of Energy - 2009 - Scientific Grand Challenges in National Security The Role of Computing at the Extreme Scale.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
pages = {255},
title = {{Scientific Grand Challenges in National Security: The Role of Computing at the Extreme Scale}},
year = {2009}
}
@article{Guerra2009,
author = {Guerra, Gabriel and Rochinha, Fernando and Elias, Renato and Coutinho, Alvaro and Braganholo, Vanessa and de Oliveira, Daniel and Ogasawara, Eduardo and Chirigati, Fernando and Mattoso, Marta},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Guerra et al. - 2009 - Scientific Workflow Management System Applied to Uncertainty Quantification in Large Eddy Simulation.pdf:pdf},
number = {November},
title = {{Scientific Workflow Management System Applied to Uncertainty Quantification in Large Eddy Simulation}},
year = {2009}
}
@article{Guerra2009a,
author = {Guerra, Gabriel and Rochinha, Fernando and Elias, Renato and Coutinho, Alvaro and Braganholo, Vanessa and de Oliveira, Daniel and Ogasawara, Eduardo and Chirigati, Fernando and Mattoso, Marta},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Guerra et al. - 2009 - Scientific Workflow Management System Applied to Uncertainty Quantification in Large Eddy Simulation.pdf:pdf},
number = {November},
title = {{Scientific Workflow Management System Applied to Uncertainty Quantification in Large Eddy Simulation}},
year = {2009}
}
@article{Guerra2009b,
author = {Guerra, Gabriel and Rochinha, Fernando and Elias, Renato and Coutinho, Alvaro and Braganholo, Vanessa and de Oliveira, Daniel and Ogasawara, Eduardo and Chirigati, Fernando and Mattoso, Marta},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Guerra et al. - 2009 - Scientific Workflow Management System Applied to Uncertainty Quantification in Large Eddy Simulation.pdf:pdf},
number = {November},
title = {{Scientific Workflow Management System Applied to Uncertainty Quantification in Large Eddy Simulation}},
year = {2009}
}
@article{Guerra2009c,
author = {Guerra, Gabriel and Rochinha, Fernando and Elias, Renato and Coutinho, Alvaro and Braganholo, Vanessa and de Oliveira, Daniel and Ogasawara, Eduardo and Chirigati, Fernando and Mattoso, Marta},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Guerra et al. - 2009 - Scientific Workflow Management System Applied to Uncertainty Quantification in Large Eddy Simulation.pdf:pdf},
number = {November},
title = {{Scientific Workflow Management System Applied to Uncertainty Quantification in Large Eddy Simulation}},
year = {2009}
}
@article{Farrell2015a,
author = {Farrell, Kathryn Anne},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Farrell - 2015 - Selection , Calibration , and Validation of Coarse-Grained Models of Atomistic Systems.pdf:pdf},
title = {{Selection , Calibration , and Validation of Coarse-Grained Models of Atomistic Systems}},
year = {2015}
}
@article{Oden2013,
abstract = {We address general approaches to the rational selection and validation of mathematical and computational models of tumor growth using methods of Bayesian inference. The model classes are derived from a general diffuse-interface, continuum mixture theory and focus on mass conservation of mixtures with up to four species. Synthetic data are generated using higher-order base models. We discuss general approaches to model cal- ibration, validation, plausibility, and selection based on Bayesian-based methods, infor- mation theory, and maximum information entropy.We also address computational issues and provide numerical experiments based on Markov chain Monte Carlo algorithms and high performance computing implementations.},
author = {Oden, J Tinsley and Prudencio, Ernesto E and Hawkins-Daarud, Andrea},
doi = {10.1142/S0218202513500103},
issn = {0218-2025},
journal = {Mathematical Models and Methods in Applied Sciences},
keywords = {Bayesian statistics,Markov chain Monte Carlo methods.,diffuse-interface models,model selec- tion,model validation},
number = {7},
pages = {1309--1338},
title = {{Selection and Assessment of Phenomenological Models of Tumor Growth}},
url = {http://www.worldscientific.com/doi/abs/10.1142/S0218202513500103},
volume = {23},
year = {2013}
}
@article{Pianosi2016,
abstract = {Sensitivity Analysis (SA) investigates how the variation in the output of a numerical model can be attributed to variations of its input factors. SA is increasingly being used in environmental modelling for a variety of purposes, including uncertainty assessment, model calibration and diagnostic evaluation, dominant control analysis and robust decision-making. In this paper we review the SA literature with the goal of providing: (i) a comprehensive view of SA approaches also in relation to other methodologies for model identification and application; (ii) a systematic classification of the most commonly used SA methods; (iii) practical guidelines for the application of SA. The paper aims at delivering an introduction to SA for non-specialist readers, as well as practical advice with best practice examples from the literature; and at stimulating the discussion within the community of SA developers and users regarding the setting of good practices and on defining priorities for future research.},
author = {Pianosi, Francesca and Beven, Keith and Freer, Jim and Hall, Jim W. and Rougier, Jonathan and Stephenson, David B. and Wagener, Thorsten},
doi = {10.1016/j.envsoft.2016.02.008},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Pianosi et al. - 2016 - Sensitivity analysis of environmental models A systematic review with practical workflow.pdf:pdf},
issn = {13648152},
journal = {Environmental Modelling {\&} Software},
keywords = {Calibration,Evaluation,Robust decision-making,Sensitivity Analysis,Spatio-tenporal Sensitivity Analysis,Uncertainty Analysis},
mendeley-tags = {Spatio-tenporal Sensitivity Analysis},
pages = {214--232},
publisher = {Elsevier Ltd},
title = {{Sensitivity analysis of environmental models: A systematic review with practical workflow}},
url = {http://dx.doi.org/10.1016/j.envsoft.2016.02.008 http://www.sciencedirect.com/science/article/pii/S1364815216300287},
volume = {79},
year = {2016}
}
@article{Fordham2016,
abstract = {Spatially explicit demographic models are increasingly being used to forecast the effect of global change on the range dynamics of species. These models are typically complex, with the structure and parameter values often estimated with considerable uncertainty. If not properly accounted, this can lead to bias or false precision in projections of changes to species range dynamics and extinction risk. Here we present a new open-source freeware tool, "Sensitivity Analysis of Range Dynamics Models" (SARDM) that provides an all-in-one approach for: (i) determining the implications of integrating complex and often uncertain information into spatially explicit demographic models compiled in RAMAS GIS, and (ii) identifying and ranking the relative importance of different sources of parameter uncertainty. The sensitivity and uncertainty analysis techniques built into SARDM will facilitate ecologists and conservation scientists in better establishing confidence in forecasts of range movement and abundance.},
author = {Fordham, Damien A. and Haythorne, Sean and Brook, Barry W.},
doi = {10.1016/j.envsoft.2016.05.020},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Fordham, Haythorne, Brook - 2016 - Sensitivity Analysis of Range Dynamics Models (SARDM) Quantifying the influence of parameter uncertai.pdf:pdf},
issn = {13648152},
journal = {Environmental Modelling and Software},
keywords = {Climate change,Coupled niche-population model,Metapopulation,Population viability analysis,Propagating uncertainty,Species distribution},
pages = {193--197},
publisher = {Elsevier Ltd},
title = {{Sensitivity Analysis of Range Dynamics Models (SARDM): Quantifying the influence of parameter uncertainty on forecasts of extinction risk from global change}},
volume = {83},
year = {2016}
}
@article{Fordham2016a,
abstract = {Spatially explicit demographic models are increasingly being used to forecast the effect of global change on the range dynamics of species. These models are typically complex, with the structure and parameter values often estimated with considerable uncertainty. If not properly accounted, this can lead to bias or false precision in projections of changes to species range dynamics and extinction risk. Here we present a new open-source freeware tool, "Sensitivity Analysis of Range Dynamics Models" (SARDM) that provides an all-in-one approach for: (i) determining the implications of integrating complex and often uncertain information into spatially explicit demographic models compiled in RAMAS GIS, and (ii) identifying and ranking the relative importance of different sources of parameter uncertainty. The sensitivity and uncertainty analysis techniques built into SARDM will facilitate ecologists and conservation scientists in better establishing confidence in forecasts of range movement and abundance.},
author = {Fordham, Damien A. and Haythorne, Sean and Brook, Barry W.},
doi = {10.1016/j.envsoft.2016.05.020},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Fordham, Haythorne, Brook - 2016 - Sensitivity Analysis of Range Dynamics Models (SARDM) Quantifying the influence of parameter uncertai.pdf:pdf},
issn = {13648152},
journal = {Environmental Modelling and Software},
keywords = {Climate change,Coupled niche-population model,Metapopulation,Population viability analysis,Propagating uncertainty,Species distribution},
pages = {193--197},
publisher = {Elsevier Ltd},
title = {{Sensitivity Analysis of Range Dynamics Models (SARDM): Quantifying the influence of parameter uncertainty on forecasts of extinction risk from global change}},
volume = {83},
year = {2016}
}
@article{Mullins2016,
abstract = {This paper investigates model validation under a variety of different data scenarios and clarifies how different validation metrics may be appropriate for different scenarios. In the presence of multiple uncertainty sources, model validation metrics that compare the distributions of model prediction and observation are considered. Both ensemble validation and point-by-point approaches are discussed, and it is shown how applying the model reliability metric point-by-point enables the separation of contributions from aleatory and epistemic uncertainty sources. After individual validation assessments are made at different input conditions, it may be desirable to obtain an overall measure of model validity across the entire domain. This paper proposes an integration approach that assigns weights to the validation results according to the relevance of each validation test condition to the overall intended use of the model in prediction. Since uncertainty propagation for probabilistic validation is often unaffordable for complex computational models, surrogate models are often used; this paper proposes an approach to account for the additional uncertainty introduced in validation by the uncertain fit of the surrogate model. The proposed methods are demonstrated with a microelectromechanical system (MEMS) example.},
author = {Mullins, Joshua and Ling, You and Mahadevan, Sankaran and Sun, Lin and Strachan, Alejandro},
doi = {10.1016/j.ress.2015.10.003},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Mullins et al. - 2016 - Separation of aleatory and epistemic uncertainty in probabilistic model validation.pdf:pdf},
issn = {09518320},
journal = {Reliability Engineering and System Safety},
keywords = {Epistemic uncertainty,Imprecise data,Model validation,Reliability,Validation metrics},
pages = {49--59},
title = {{Separation of aleatory and epistemic uncertainty in probabilistic model validation}},
volume = {147},
year = {2016}
}
@article{Cai2013,
abstract = {This paper describes the SimSQL system, which allows for SQL-based specification, simulation, and querying of database-valued Markov chains, i.e., chains whose value at any time step comprises the contents of an entire database. SimSQL extends the earlier Monte Carlo database system (MCDB), which permitted Monte Carlo simulation of static database-valued random variables. Like MCDB, SimSQL uses user-specified "VG functions" to generate the simulated data values that are the building blocks of a simulated database. The enhanced functionality of SimSQL is enabled by the ability to parametrize VG functions using stochastic tables, so that one stochastic database can be used to parametrize the generation of another stochastic database, which can parametrize another, and so on. Other key extensions include the ability to explicitly define recursive versions of a stochastic table and the ability to execute the simulation in a MapReduce environment. We focus on applying SimSQL to Bayesian machine learning. Copyright {\textcopyright} 2013 ACM.},
author = {Cai, Z.a and Vagena, Z.b and Perez, L.c and Arumugam, S.a and Haas, P.J.c and Jermaine, C.a},
doi = {10.1145/2463676.2465283},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Cai et al. - 2013 - Simulation of database-valued Markov chains using SimSQL.pdf:pdf},
isbn = {9781450320375},
issn = {07308078},
journal = {Proceedings of the ACM SIGMOD International Conference on Management of Data},
keywords = {any time step comprises,chains,chains whose value at,database,databases,e,i,machine learning,markov chains,simsql employs many of,tents of an entire,the con-,the ideas},
pages = {637--648},
title = {{Simulation of database-valued Markov chains using SimSQL}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880568698{\&}partnerID=40{\&}md5=756efc38f2843d16507a920c0a4feec9},
year = {2013}
}
@article{Alden2013,
abstract = {Integrating computer simulation with conventional wet-lab research has proven to have much potential in furthering the understanding of biological systems. Success requires the relationship between simulation and the real-world system to be established: substantial aspects of the biological system are typically unknown, and the abstract nature of simulation can complicate interpretation of in silico results in terms of the biology. Here we present spartan (Simulation Parameter Analysis R Toolkit ApplicatioN), a package of statistical techniques specifically designed to help researchers understand this relationship and provide novel biological insight. The tools comprising spartan help identify which simulation results can be attributed to the dynamics of the modelled biological system, rather than artefacts of biological uncertainty or parametrisation, or simulation stochasticity. Statistical analyses reveal the influence that pathways and components have on simulation behaviour, offering valuable biological insight into aspects of the system under study. We demonstrate the power of spartan in providing critical insight into aspects of lymphoid tissue development in the small intestine through simulation. Spartan is released under a GPLv2 license, implemented within the open source R statistical environment, and freely available from both the Comprehensive R Archive Network (CRAN) and http://www.cs.york.ac.uk/spartan. The techniques within the package can be applied to traditional ordinary or partial differential equation simulations as well as agent-based implementations. Manuals, comprehensive tutorials, and example simulation data upon which spartan can be applied are available from the website.},
author = {Alden, Kieran and Read, Mark and Timmis, Jon and Andrews, Paul S. and Veiga-Fernandes, Henrique and Coles, Mark},
doi = {10.1371/journal.pcbi.1002916},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Alden et al. - 2013 - Spartan A Comprehensive Tool for Understanding Uncertainty in Simulations of Biological Systems.pdf:pdf},
isbn = {1553-7358},
issn = {1553734X},
journal = {PLoS Computational Biology},
number = {2},
pmid = {23468606},
title = {{Spartan: A Comprehensive Tool for Understanding Uncertainty in Simulations of Biological Systems}},
volume = {9},
year = {2013}
}
@article{Graler2016,
abstract = {We present new spatio-temporal geostatistical modelling and interpolation capabilities of the R package gstat. Various spatio-temporal covariance models have been implemented, such as the separable, product-sum, metric and sum-metric models. In a real-world application we compare spatio-temporal interpolations using these models with a purely spatial kriging approach. The target variable of the application is the daily mean PM10 concentration measured at rural air quality monitoring stations across Germany in 2005. R code for variogram fitting and interpolation is presented in this paper to illustrate the workflow of spatio-temporal interpolation using gstat. We conclude that the system works properly and that the extension of gstat facilitates and eases spatio-temporal geostatistical modelling and prediction for R users.},
author = {Graler, Benedikt and Pebesma, Edzer and Heuvelink, Gerard},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Graler, Pebesma, Heuvelink - 2016 - Spatio-Temporal Interpolation using gstat.pdf:pdf},
issn = {20734859},
journal = {Wp},
pages = {1--20},
title = {{Spatio-Temporal Interpolation using gstat}},
volume = {8},
year = {2016}
}
@article{Tobergte2016,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Tobergte, David R. and Curtis, Shirley},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Tobergte, Curtis - 2016 - spup- an R package for uncertainty propagation in spatial environmental modelling.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {International symposium on "Spatial Accuracy Assessment in Natural Resources and Environmental Sciences"},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{spup- an R package for uncertainty propagation in spatial environmental modelling}},
url = {http://spatial-accuracy.org/Accuracy2016},
volume = {53},
year = {2016}
}
@article{Hartmann2008,
abstract = {For the simulation of structural collapse using controlled explosives, the quantification of structural parameters has to be accomplished on the basis of only few data, which may additionally be characterized by vagueness, e.g. due to uncertain measurements or changing reproduction conditions. To ensure a reliable prediction of a structural collapse, next to a close to reality simulation of the complex dynamic process, this uncertainty has to be taken into account. With regard to very high computation associated with the simulation of collapses of real world structures based on conventional finite element models, this paper addresses an efficient approach for the simulation of structural collapse based on multibody models, that simultaneously allow for the investigation of uncertainty. ?? 2008 Elsevier Ltd. All rights reserved.},
author = {Hartmann, Dietrich and Breidt, Michael and Nguyen, van Vinh and Stangenberg, Friedhelm and H??hler, Sebastian and Schweizerhof, Karl and Mattern, Steffen and Blankenhorn, Gunther and M??ller, Bernd and Liebscher, Martin},
doi = {10.1016/j.compstruc.2008.03.004},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Hartmann et al. - 2008 - Structural collapse simulation under consideration of uncertainty-Fundamental concept and results.pdf:pdf},
issn = {00457949},
journal = {Computers and Structures},
keywords = {Demolition,Explosives,Fuzziness,Multi-level simulation,Multibody dynamics},
number = {21-22},
pages = {2064--2078},
publisher = {Elsevier Ltd},
title = {{Structural collapse simulation under consideration of uncertainty-Fundamental concept and results}},
url = {http://dx.doi.org/10.1016/j.compstruc.2008.03.004},
volume = {86},
year = {2008}
}
@article{Jampani2011,
abstract = {The application of stochastic models and analysis techniques to large datasets is now commonplace. Unfortunately, in practice this usually means extracting data from a database system into an external tool (such as SAS, R, Arena, or Matlab), and then running the analysis there. This extract-and-model paradigm is typically error-prone, slow, does not support fine-grained modeling, and discourages what-if and sensitivity analyses. In this article we describe MCDB, a database system that permits a wide spectrum of stochastic models to be used in conjunction with the data stored in a large database, without ever extracting the data. MCDB facilitates in-database execution of tasks such as risk assessment, prediction, and imputation of missing data, as well as management of errors due to data integration, information extraction, and privacy-preserving data anonymization. MCDB allows a user to define random relations whose contents are determined by stochastic models. The models can then be queried using standard SQL. Monte Carlo techniques are used to analyze the probability distribution of the result of an SQL query over random relations. Novel tuple-bundle processing techniques can effectively control the Monte Carlo overhead, as shown in our experiments.},
author = {Jampani, Ravi and Xu, Fei and Wu, Mingxi and Perez, Luis and Jermaine, Chris and Haas, Peter J},
doi = {10.1145/2000824.2000828},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Jampani et al. - 2011 - The monte carlo database system.pdf:pdf},
issn = {03625915},
journal = {ACM Transactions on Database Systems},
keywords = {MCDB,relational database systems,uncertainty},
number = {3},
pages = {1--41},
title = {{The monte carlo database system}},
url = {http://dl.acm.org/citation.cfm?doid=2000824.2000828},
volume = {36},
year = {2011}
}
@article{Jampani2011,
abstract = {The application of stochastic models and analysis techniques to large datasets is now commonplace. Unfortunately, in practice this usually means extracting data from a database system into an external tool (such as SAS, R, Arena, or Matlab), and then running the analysis there. This extract-and-model paradigm is typically error-prone, slow, does not support fine-grained modeling, and discourages what-if and sensitivity analyses. In this article we describe MCDB, a database system that permits a wide spectrum of stochastic models to be used in conjunction with the data stored in a large database, without ever extracting the data. MCDB facilitates in-database execution of tasks such as risk assessment, prediction, and imputation of missing data, as well as management of errors due to data integration, information extraction, and privacy-preserving data anonymization. MCDB allows a user to define random relations whose contents are determined by stochastic models. The models can then be queried using standard SQL. Monte Carlo techniques are used to analyze the probability distribution of the result of an SQL query over random relations. Novel tuple-bundle processing techniques can effectively control the Monte Carlo overhead, as shown in our experiments.},
author = {Jampani, Ravi and Xu, Fei and Wu, Mingxi and Perez, Luis and Jermaine, Chris and Haas, Peter J},
doi = {10.1145/2000824.2000828},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Jampani et al. - 2011 - The monte carlo database system.pdf:pdf},
issn = {03625915},
journal = {ACM Transactions on Database Systems},
keywords = {MCDB,relational database systems,uncertainty},
number = {3},
pages = {1--41},
title = {{The monte carlo database system}},
url = {http://dl.acm.org/citation.cfm?doid=2000824.2000828},
volume = {36},
year = {2011}
}
@article{Crespo2014,
abstract = {ABSTRACT In this work, an integrated framework to deal with scarce data, aleatory and epistemic uncertainties is presented. Generally, dealing with the uncertainty requires the availability of efficient and scalable computational tools. For this reason, the proposed strategies have been implemented in an open general purpose computational framework for uncertainty quantification and management that allows for a significant reduction of the computational time required by adopting efficient techniques for uncertainty quantification and resorting to the computational power of a cluster computing. The proposed framework has been adopted to solve the NASA Langley multidisciplinary uncertainty quantification challenge. All the five subproblems have been tacked, i.e. uncertainty characterization, sensitivity analysis, uncertainty quantification, extreme case analysis and robust design. All the subproblems have been solved using different approaches based on different hypotheses and assumption in order to cross-validate the results and showing the flexibility and potentiality and computational framework.},
author = {Crespo, Luis G. and Kenny, Sean P. and Giesy, Daniel P.},
doi = {10.2514/6.2014-1347},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Crespo, Kenny, Giesy - 2014 - The NASA Langley Multidisciplinary Uncertainty Quantification Challenge.pdf:pdf},
isbn = {978-1-62410-312-4},
journal = {16th AIAA Non-Deterministic Approaches Conference},
number = {January},
pages = {1--9},
title = {{The NASA Langley Multidisciplinary Uncertainty Quantification Challenge}},
url = {http://arc.aiaa.org/doi/abs/10.2514/6.2014-1347},
year = {2014}
}
@article{Peckham2015,
abstract = {Component-based modeling frameworks make it easier for users to access, configure, couple, run and test numerical models. However, they do not typically provide tools for uncertainty quantification or data-based model verification and calibration. To better address these important issues, modeling frameworks should be integrated with existing, general-purpose toolkits for optimization, parameter estimation and uncertainty quantification.This paper identifies and then examines the key issues that must be addressed in order to make a component-based modeling framework interoperable with general-purpose packages for model analysis. As a motivating example, one of these packages, DAKOTA, is applied to a representative but nontrivial surface process problem of comparing two models for the longitudinal elevation profile of a river to observational data. Results from a new mathematical analysis of the resulting nonlinear least squares problem are given and then compared to results from several different optimization algorithms in DAKOTA.},
author = {Peckham, Scott D. and Kelbert, Anna and Hill, Mary C. and Hutton, Eric W H},
doi = {10.1016/j.cageo.2016.03.005},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Peckham et al. - 2016 - Towards uncertainty quantification and parameter estimation for Earth system models in a component-based modelin.pdf:pdf},
isbn = {00983004},
issn = {00983004},
journal = {Computers and Geosciences},
keywords = {Component-based modeling,Inverse problems,Longitudinal river elevation profiles,Model uncertainty,Modeling frameworks,Nonlinear least squares,Optimization,Parameter estimation},
pages = {152--161},
publisher = {Elsevier},
title = {{Towards uncertainty quantification and parameter estimation for Earth system models in a component-based modeling framework}},
volume = {90},
year = {2016}
}
@article{Ferretti2016,
abstract = {The majority of published sensitivity analyses (SAs) are either local or one factor-at-a-time (OAT) analyses, relying on unjustified assumptions of model linearity and additivity. Global approaches to sensitivity analyses (GSA) which would obviate these shortcomings, are applied by a minority of researchers. By reviewing the academic literature on SA, we here present a bibliometric analysis of the trends of different SA practices in last decade. The review has been conducted both on some top ranking journals (Nature and Science) and through an extended analysis in the Elsevier's Scopus database of scientific publications. After correcting for the global growth in publications, the amount of papers performing a generic SA has notably increased over the last decade. Even if OAT is still the most largely used technique in SA, there is a clear increase in the use of GSA with preference respectively for regression and variance-based techniques. Even after adjusting for the growth of publications in the sole modelling field, to which SA and GSA normally apply, the trend is confirmed. Data about regions of origin and discipline are also briefly discussed. The results above are confirmed when zooming on the sole articles published in chemical modelling, a field historically proficient in the use of SA methods.},
annote = {Solo es una revision bibliografica, habla del aumento del uso de los metodos de analisis de sensibilidad en los ultimos anos. De como se usa mucho el analisis de un solo parametro a la vez, lo que no funciona correctamente.},
author = {Ferretti, Federico and Saltelli, Andrea and Tarantola, Stefano},
doi = {10.1016/j.scitotenv.2016.02.133},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Ferretti, Saltelli, Tarantola - 2016 - Trends in Sensitivity Analysis practice in the last decade.pdf:pdf},
issn = {00489697},
journal = {Science of the Total Environment},
keywords = {Bibliometric analysis,Chemical modelling,Global sensitivity analysis,Sensitivity analysis,bibliometric analysis,global sensitivity analysis,sensitivity analysis},
pages = {2--6},
pmid = {26934843},
publisher = {Elsevier B.V.},
title = {{Trends in Sensitivity Analysis practice in the last decade}},
url = {http://dx.doi.org/10.1016/j.scitotenv.2016.02.133},
volume = {21027},
year = {2016}
}
@article{Wellmann2012,
abstract = {Analyzing, visualizing and communicating uncertainties are important issues as geological models can never be fully determined. To date, there exists no general approach to quantify uncertainties in geological modeling. We propose here to use information entropy as an objective measure to compare and evaluate model and observational results. Information entropy was introduced in the 50s and defines a scalar value at every location in the model for predictability. We show that this method not only provides a quantitative insight into model uncertainties but, due to the underlying concept of information entropy, can be related to questions of data integration (i.e. how is the model quality interconnected with the used input data) and model evolution (i.e. does new data - or a changed geological hypothesis - optimize the model). In other words information entropy is a powerful measure to be used for data assimilation and inversion.As a first test of feasibility, we present the application of the new method to the visualization of uncertainties in geological models, here understood as structural representations of the subsurface. Applying the concept of information entropy on a suite of simulated models, we can clearly identify (a) uncertain regions within the model, even for complex geometries; (b) the overall uncertainty of a geological unit, which is, for example, of great relevance in any type of resource estimation; (c) a mean entropy for the whole model, important to track model changes with one overall measure. These results cannot easily be obtained with existing standard methods.The results suggest that information entropy is a powerful method to visualize uncertainties in geological models, and to classify the indefiniteness of single units and the mean entropy of a model quantitatively. Due to the relationship of this measure to the missing information, we expect the method to have a great potential in many types of geoscientific data assimilation problems - beyond pure visualization. {\textcopyright} 2011 Elsevier B.V.},
annote = {In general, they present a method to quantify uncertainty using information entropy. They apply the method just to spatial models (no time evolution). In the general case of spatio-temporal models, then we can use maxEnt (described in the conclusions). This could be another contribution.

The use of information entropy could be very good to compare models. With this idea we have many models in play. Then we need to create a DB structure to store the uncertainty asociated to those models and we need to implement the operators to perform the entropy evaluation above those models. To, evaluate a model itself, or to compare with other model.},
author = {Wellmann, J. Florian and Regenauer-Lieb, Klaus},
doi = {10.1016/j.tecto.2011.05.001},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Wellmann, Regenauer-Lieb - 2012 - Uncertainties have a meaning Information entropy as a quality measure for 3-D geological models.pdf:pdf},
isbn = {0040-1951},
issn = {00401951},
journal = {Tectonophysics},
keywords = {3-D geological modeling,Fuzziness,Information entropy,Simulation,Uncertainty,Visualization},
pages = {207--216},
publisher = {Elsevier B.V.},
title = {{Uncertainties have a meaning: Information entropy as a quality measure for 3-D geological models}},
url = {http://dx.doi.org/10.1016/j.tecto.2011.05.001},
volume = {526-529},
year = {2012}
}
@article{Johnstone2015,
abstract = {Cardiac electrophysiology models have been developed for over 50 years, and now include detailed descriptions of individual ion currents and sub-cellular calcium handling. It is commonly accepted that there are many uncertainties in these systems, with quantities such as ion channel kinetics or expression levels being difficult to measure or variable between samples. Until recently, the original approach of describing model parameters using single values has been retained, and consequently the majority of mathematical models in use today provide point predictions, with no associated uncertainty. In recent years, statistical techniques have been developed and applied in many scientific areas to capture uncertainties in the quantities that determine model behaviour, and to provide a distribution of predictions which accounts for this uncertainty. In this paper we discuss this concept, which is termed uncertainty quantification, and consider how it might be applied to cardiac electrophysiology models. We present two case studies in which probability distributions, instead of individual numbers, are inferred from data to describe quantities such as maximal current densities. Then we show how these probabilistic representations of model parameters enable probabilities to be placed on predicted behaviours. We demonstrate how changes in these probability distributions across data sets offer insight into which currents cause beat-to-beat variability in canine APs. We conclude with a discussion of the challenges that this approach entails, and how it provides opportunities to improve our understanding of electrophysiology.},
author = {Johnstone, Ross H. and Chang, Eugene T Y and Bardenet, R{\'{e}}mi and de Boer, Teun P. and Gavaghan, David J. and Pathmanathan, Pras and Clayton, Richard H. and Mirams, Gary R.},
doi = {10.1016/j.yjmcc.2015.11.018},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Johnstone et al. - 2016 - Uncertainty and variability in models of the cardiac action potential Can we build trustworthy models.pdf:pdf},
issn = {10958584},
journal = {Journal of Molecular and Cellular Cardiology},
keywords = {Cardiac electrophysiology,Mathematical model,Probability,Uncertainty quantification},
pages = {49--62},
pmid = {26611884},
publisher = {The Authors},
title = {{Uncertainty and variability in models of the cardiac action potential: Can we build trustworthy models?}},
url = {http://dx.doi.org/10.1016/j.yjmcc.2015.11.018},
volume = {96},
year = {2016}
}
@book{Geris2016,
author = {Geris, Liesbet and Gomez-Cabrero, David},
doi = {10.1007/978-3-319-21296-8},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Geris, Gomez-Cabrero - 2016 - Uncertainty in Biology.pdf:pdf},
isbn = {978-3-319-21295-1},
pages = {471},
title = {{Uncertainty in Biology}},
url = {http://link.springer.com/10.1007/978-3-319-21296-8},
volume = {17},
year = {2016}
}
@article{Caers2014,
annote = {This paper could be used as a reference in the selection of a case of study.},
author = {Li, Lewis and Caers, Jef and Sava, Paul},
doi = {10.1190/segam2014-1402.1},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Li, Caers, Sava - 2014 - Uncertainty maps for seismic images through geostatistical model randomization.pdf:pdf},
journal = {SEG Technical Program Expanded Abstracts 2014},
keywords = {depth,fractals,imaging,interpretation,subsalt},
pages = {1496--1500},
title = {{Uncertainty maps for seismic images through geostatistical model randomization}},
url = {http://library.seg.org/doi/abs/10.1190/segam2014-1402.1},
year = {2014}
}
@article{Frank2012,
author = {Frank, Prof M},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Frank - 2012 - Uncertainty quantification.pdf:pdf},
number = {d},
pages = {3--4},
title = {{Uncertainty quantification}},
year = {2012}
}
@article{Ghanem2013,
author = {Ghanem, Roger},
pages = {1--42},
title = {{Uncertainty Quantification}},
year = {2013}
}
@phdthesis{Sankararaman2012,
author = {Sankararaman, Shankar},
booktitle = {PhD Dissertation},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Sankararaman - 2012 - Uncertainty Quantification and Integration.pdf:pdf},
school = {Vanderbilt University},
title = {{Uncertainty Quantification and Integration}},
year = {2012}
}
@book{DeCursi2015a,
abstract = {Formerly CIP.},
annote = {From Duplicate 2 (Uncertainty Quantification and Stochastic Modeling with Matlab - de Cursi, Eduardo Souza; Sampaio, Rubens)

From Duplicate 2 (Uncertainty Quantification and Stochastic Modeling with Matlab - de Cursi, Eduardo Souza; Sampaio, Rubens)

Muy dificil porque usa Analisis Funcional.},
author = {de Cursi, Eduardo Souza and Sampaio, Rubens},
doi = {10.1016/B978-1-78548-005-8.50009-8},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/de Cursi, Sampaio - 2015 - Uncertainty Quantification and Stochastic Modeling with Matlab.pdf:pdf},
isbn = {9780081004715},
pages = {442},
title = {{Uncertainty Quantification and Stochastic Modeling with Matlab}},
url = {http://www.amazon.com/Uncertainty-Quantification-Stochastic-Modeling-Matlab/dp/1785480057},
year = {2015}
}
@article{Guerra2012,
abstract = {Computational simulation of complex engineered systems requires intensive computation and a significant amount of data management. Today, this management is often carried out on a case-by-case basis and requires great effort to track it. This is due to the complexity of controlling a large amount of data flowing along a chain of simulations. Moreover, many times there is a need to explore parameter variability for the same set of data. On a case-by-case basis, there is no register of data involved in the simulation, making this process prone to errors. In addition, if the user wants to analyze the behavior of a simulation sample, then he/she must wait until the end of the whole simulation. In this context, techniques and methodologies of scientific workflows can improve the management of simulations. Parameter variability can be put in the general context of uncertainty quantification (UQ), which provides a rational perspective for analysts and decision makers. The objective of this work is to use scientific workflows to provide a systematic approach in: (i) modeling UQ numerical experiments as scientific workflows, (ii) offering query tools to evaluate UQ processes at runtime, (iii) managing theUQanalysis, and (iv) managingUQin parallel executions. When using scientific workflow engines, one can collect data in a transparent manner, allowing execution steering, the postassessment of results, and providing the information for reexecuting the experiment, thereby ensuring reproducibility, an essential characteristic in a scientific or engineering computational experiment.},
author = {Guerra, Gabriel and Rochinha, Fernando A. and Elias, Renato and de Oliveira, Daniel and Ogasawara, Eduardo and Dias, Jonas Furtado and Mattoso, Marta and Coutinho, Alvaro L. G. A.},
doi = {10.1615/Int.J.UncertaintyQuantification.2012003593},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Guerra et al. - 2012 - Uncertainty Quantification in Computational Predictive Models for Fluid Dynamics Using a Workflow Management Engi.pdf:pdf},
issn = {2152-5080},
journal = {International Journal for Uncertainty Quantification},
keywords = {adaptive sparse grid,computational fluid dynamics,method,parallelization,provenance,scientific workflows,sparse grid stochastic collocation},
number = {1},
pages = {53--71},
title = {{Uncertainty Quantification in Computational Predictive Models for Fluid Dynamics Using a Workflow Management Engine}},
url = {http://www.begellhouse.com/journals/52034eb04b657aea,69f226067bce0f5b,1b427aac4a956792.html},
volume = {2},
year = {2012}
}
@article{Alvin1998a,
author = {Alvin, K F and Oberkampf, William L and Diegert, K V and Rutherford, B M},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Alvin et al. - 1998 - Uncertainty quantification in computational structural dynamics a new paradigm for model validation.pdf:pdf},
journal = {Society for Experimental Mechanics, Inc, 16 th International Modal Analysis Conference.},
keywords = {uncertainty - general},
pages = {1191--1198},
title = {{Uncertainty quantification in computational structural dynamics: a new paradigm for model validation}},
volume = {2},
year = {1998}
}
@article{Alvin1998,
annote = {From Duplicate 2 (Uncertainty quantification in computational structural dynamics: a new paradigm for model validation - Alvin, K F; Oberkampf, William L; Diegert, K V; Rutherford, B M)

From Duplicate 1 (Uncertainty quantification in computational structural dynamics: a new paradigm for model validation - Alvin, K F; Oberkampf, William L; Diegert, K V; Rutherford, B M)

Uncertainty when we have different models. They use the same approach as Bernardo, Bayesian model to propagate de uncertainty.},
author = {Alvin, K F and Oberkampf, William L and Diegert, K V and Rutherford, B M},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Alvin et al. - 1998 - Uncertainty quantification in computational structural dynamics a new paradigm for model validation.pdf:pdf},
journal = {Society for Experimental Mechanics, Inc, 16 th International Modal Analysis Conference.},
keywords = {uncertainty - general},
pages = {1191--1198},
title = {{Uncertainty quantification in computational structural dynamics: a new paradigm for model validation}},
volume = {2},
year = {1998}
}
@article{Guerra2016,
author = {Guerra, Gabriel M. and Zio, Souleymane and Camata, Jose J. and Dias, Jonas and Elias, Renato N. and Mattoso, Marta and {B. Paraizo}, Paulo L. and {G. A. Coutinho}, Alvaro L. and Rochinha, Fernando A.},
doi = {10.1007/s10596-016-9563-6},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Guerra et al. - 2016 - Uncertainty quantification in numerical simulation of particle-laden flows.pdf:pdf},
issn = {1420-0597},
journal = {Computational Geosciences},
number = {1},
pages = {265--281},
title = {{Uncertainty quantification in numerical simulation of particle-laden flows}},
url = {http://link.springer.com/10.1007/s10596-016-9563-6},
volume = {20},
year = {2016}
}
@article{Santonja2012,
abstract = {Mathematical models based on ordinary differential equations are a useful tool to study the processes involved in epidemiology. Many models consider that the parameters are deterministic variables. But in practice, the transmission parameters present large variability and it is not possible to determine them exactly, and it is necessary to introduce randomness. In this paper, we present an application of the polynomial chaos approach to epidemiological mathematical models based on ordinary differential equations with random coefficients. Taking into account the variability of the transmission parameters of the model, this approach allows us to obtain an auxiliary system of differential equations, which is then integrated numerically to obtain the first-and the second-order moments of the output stochastic processes. A sensitivity analysis based on the polynomial chaos approach is also performed to determine which parameters have the greatest influence on the results. As an example, we will apply the approach to an obesity epidemic model.},
author = {Santonja, F. and Chen-Charpentier, B.},
doi = {10.1155/2012/742086},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Santonja, Chen-Charpentier - 2012 - Uncertainty quantification in simulations of epidemics using polynomial chaos.pdf:pdf},
issn = {1748670X},
journal = {Computational and Mathematical Methods in Medicine},
pmid = {22927889},
title = {{Uncertainty quantification in simulations of epidemics using polynomial chaos}},
volume = {2012},
year = {2012}
}
@article{Santonja2012,
abstract = {Mathematical models based on ordinary differential equations are a useful tool to study the processes involved in epidemiology. Many models consider that the parameters are deterministic variables. But in practice, the transmission parameters present large variability and it is not possible to determine them exactly, and it is necessary to introduce randomness. In this paper, we present an application of the polynomial chaos approach to epidemiological mathematical models based on ordinary differential equations with random coefficients. Taking into account the variability of the transmission parameters of the model, this approach allows us to obtain an auxiliary system of differential equations, which is then integrated numerically to obtain the first-and the second-order moments of the output stochastic processes. A sensitivity analysis based on the polynomial chaos approach is also performed to determine which parameters have the greatest influence on the results. As an example, we will apply the approach to an obesity epidemic model.},
author = {Santonja, F. and Chen-Charpentier, B.},
doi = {10.1155/2012/742086},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Santonja, Chen-Charpentier - 2012 - Uncertainty quantification in simulations of epidemics using polynomial chaos.pdf:pdf},
issn = {1748670X},
journal = {Computational and Mathematical Methods in Medicine},
pmid = {22927889},
title = {{Uncertainty quantification in simulations of epidemics using polynomial chaos}},
volume = {2012},
year = {2012}
}
@misc{Lataniotis2015,
author = {Lataniotis, C and Marelli, S and Sudret, B},
keywords = {Computational Model,Model,Plugin,UQLab,Uncertainty Quantification,Wrapper},
title = {{UQLAB USER MANUAL THE MODEL MODULE}},
year = {2015}
}
@misc{Marelli2014,
author = {Marelli, Stefano and Sudret, Bruno},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Marelli, Sudret - 2014 - UQLAB a framework for Uncertainty Quantification in MATLAB.pdf:pdf},
number = {Bourinet 2009},
pages = {2554--2563},
title = {{UQLAB: a framework for Uncertainty Quantification in MATLAB}},
year = {2014}
}
@article{Estacio-Hiroms2012,
author = {Estacio-Hiroms, Kemelli C and Prudencio, Ernesto E},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Estacio-Hiroms, Prudencio - 2012 - User's Manual Quantification of Uncertainty for Estimation, Simulation, and Optimization (QUESO).pdf:pdf},
keywords = {research,statistical,uncertainty quantification},
title = {{User's Manual: Quantification of Uncertainty for Estimation, Simulation, and Optimization (QUESO)}},
year = {2012}
}
@article{Oberkampf2004,
abstract = {Developers of computer codes, analysts who use the codes, and decision makers who rely on the results of the analyses face a critical question: How should confidence in modeling and simulation be critically assessed? Verification and validation (V{\&}V) of computational simulations are the primary methods for building and quantifying this confidence. Briefly, verification is the assessment of the accuracy of the solution to a computational model. Validation is the assessment of the accuracy of a computational simulation by comparison with experimental data. In verification, the relationship of the simulation to the real world is not an issue. In validation, the relationship between computation and the real world, ie, experimental data, is the issue. This paper presents our viewpoint of the state of the art in V{\&}V in computational physics. (In this paper we refer to all fields of computational engineering and physics, eg, computational fluid dynamics, computational solid mechanics, structural dynamics, shock wave physics, computational chemistry, etc, as computational physics.) We describe our view of the framework in which predictive capability relies on V{\&}V, as well as other factors that affect predictive capability. Our opinions about the research needs and management issues in V{\&}V are very practical: What methods and techniques need to be developed and what changes in the views of management need to occur to increase the usefulness, reliability, and impact of computational physics for decision making about engineering systems? We review the state of the art in V{\&}V over a wide range of topics, for example, prioritization of V{\&}V activities using the Phenomena Identification and Ranking Table (PIRT), code verification, software quality assurance (SQA), numerical error estimation, hierarchical experiments for validation, characteristics of validation experiments, the need to perform nondeterministic computational simulations in comparisons with experimental data, and validation metrics. We then provide an extensive discussion of V{\&}V research and implementation issues that we believe must be addressed for V{\&}V to be more effective in improving confidence in computational predictive capability. Some of the research topics addressed are development of improved procedures for the use of the PIRT for prioritizing V{\&}V activities, the method of manufactured solutions for code verification, development and use of hierarchical validation diagrams, and the construction and use of validation metrics incorporating statistical measures. Some of the implementation topics addressed are the needed management initiatives to better align and team computationalists and experimentalists in conducting validation activities, the perspective of commercial software companies, the key role of analysts and decision makers as code customers, obstacles to the improved effectiveness of V{\&}V, effects of cost and schedule constraints on practical applications in industrial settings, and the role of engineering standards committees in documenting best practices for V{\&}V. There are 207 references cited in this review article.},
author = {Oberkampf, William L. and Trucano, Timothy G. and Hirsch, Charles},
doi = {10.1115/1.1767847},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Oberkampf, Trucano, Hirsch - 2004 - Verification, validation, and predictive capability in computational engineering and physics.pdf:pdf},
isbn = {0003-6900},
issn = {00036900},
journal = {Applied Mechanics Reviews},
number = {5},
pages = {345},
title = {{Verification, validation, and predictive capability in computational engineering and physics}},
volume = {57},
year = {2004}
}
@article{Oberkampf2004a,
abstract = {Developers of computer codes, analysts who use the codes, and decision makers who rely on the results of the analyses face a critical question: How should confidence in modeling and simulation be critically assessed? Verification and validation (V{\&}V) of computational simulations are the primary methods for building and quantifying this confidence. Briefly, verification is the assessment of the accuracy of the solution to a computational model. Validation is the assessment of the accuracy of a computational simulation by comparison with experimental data. In verification, the relationship of the simulation to the real world is not an issue. In validation, the relationship between computation and the real world, ie, experimental data, is the issue. This paper presents our viewpoint of the state of the art in V{\&}V in computational physics. (In this paper we refer to all fields of computational engineering and physics, eg, computational fluid dynamics, computational solid mechanics, structural dynamics, shock wave physics, computational chemistry, etc, as computational physics.) We describe our view of the framework in which predictive capability relies on V{\&}V, as well as other factors that affect predictive capability. Our opinions about the research needs and management issues in V{\&}V are very practical: What methods and techniques need to be developed and what changes in the views of management need to occur to increase the usefulness, reliability, and impact of computational physics for decision making about engineering systems? We review the state of the art in V{\&}V over a wide range of topics, for example, prioritization of V{\&}V activities using the Phenomena Identification and Ranking Table (PIRT), code verification, software quality assurance (SQA), numerical error estimation, hierarchical experiments for validation, characteristics of validation experiments, the need to perform nondeterministic computational simulations in comparisons with experimental data, and validation metrics. We then provide an extensive discussion of V{\&}V research and implementation issues that we believe must be addressed for V{\&}V to be more effective in improving confidence in computational predictive capability. Some of the research topics addressed are development of improved procedures for the use of the PIRT for prioritizing V{\&}V activities, the method of manufactured solutions for code verification, development and use of hierarchical validation diagrams, and the construction and use of validation metrics incorporating statistical measures. Some of the implementation topics addressed are the needed management initiatives to better align and team computationalists and experimentalists in conducting validation activities, the perspective of commercial software companies, the key role of analysts and decision makers as code customers, obstacles to the improved effectiveness of V{\&}V, effects of cost and schedule constraints on practical applications in industrial settings, and the role of engineering standards committees in documenting best practices for V{\&}V. There are 207 references cited in this review article.},
author = {Oberkampf, William L. and Trucano, Timothy G. and Hirsch, Charles},
doi = {10.1115/1.1767847},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Oberkampf, Trucano, Hirsch - 2004 - Verification, validation, and predictive capability in computational engineering and physics.pdf:pdf},
isbn = {0003-6900},
issn = {00036900},
journal = {Applied Mechanics Reviews},
number = {5},
pages = {345},
title = {{Verification, validation, and predictive capability in computational engineering and physics}},
volume = {57},
year = {2004}
}
@article{Zehner2010,
abstract = {Characterization of the earth's subsurface involves the construction of 3D models from sparse data and so leads to simulation results that involve some degree of uncertainty. This uncertainty is often neglected in the subsequent visualization, due to the fact that no established methods or available software exist. We describe a visualization method to render scalar fields with a probability density function at each data point. We render these data as isosurfaces and make use of a colour scheme, which intuitively gives the viewer an idea of which parts of the surface are more reliable than others. We further show how to extract an envelope that indicates within which volume the isosurface will lie with a certain confidence, and augment the isosurfaces with additional geometry in order to show this information. The resulting visualization is easy and intuitive to understand and is suitable for rendering multiple distinguishable isosurfaces at a time. It can moreover be easily used together with other visualized objects, such as the geological context. Finally we show how we have integrated this into a visualization pipeline that is based on the Visualization Toolkit (VTK) and the open source scenegraph OpenSG, allowing us to render the results on a desktop and in different kinds of virtual environments. ?? 2010 Elsevier Ltd.},
author = {Zehner, Bj{\"{o}}rn and Watanabe, Norihiro and Kolditz, Olaf},
doi = {10.1016/j.cageo.2010.02.010},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Zehner, Watanabe, Kolditz - 2010 - Visualization of gridded scalar data with uncertainty in geosciences.pdf:pdf},
isbn = {0098-3004},
issn = {00983004},
journal = {Computers and Geosciences},
keywords = {3D,Monte carlo simulation,Scalar fields,Uncertainty,Visualisation,Visualization},
number = {10},
pages = {1268--1275},
title = {{Visualization of gridded scalar data with uncertainty in geosciences}},
volume = {36},
year = {2010}
}
@misc{deLaPuente2015,
author = {{Josep de la Puente}, Alvaro Coutinho},
file = {:Users/nmlemus/Dropbox/Noel{\_}Uncertainty/Papers/News/HPC4E{\_}D6.3 v1.0.pdf:pdf},
pages = {1--9},
title = {{Website deploying a suite of geophysical tests for wave propagation problems on extreme scale machines}},
year = {2015}
}
@article{Tobergte2013,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Tobergte, David R. and Curtis, Shirley},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Tobergte, Curtis - 2013 - Workshop on Quantification, Communication, and Interpretation of Uncertainty in Simulation and Data Science.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Journal of Chemical Information and Modeling},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{Workshop on Quantification, Communication, and Interpretation of Uncertainty in Simulation and Data Science}},
volume = {53},
year = {2013}
}
@article{Tobergte2013,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Tobergte, David R. and Curtis, Shirley},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:Users/nmlemus/Library/Application Support/Mendeley Desktop/Downloaded/Tobergte, Curtis - 2013 - Workshop on Quantification, Communication, and Interpretation of Uncertainty in Simulation and Data Science.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Journal of Chemical Information and Modeling},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{Workshop on Quantification, Communication, and Interpretation of Uncertainty in Simulation and Data Science}},
volume = {53},
year = {2013}
}
