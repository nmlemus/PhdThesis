@techreport{DEnergy2009,
author = {{U.S. Department of Energy}},
file = {:home/nmlemus/Documents/Mendeley Desktop/Nnsa{\_}grand{\_}challenges{\_}report(2).pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
pages = {255},
title = {{Scientific Grand Challenges in National Security: The Role of Computing at the Extreme Scale}},
year = {2009}
}
@article{Cox2012,
abstract = {Numerical quantification of the results from a measurement uncertainty computation is considered in terms of the inputs to that computation. The primary output is often an approximation to the PDF (probability density function) for the univariate or multivariate measurand (the quantity intended to be measured). All results of interest can be derived from this PDF. We consider uncertainty elicitation, propagation of distributions through a computational model, Bayes' rule and its implementation and other numerical considerations, representation of the PDF for the measurand, and sensitivities of the numerical results with respect to the inputs to the computation. Speculations are made regarding future requirements in the area and relationships to problems in uncertainty quantification for scientific computing. {\textcopyright} 2012 IFIP International Federation for Information Processing.},
author = {Cox, M. and Forbes, A. and Harris, P. and Matthews, C.},
doi = {10.1007/978-3-642-32677-6_12},
file = {:home/nmlemus/Documents/Mendeley Desktop/CoxFHM11.pdf:pdf},
isbn = {9783642326769},
issn = {18684238},
journal = {IFIP Advances in Information and Communication Technology},
keywords = {Measurement uncertainty,Monte Carlo method,probability density function,sensitivity measure,uncertainty quantification},
pages = {180--192},
title = {{Numerical aspects in the evaluation of measurement uncertainty}},
volume = {377 AICT},
year = {2012}
}
@article{Su2011,
author = {Su, Steve},
doi = {10.22237/jmasm/1320120960},
file = {:home/nmlemus/Documents/Mendeley Desktop/7e6941c80e0820af3da317ce38ba8b12463a.pdf:pdf},
issn = {1538-9472},
journal = {Journal of Modern Applied Statistical Methods},
keywords = {empirical data analysis,fitting distributions,generalized lambda distributions,mixture distributions,prior distributions},
number = {2},
pages = {599--606},
title = {{Maximum Log Likelihood Estimation using EM Algorithm and Partition Maximum Log Likelihood Estimation for Mixtures of Generalized Lambda Distributions}},
url = {http://digitalcommons.wayne.edu/jmasm/vol10/iss2/17},
volume = {10},
year = {2011}
}
@article{Joiner1971,
author = {Joiner, Brian L. and Rosenblatt, Joan R.},
doi = {10.1080/01621459.1971.10482275},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
month = {jun},
number = {334},
pages = {394--399},
title = {{Some Properties of the Range in Samples from Tukey's Symmetric Lambda Distributions}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1971.10482275},
volume = {66},
year = {1971}
}
@article{Freimer1988,
abstract = {The Tukey lambda family of distributions together with its extensions have played an important role in statistical practice. In this paper a con¬tinuously defined two-parameter generalization of this family, which holds promise of a variety of additional applications, is variously studied. The coefficients of skewness and kurtosis and the density shapes of its members are examined and the family is related to the classical Pearsonian system of distributions.},
author = {Freimer, Marshall and Lin, C. Thomas and Mudholkar, Govind S.},
doi = {10.1080/03610928808829820},
file = {:home/nmlemus/Documents/Mendeley Desktop/freimer1988.pdf:pdf},
isbn = {0361-0926},
issn = {1532415X},
journal = {Communications in Statistics - Theory and Methods},
keywords = {Pearson system of distributions,density quantile function,kurtosis contours,quantile function,skewness contours},
number = {10},
pages = {3547--3567},
title = {{A Study Of The Generalized Tukey Lambda Family}},
volume = {17},
year = {1988}
}
@article{Su2015,
abstract = {This article introduces regression quantile mod- els using both RS (Ramberg and Schmeiser, CommunAssoc Comput Mach 17:78–82, 1974) and FKML (Freimer et al., Commun Stat 17(10):3547–3567, 1988) generalised lambda distributions (GLD) and demonstrates the versatility of pro- posed models for a range of linear/non-linear and het- eroscedastic/homoscedastic empirical data.Owingto the rich shapes of GLDs, GLD quantile regression is a competitive flexiblemodel compared to standard quantile regression. The proposed method has some major advantages: (1) it provides a reference line which is very robust to outliers with the attractive property of zero mean residuals and (2) it gives a unified, elegant quantile regression model from the refer- ence linewith smooth regression coefficients across different quantiles. The proposed method has wide applications given the flexibility of GLDs. The goodness of fit of the proposed model can be assessed via QQ plots and the Kolmogorov– Smirnov test, to ensure the appropriateness of the statistical inference under consideration.},
author = {Su, Steve},
doi = {10.1007/s11222-014-9457-1},
file = {:home/nmlemus/Documents/Mendeley Desktop/s11222-014-9457-1.pdf:pdf},
isbn = {1122201494},
issn = {15731375},
journal = {Statistics and Computing},
keywords = {Generalised lambda distributions,Inverse quantile functions,Maximum likelihood estimation,Parametric modelling,Quantile regression,Robust regression},
number = {3},
pages = {635--650},
title = {{Flexible parametric quantile regression model}},
volume = {25},
year = {2015}
}
@article{Corlu2016,
abstract = {Fitting a probability distribution to observed or generated data constitutes an essential part of any data analysis system. The Generalized Lambda Distribution, while extremely versatile in this regard, is also a difficult distribution to fit. Parameter estimation methods that attempt to match moments or quantiles of the data require minimizing a bivariate non-linear function. The suitability of the resulting fitted distribution must be evaluated using a goodness-of-fit test and, if found unacceptable, the minimization procedure must be manually restarted from a new initial point. We propose a parameter estimation method that automatically generates initial points, as necessary, using quasi-random Sobol sequences.},
author = {Corlu, Canan G. and Meterelliyoz, Melike},
doi = {10.1080/03610918.2014.901355},
file = {:home/nmlemus/Documents/Mendeley Desktop/corlu2015.pdf:pdf},
isbn = {0361-0918},
issn = {15324141},
journal = {Communications in Statistics: Simulation and Computation},
keywords = {Generalized lambda distribution,Genetic algorithm,Least-squares,Method of matching percentiles,Parameter estimation},
number = {7},
pages = {2276--2296},
title = {{Estimating the Parameters of the Generalized Lambda Distribution: Which Method Performs Best?}},
volume = {45},
year = {2016}
}
@article{Liu2018,
abstract = {We consider big spatial data, which is typically produced in scientific areas such as geological or seismic interpretation. The spatial data can be produced by observation (e.g. using sensors or soil instrument) or numerical simulation programs and correspond to points that represent a 3D soil cube area. However, errors in signal processing and modeling create some uncertainty, and thus a lack of accuracy in identifying geological or seismic phenomenons. Such uncertainty must be carefully analyzed. To analyze uncertainty, the main solution is to compute a Probability Density Function (PDF) of each point in the spatial cube area. However, computing PDFs on big spatial data can be very time consuming (from several hours to even months on a parallel computer). In this paper, we propose a new solution to efficiently compute such PDFs in parallel using Spark, with three methods: data grouping, machine learning prediction and sampling. We evaluate our solution by extensive experiments on different computer clusters using big data ranging from hundreds of GB to several TB. The experimental results show that our solution scales up very well and can reduce the execution time by a factor of 33 (in the order of seconds or minutes) compared with a baseline method.},
archivePrefix = {arXiv},
arxivId = {1805.03141},
author = {Liu, Ji and Lemus, Noel Moreno and Pacitti, Esther and Porto, Fabio and Valduriez, Patrick},
eprint = {1805.03141},
file = {:home/nmlemus/Documents/Mendeley Desktop/Liu et al. - Unknown - Parallel Computation of PDFs on Big Spatial Data Using Spark.pdf:pdf},
keywords = {Spark,Spatial data,big data,parallel processing},
title = {{Parallel Computation of PDFs on Big Spatial Data Using Spark}},
url = {https://arxiv.org/pdf/1805.03141.pdf}
}
@article{Zhou,
abstract = {—Uncertain data clustering is an essential task in the research of data mining. Lots of traditional clustering methods are extended with new similarity measurements to tackle this issue. Different from certain data clustering, uncertain data clustering focus more on the evaluation of distribution similarity between uncertain data objects. In this paper, based on the KL-divergence and the JS-divergence, we propose a novel K-medoids method for clustering uncertain data, named UK-medoids. Good performance of the proposed algorithm is shown in experiments on synthetic datasets.},
author = {Zhou, Jin and Pan, Yuqi and {Philip Chen}, C L and ieeeorg {Dong Wang} and Han, Shiyuan},
file = {:home/nmlemus/Documents/Mendeley Desktop/Zhou et al. - Unknown - K-medoids Method based on Divergence for Uncertain Data Clustering.pdf:pdf},
keywords = {JS-divergence,K-medoids method,KL-divergence,—uncertain data clustering},
title = {{K-medoids Method based on Divergence for Uncertain Data Clustering}},
url = {http://www.uni-obuda.hu/users/szakala/SMC 2016 pendrive/1759{\_}smc2016.pdf}
}
@article{Jiang2011,
abstract = {Clustering on uncertain data, one of the essential tasks in mining uncertain data, posts significant challenges on both modeling similarity between uncertain objects and developing efficient computational methods. The previous methods extend traditional partitioning clustering methods like {\$}(k){\$}-means and density-based clustering methods like DBSCAN to uncertain data, thus rely on geometric distances between objects. Such methods cannot handle uncertain objects that are geometrically indistinguishable, such as products with the same mean but very different variances in customer ratings. Surprisingly, probability distributions, which are essential characteristics of uncertain objects, have not been considered in measuring similarity between uncertain objects. In this paper, we systematically model uncertain objects in both continuous and discrete domains, where an uncertain object is modeled as a continuous and discrete random variable, respectively. We use the well-known Kullback-Leibler divergence to measure similarity between uncertain objects in both the continuous and discrete cases, and integrate it into partitioning and density-based clustering methods to cluster uncertain objects. Nevertheless, a na{\&}{\#}x00EF;ve implementation is very costly. Particularly, computing exact KL divergence in the continuous case is very costly or even infeasible. To tackle the problem, we estimate KL divergence in the continuous case by kernel density estimation and employ the fast Gauss transform technique to further speed up the computation. Our extensive experiment results verify the effectiveness, efficiency, and scalability of our approaches.},
author = {Jiang, Bin and Pei, Jian and Tao, Yufei and Lin, Xuemin},
doi = {10.1109/TKDE.2011.221},
file = {:home/nmlemus/Documents/Mendeley Desktop/Jiang et al. - Unknown - CLUSTERING UNCERTAIN DATA BASED ON PROBABILITY DISTRIBUTION SIMILARITY.pdf:pdf},
isbn = {1041-4347},
issn = {1041-4347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Density estimation,Fast Gauss Transform,Index Terms—Clustering,Probabilistic distribution,Uncertain data},
pages = {1--14},
title = {{Clustering Uncertain Data Based on Probability Distribution Similarity}},
url = {https://pdfs.semanticscholar.org/e172/2c8911b7db1a3114fbd38b3ea5a9e93d1290.pdf},
year = {2011}
}
@article{Su2016,
abstract = {This article outlines the functionality of the GLDreg package in R which fits parametric regression models using generalized lambda distributions via maximum likelihood estimation and L moment matching. The main advantage of GLDreg is the provision of robust regression lines and smooth regression quantiles beyond the capabilities of existing known methods.},
author = {Su, Steve},
doi = {10.22237/jmasm/1478004240},
file = {:home/nmlemus/Documents/Mendeley Desktop/Su, Su is Senior Statistician - 2016 - Fitting Flexible Parametric Regression Models with GLDreg in R.pdf:pdf},
issn = {1538-9472},
journal = {Journal of Modern Applied Statistical Methods},
number = {2},
pages = {768--787},
title = {{Fitting Flexible Parametric Regression Models with GLDreg in R}},
url = {http://digitalcommons.wayne.edu/jmasm http://digitalcommons.wayne.edu/jmasm/vol15/iss2/46},
volume = {15},
year = {2016}
}
@article{Marcondes2018,
abstract = {In order to fit a model to healthcare expenses data, it is necessary to take into account some of its peculiarities, as the excess of zeros and its skewness, what demands flexible models instead of the usual ones from the exponential family. In this context, the Generalized Lambda Distribution (G$\lambda$D) is quite useful, as it is highly flexible, for its parameters may be chosen in a way such that it has a given mean, variance, skewness and kurtosis. Furthermore, the G$\lambda$D ap-proximates very well other distributions, so that it may be employed as a wild-card distribution in many applications. Taking advantage of the G$\lambda$D flexibility, we develop and apply to healthcare expenses data a hurdle, or two-way, model whose associated distribution is the G$\lambda$D. We first present a thorough review of the literature about the G$\lambda$D and then develop hurdle G$\lambda$D marginal and regression models. Finally, we apply the developed models to a dataset consisting of yearly healthcare expenses, and model it in function of the covariates sex, age and previous year expenses. The fitted models are compared with the kernel density estimate and models based on the Gener-alised Pareto Distribution (GPD). It is established that the G$\lambda$D models perform better than the GPD ones in modelling healthcare expenses.},
author = {Marcondes, Diego and Peixoto, Cl{\'{a}}udia and Maia, Ana Carolina},
file = {:home/nmlemus/Documents/Mendeley Desktop/Marcondes, Peixoto, Maia - Unknown - Fitting a Hurdle Generalized Lambda Distribution to healthcare expenses.pdf:pdf},
journal = {Annals of Applied Statistics},
keywords = {Generalized Lambda Distribution,Generalized Pareto Distribution,K E Y W O R D S applied statistics,healthcare expenses,hurdle models,two-way models},
title = {{FITTING A HURDLE GENERALIZED LAMBDA DISTRIBUTION TO HEALTHCARE EXPENSES}},
url = {https://arxiv.org/pdf/1712.02183.pdf},
year = {2017}
}
@article{Su2007b,
abstract = {This paper presents a two-step procedure using the method of moment or percentile to find initial values and then maximize the numerical log likelihood to fit the appropriate generalized lambda distribution to data. This paper demonstrates the use of this procedure to fit well-known statistical distributions as well as some empirical data. Overall, the use of numerical maximum log likelihood estimation is a valuable alternative among existing methods of fitting. It provides not only convincing results in terms of quantile plots and goodness of fit tests but also has the advantage of a lower variability in its parameter estimation compared to the existing starship (King and MacGillivray, 1999) and method of moment (Karian and Dudewicz, 2000) fitting schemes.},
author = {Su, Steve},
doi = {10.1016/j.csda.2006.06.008},
file = {:home/nmlemus/Documents/Mendeley Desktop/Su - 2007 - Numerical maximum log likelihood estimation for generalized lambda distributions.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {Empirical data analysis,Fitting distributions,Prior distributions},
month = {may},
number = {8},
pages = {3983--3998},
publisher = {North-Holland},
title = {{Numerical maximum log likelihood estimation for generalized lambda distributions}},
url = {https://www.sciencedirect.com/science/article/pii/S0167947306001988{\#}fig2},
volume = {51},
year = {2007}
}
@article{Lakhany2000,
author = {Lakhany, Asif and Mausser, Helmut},
file = {:home/nmlemus/Documents/Mendeley Desktop/Lakhany, Mausser - 2000 - Estimating the Parameters of the Generalized Lambda Distribution.pdf:pdf},
journal = {ALGO RESEARCH QUARTERLY},
number = {3},
title = {{Estimating the Parameters of the Generalized Lambda Distribution}},
url = {https://pdfs.semanticscholar.org/0f9d/1848671969232d58cb7cf3d2d06d9c4c347e.pdf?{\_}ga=2.56724463.474793085.1524894682-1121088995.1524894682},
volume = {3},
year = {2000}
}
@unpublished{Chalabi2012,
abstract = {flexible distribution with which to model financial data sets. The GLD can assume distributions with a large range of shapes. Analysts can therefore work with a single distribution to model almost any class of financial assets, rather than needing several. This becomes especially useful in the estimation of risk measures, where the choice of the distribution is crucial for accuracy. We introduce a new parameterization of the GLD, wherein the location and scale parameters are directly expressed as the median and interquartile range of the distribution. The two remaining parameters characterize the asymmetry and steepness of the distribution. Conditions are given for the existence of its moments, and for it to have the appropriate support. The tail behavior is also studied. The new parameterization brings a clearer interpretation of the parameters, whereby the distribution's asymmetry can be more readily distinguished from its steepness. This is in contrast to current parameterizations of the GLD, where the asymmetry and steepness are simultaneously described by a combination of the tail indices. Moreover, the new parameterization can be used to compare data sets in a convenient asymmetry and steepness shape plot.},
author = {Chalabi, Yohan and Diethelm, W and Scott, David J},
file = {:home/nmlemus/Documents/Mendeley Desktop/Chalabi, W{\"{u}}rtz, Scott - 2012 - Flexible Distribution Modeling with the Generalized Lambda Distribution.pdf:pdf},
keywords = {Classification C16 {\textperiodcentered} C13,Generalized lambda distribution {\textperiodcentered},Quantile distributions {\textperiodcentered},Shape plot representation JEL},
title = {{Flexible Distribution Modeling with the Generalized Lambda Distribution}},
url = {https://pdfs.semanticscholar.org/6b34/5bfa8ca3e73fadc11359155c2c5f33e63a7b.pdf},
year = {2012}
}
@article{Karvanen2008,
abstract = {The generalized lambda distribution (GLD) is a flexible four parameter distribution with many practical applications. L-moments of the GLD can be expressed in closed form and are good alternatives for the central moments. The L-moments of the GLD up to an arbitrary order are presented, and a study of L-skewness and L-kurtosis that can be achieved by the GLD is provided. The boundaries of L-skewness and L-kurtosis are derived analytically for the symmetric GLD and calculated numerically for the GLD in general. Additionally, the contours of L-skewness and L-kurtosis are presented as functions of the GLD parameters. It is found that with an exception of the smallest values of L-kurtosis, the GLD covers all possible pairs of L-skewness and L-kurtosis and often there are two or more distributions that share the same L-skewness and the same L-kurtosis. Examples that demonstrate situations where there are four GLD members with the same L-skewness and the same L-kurtosis are presented. The estimation of the GLD parameters is studied in a simulation example where method of L-moments compares favorably to more complicated estimation methods. The results increase the knowledge on the distributions that belong to the GLD family and can be utilized in model selection and estimation. {\textcopyright} 2007 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {math/0701405},
author = {Karvanen, Juha and Nuutinen, Arto},
doi = {10.1016/j.csda.2007.06.021},
eprint = {0701405},
file = {:home/nmlemus/Documents/Mendeley Desktop/Karvanen, Nuutinen - 2007 - Characterizing the generalized lambda distribution by L-moments.pdf:pdf},
isbn = {0167-9473},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {Kurtosis,L-moment ratio diagram,Method of L-moments,Method of moments,Skewness},
number = {4},
pages = {1971--1983},
primaryClass = {math},
title = {{Characterizing the generalized lambda distribution by L-moments}},
url = {https://ia801001.us.archive.org/27/items/arxiv-math0701405/math0701405.pdf},
volume = {52},
year = {2008}
}
@article{Movahedi2013,
abstract = {At the end of the manufacturing cycle, performance tests are often carried out to ensure that the product meets or exceeds all specified performance parameters. In addition to initial performance, customers are interested in knowing how long the product will last, how many products will fail per year, and how many will last more than some number of years. One method for determining the reliability is the application of statistical distributions. Of the most significant and common distributions currently utilized are normal, weibull, exponential, and lognormal distributions, which are used to study most of the products' and systems' reliability. However, there are products that do not follow a specified lifetime distribution and cannot be investigated by these distributions. Instead, Generalized Lambda Distribution (GLD) can be deployed to investigate the identified and unidentified distributions, so it can resolve the problem. In this research, we introduce a method for determining the reliability, using GLD in a practical and operational way.},
author = {Movahedi, M M and Lotfi, M R and Nayyeri, M},
file = {:home/nmlemus/Documents/Mendeley Desktop/Movahedi, Lotfi, Nayyeri - 2013 - A solution to determining the reliability of products {\&}quot Using Generalized Lambda Distribution {\&}quo.pdf:pdf},
journal = {Research Journal of Recent Sciences Res.J.Recent Sci},
keywords = {Reliability,generalized lambda distribution (GLD),life cycle},
number = {10},
pages = {41--47},
title = {{A solution to determining the reliability of products Using Generalized Lambda Distribution}},
url = {http://www.isca.in/rjrs/archive/v2/i10/7.ISCA-RJRS-2013-227.pdf},
volume = {2},
year = {2013}
}
@article{Lodziensis2013,
author = {Lodziensis, Acta Universitatis},
file = {:home/nmlemus/Documents/Mendeley Desktop/05-doma ski.pdf:pdf},
keywords = {distribution fitting,estimation methods,generalized lambda distribution},
title = {{Generalizations of tukey-lambda distributions}},
year = {2013}
}
@phdthesis{Friedman2004,
abstract = {Limb girdle muscular dystrophy type 2B and Miyoshi myopathy are clinically distinct forms of muscular dystrophy that arise from defects in the dysferlin gene. Here, we report two novel lines of dysferlin-deficient mice obtained by (a) gene targeting and (b) identification of an inbred strain, A/J, bearing a retrotransposon insertion in the dysferlin gene. The mutations in these mice were located at the 3' and 5' ends of the dysferlin gene. Both lines of mice lacked dysferlin and developed a progressive muscular dystrophy with histopathological and ultrastructural features that closely resemble the human disease. Vital staining with Evans blue dye revealed loss of sarcolemmal integrity in both lines of mice, similar to that seen in mdx and caveolin-3 deficient mice. However, in contrast to the latter group of animals, the dysferlin-deficient mice have an intact dystrophin glycoprotein complex and normal levels of caveolin-3. Our findings indicate that muscle membrane disruption and myofiber degeneration in dysferlinopathy were directly mediated by the loss of dysferlin via a new pathogenic mechanism in muscular dystrophies. We also show that the mutation in the A/J mice arose between the late 1970s and the early 1980s, and had become fixed in the production breeding stocks. Therefore, all studies involving the A/J mice or mice derived from A/J, including recombinant inbred, recombinant congenic and chromosome substitution strains, should take into account the dysferlin defect in these strains. These new dysferlin-deficient mice should be useful for elucidating the pathogenic pathway in dysferlinopathy and for developing therapeutic strategies.},
archivePrefix = {arXiv},
arxivId = {1707.04192},
author = {ALJAZAR, A-NASSER L.},
booktitle = {Neurologic Clinics},
doi = {10.1016/S0733-8619(03)00096-3},
eprint = {1707.04192},
file = {:home/nmlemus/Documents/Mendeley Desktop/65070.pdf:pdf},
isbn = {1359-7345},
issn = {07338619},
number = {1},
pages = {99--131},
pmid = {24239943},
title = {{Generalized Lambda Distribution and Estimation Parameters}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0733861903000963},
volume = {22},
year = {2004}
}
@article{Aldeni2017,
author = {Aldeni, Mahmoud and Lee, Carl and Famoye, Felix},
doi = {10.1186/s40488-017-0081-4},
file = {:home/nmlemus/Documents/Mendeley Desktop/Aldeni, Lee, Famoye - 2017 - Families of distributions arising from the quantile of generalized lambda distribution.pdf:pdf},
issn = {2195-5832},
journal = {Journal of Statistical Distributions and Applications},
month = {dec},
number = {1},
pages = {25},
publisher = {Springer Berlin Heidelberg},
title = {{Families of distributions arising from the quantile of generalized lambda distribution}},
url = {http://jsdajournal.springeropen.com/articles/10.1186/s40488-017-0081-4},
volume = {4},
year = {2017}
}
@article{Kassambara2015,
author = {Kassambara, Alboukadel},
file = {:home/nmlemus/Documents/Mendeley Desktop/Alboukadel Kassambara - Practical Guide to Cluster Analysis in R. Unsupervised Machine Learning (2017, sthda.com).pdf:pdf},
title = {{Alboukadel Kassambara Practical Guide To Cluster Analysis in R}},
year = {2015}
}
@article{Rajan2016a,
abstract = {{\textcopyright} 2015 IEEE.Expanded uncertainty estimation is normally required for mission-critical applications, e.g., those involving health and safety. It helps to get a distribution range of the required confidence level for the uncertainty evaluation of a system. There are a number of available techniques to estimate the expanded uncertainty. However, there is currently no commonly accepted benchmark test distribution set adopted to compare the performances of different techniques when they are used to estimate the expanded uncertainty. Without such a common benchmarking platform, the relative reliability of a particular technique in comparison to other techniques can be untrustworthy. To address the shortcoming, this paper proposes a set of analytically derived benchmark test distributions. It goes on to show the benefits of using them by comparing the performance of existing distribution fitting techniques when applied to the moment-based expanded uncertainty evaluation. The most commonly used moment-based distribution fitting techniques, such as Pearson, Tukey's gh, Cornish-Fisher expansion, and extended generalized lambda distributions, are employed as test cases in this paper. The test distribution set proposed in this paper provides a common benchmarking platform for metrologists intending to assess the performance of different expanded uncertainty estimation techniques. Results from the performance comparison would help practitioners to make a better choice of a distribution fitting technique that would best suit their respective systems.},
author = {Rajan, Arvind and Kuang, Ye Chow and Ooi, Melanie Po Leen and Demidenko, Serge N.},
doi = {10.1109/TIM.2015.2507418},
file = {:home/nmlemus/Documents/Mendeley Desktop/TIM{\_}UPLOAD.pdf:pdf},
isbn = {0018-9456 VO  - 65},
issn = {00189456},
journal = {IEEE Transactions on Instrumentation and Measurement},
keywords = {Benchmark distributions,Cornish-Fisher (CF),Monte Carlo (MC),Pearson,Tukey's gh,expanded uncertainty,extended generalized lambda distributions (EGLD),guide to the expression of uncertainty in measurem},
number = {5},
pages = {1022--1034},
title = {{Benchmark Test Distributions for Expanded Uncertainty Evaluation Algorithms}},
volume = {65},
year = {2016}
}
@article{Lima2016,
abstract = {This paper presents general approaches for addressing some of the most important issues in predictive computational oncology concerned with developing classes of predictive models of tumor growth. First, the process of developing mathematical models of vascular tumors evolving in the complex, heterogeneous, macroenvironment of living tissue; second, the selection of the most plausible models among these classes, given relevant observational data; third, the statistical calibration and validation of models in these classes, and finally, the prediction of key Quantities of Interest (QOIs) relevant to patient survival and the effect of various therapies. The most challenging aspects of this endeavor is that all of these issues often involve confounding uncertainties: in observational data, in model parameters, in model selection, and in the features targeted in the prediction. Our approach can be referred to as “model agnostic” in that no single model is advocated; rather, a general approach that explores po...},
author = {Lima, E A B F and Oden, J T and Hormuth, D A and Yankeelov, T E and Almeida, R. C.},
doi = {10.1142/S021820251650055X},
file = {:home/nmlemus/Documents/Mendeley Desktop/Lima et al. - 2016 - Selection, calibration, and validation of models of tumor growth.pdf:pdf},
issn = {0218-2025},
journal = {Mathematical Models and Methods in Applied Sciences},
keywords = {22E46,53C35,57S20,Predictive model,cancer,computational oncology,model calibration,systems biology},
month = {nov},
number = {12},
pages = {2341--2368},
pmid = {28827890},
publisher = {NIH Public Access},
title = {{Selection, calibration, and validation of models of tumor growth}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28827890 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5560997 http://www.worldscientific.com/doi/abs/10.1142/S021820251650055X},
volume = {26},
year = {2016}
}
@article{Patterson2017,
abstract = {Computational models in biology and biomedical science are often constructed to aid people's understanding of phenomena or to inform decisions with socioeconomic consequences. Model credibility is the willingness of people to trust a model's predictions and is often difficult to establish for computational biology models. A 3 × 3 matrix has been proposed to allow such models to be categorised with respect to their testability and epistemic foundation in order to guide the selection of an appropriate process of validation to supply evidence to establish credibility. Three approaches to validation are identified that can be deployed depending on whether a model is deemed untestable, testable or lies somewhere in between. In the latter two cases, the validation process involves the quantification of uncertainty which is a key output. The issues arising due to the complexity and inherent variability of biological systems are discussed and the creation of ‘digital twins' proposed as a means to alleviate the issues and provide a more robust, transparent and traceable route to model credibility and acceptance.},
author = {Patterson, Eann A. and Whelan, Maurice P.},
doi = {10.1016/J.PBIOMOLBIO.2016.08.007},
file = {:home/nmlemus/Documents/Mendeley Desktop/Patterson, Whelan - 2017 - A framework to establish credibility of computational models in biology.pdf:pdf},
issn = {0079-6107},
journal = {Progress in Biophysics and Molecular Biology},
month = {oct},
pages = {13--19},
publisher = {Pergamon},
title = {{A framework to establish credibility of computational models in biology}},
url = {https://www.sciencedirect.com/science/article/pii/S0079610716300219?via{\%}3Dihub},
volume = {129},
year = {2017}
}
@article{Tennøe2018,
abstract = {Computational models in neuroscience typically contain many parameters that are poorly constrained by experimental data. Uncertainty quantification and sensitivity analysis provide rigorous procedures to quantify how the model output depends on this parameter uncertainty. Unfortunately, the application of such methods is not yet standard within the field of neuroscience. Here we present Uncertainpy, an open-source Python toolbox, tailored to perform uncertainty quantification and sensitivity analysis of neuroscience models. Uncertainpy aims to make it easy and quick to get started with uncertainty analysis, without any need for detailed prior knowledge. The toolbox allows uncertainty quantification and sensitivity analysis to be performed on already existing models without needing to modify the model equations or model implementation. Uncertainpy bases its analysis on polynomial chaos expansions, which are more efficient than the more standard Monte-Carlo based approaches. Uncertainpy is tailored for neuroscience applications by its built-in capability for calculating characteristic features in the model output. The toolbox does not merely perform a point-to-point comparison of the " raw " model output (e.g. membrane voltage traces), but can also calculate the uncertainty and sensitivity of salient model response features such as spike timing, action potential width, mean interspike interval, and other features relevant for various neural and neural network models. Uncertainpy comes with several common models and features built in, and including custom models and new features is easy. The aim of the current paper is to present Uncertainpy for the neuroscience community in a user-oriented manner. To demonstrate its broad applicability, we perform an uncertainty quantification and sensitivity analysis on three case studies relevant for neuroscience: the original Hodgkin-Huxley point-neuron model for action potential generation, a multi-compartmental model of a thalamic interneuron implemented in the NEURON simulator, and a sparsely connected recurrent network model implemented in the NEST simulator. A major challenge in computational neuroscience is to specify the often large number of parameters that define the neuron and neural network models. Many of these parameters have an inherent variability, and some may even be actively regulated and change with time. It is important to know how the uncertainty in model parameters affects the model predictions. To address this need we here present Uncertainpy, an open-source Python toolbox tailored to perform uncertainty quantification and sensitivity analysis of neuroscience models.},
author = {Tenn{\o}e, Simen and Halnes, Geir and Einevoll, Gaute T},
doi = {10.1101/274779},
file = {:home/nmlemus/Documents/Mendeley Desktop/Tenn{\o}e, Halnes, Einevoll - 2018 - Uncertainpy A Python toolbox for uncertainty quantification and sensitivity analysis in computatio(2).pdf:pdf},
keywords = {Python SIGNIFICANCE STATEMENT,computational modeling,features,polynomial chaos,quasi-Monte Carlo methods,sensitivity analysis,stochastic modeling,uncertainty quantification},
title = {{Uncertainpy: A Python toolbox for uncertainty quantification and sensitivity analysis in computational neuroscience.}},
url = {http://dx.doi.org/10.1101/274779},
year = {2018}
}
@article{Goh,
abstract = {We present an algorithm for grouping families of probability density functions (pdfs). We exploit the fact that under the square-root re-parametrization, the space of pdfs forms a Riemannian manifold, namely the unit Hilbert sphere. An immediate consequence of this re-parametrization is that different families of pdfs form different submanifolds of the unit Hilbert sphere. Therefore, the problem of clustering pdfs reduces to the problem of clustering multiple sub-manifolds on the unit Hilbert sphere. We solve this problem by first learning a low-dimensional representation of the pdfs using generalizations of local nonlin-ear dimensionality reduction algorithms from Euclidean to Riemannian spaces. Then, by assuming that the pdfs from different groups are separated, we show that the null space of a matrix built from the local representation gives the seg-mentation of the pdfs. We also apply of our approach to the texture segmentation problem in computer vision.},
author = {Goh, Alvina and Vidal, Ren{\'{e}}},
file = {:home/nmlemus/Documents/Mendeley Desktop/Goh, Vidal - Unknown - Unsupervised Riemannian Clustering of Probability Density Functions.pdf:pdf},
keywords = {Manifold learning,manifold clustering,probability density functions},
title = {{Unsupervised Riemannian Clustering of Probability Density Functions}},
url = {https://pdfs.semanticscholar.org/1519/b17a2c7e0bbc6a4995f817a6fac2bcc15b5b.pdf}
}
@article{Loikith,
author = {Loikith, P C and Lintner, B R and Kim, J and Lee, H and Neelin, J D and Waliser, D E},
file = {:home/nmlemus/Documents/Mendeley Desktop/Loikith et al. - Unknown - Classifying reanalysis surface temperature probability density functions (PDFs) over North 1 America with clu.pdf:pdf},
title = {{Classifying reanalysis surface temperature probability density functions (PDFs) over North 1 America with cluster analysis 2}},
url = {http://research.atmos.ucla.edu/csi//REF/pdfs/LoikithEtal2013{\_}revised.pdf}
}
@article{Jiang,
abstract = {—Clustering on uncertain data, one of the essential tasks in mining uncertain data, posts significant challenges on both modeling similarity between uncertain objects and developing efficient computational methods. The previous methods extend traditional partitioning clustering methods like k-means and density-based clustering methods like DBSCAN to uncertain data, thus rely on geometric distances between objects. Such methods cannot handle uncertain objects that are geometrically indistinguishable, such as products with the same mean but very different variances in customer ratings. Surprisingly, probability distributions, which are essential characteristics of uncertain objects, have not been considered in measuring similarity between uncertain objects. In this paper, We systematically model uncertain objects in both continuous and discrete domains, where an uncertain object is modeled as a continuous and discrete random variable, respectively. We use the well known Kullback-Leibler divergence to measure similarity between uncertain objects in both the continuous and discrete cases, and integrate it into partitioning and density-based clustering methods to cluster uncertain objects. Nevertheless, a nave implementation is very costly. Particularly, computing exact KL divergence in the continuous case is very costly or even infeasible. To tackle the problem, we estimate KL divergence in the continuous case by kernel density estimation and employ the fast Gauss transform technique to further speed up the computation. Our extensive experiment results verify the effectiveness, efficiency, and scalability of our approaches.},
author = {Jiang, Bin and Pei, Jian and Tao, Yufei and Lin, Xuemin},
doi = {10.1109/TKDE.2011.221},
file = {:home/nmlemus/Documents/Mendeley Desktop/Jiang et al. - Unknown - CLUSTERING UNCERTAIN DATA BASED ON PROBABILITY DISTRIBUTION SIMILARITY.pdf:pdf},
keywords = {Density estimation,Fast Gauss Transform,Index Terms—Clustering,Probabilistic distribution,Uncertain data},
title = {{CLUSTERING UNCERTAIN DATA BASED ON PROBABILITY DISTRIBUTION SIMILARITY}},
url = {https://pdfs.semanticscholar.org/e172/2c8911b7db1a3114fbd38b3ea5a9e93d1290.pdf}
}
@inproceedings{Zhang2016b,
abstract = {Fabrication process variations are a major source of yield degradation in the nano-scale design of integrated circuits (IC), microelectromechanical systems (MEMS) and photonic circuits. Stochastic spectral methods are a promising technique to quantify the uncertainties caused by process variations. Despite their superior efficiency over Monte Carlo for many design cases, these algorithms suffer from the curse of dimensionality; i.e., their computational cost grows very fast as the number of random parameters increases. In order to solve this challenging problem, this paper presents a high-dimensional uncertainty quantification algorithm from a big-data perspective. Specifically, we show that the huge number of (e.g., {\$}1.5 \backslashtimes 10{\^{}}{\{}27{\}}{\$}) simulation samples in standard stochastic collocation can be reduced to a very small one (e.g., {\$}500{\$}) by exploiting some hidden structures of a high-dimensional data array. This idea is formulated as a tensor recovery problem with sparse and low-rank constraints; and it is solved with an alternating minimization approach. Numerical results show that our approach can simulate efficiently some ICs, as well as MEMS and photonic problems with over 50 independent random parameters, whereas the traditional algorithm can only handle several random parameters.},
archivePrefix = {arXiv},
arxivId = {1611.02256},
author = {Zhang, Zheng and Weng, Tsui Wei and Daniel, Luca},
booktitle = {2016 IEEE 20th Workshop on Signal and Power Integrity, SPI 2016 - Proceedings},
doi = {10.1109/SaPIW.2016.7496314},
eprint = {1611.02256},
file = {:home/nmlemus/Documents/Mendeley Desktop/Zhang, Weng, Daniel - 2016 - A Big-Data Approach to Handle Process Variations Uncertainty Quantification by Tensor Recovery(3).pdf:pdf},
isbn = {9781509003495},
issn = {2475-9481},
keywords = {()},
title = {{A big-data approach to handle process variations: Uncertainty quantification by tensor recovery}},
url = {https://pdfs.semanticscholar.org/e9fe/6a240d77c3e9e68f547df3a06c30eba85be4.pdf},
year = {2016}
}
@article{Tennøe2018a,
abstract = {Computational models in neuroscience typically contain many parameters that are poorly constrained by experimental data. Uncertainty quantification and sensitivity analysis provide rigorous procedures to quantify how the model output depends on this parameter uncertainty. Unfortunately, the application of such methods is not yet standard within the field of neuroscience. Here we present Uncertainpy, an open-source Python toolbox, tailored to perform uncertainty quantification and sensitivity analysis of neuroscience models. Uncertainpy aims to make it easy and quick to get started with uncertainty analysis, without any need for detailed prior knowledge. The toolbox allows uncertainty quantification and sensitivity analysis to be performed on already existing models without needing to modify the model equations or model implementation. Uncertainpy bases its analysis on polynomial chaos expansions, which are more efficient than the more standard Monte-Carlo based approaches. Uncertainpy is tailored for neuroscience applications by its built-in capability for calculating characteristic features in the model output. The toolbox does not merely perform a point-to-point comparison of the "raw'' model output (e.g. membrane voltage traces), but can also calculate the uncertainty and sensitivity of salient model response features such as spike timing, action potential width, mean interspike interval, and other features relevant for various neural and neural network models. Uncertainpy comes with several common models and features built in, and including custom models and new features is easy. The aim of the current paper is to present Uncertainpy for the neuroscience community in a user-oriented manner. To demonstrate its broad applicability, we perform an uncertainty quantification and sensitivity analysis on three case studies relevant for neuroscience: the original Hodgkin-Huxley point-neuron model for action potential generation, a multi-compartmental model of a thalamic interneuron implemented in the NEURON simulator, and a sparsely connected recurrent network model implemented in the NEST simulator.},
author = {Tenn{\o}e, Simen and Halnes, Geir and Einevoll, Gaute T.},
doi = {10.1101/274779},
file = {:home/nmlemus/Documents/Mendeley Desktop/Tenn{\o}e, Halnes, Einevoll - 2018 - Uncertainpy A Python toolbox for uncertainty quantification and sensitivity analysis in computational.pdf:pdf},
journal = {bioRxiv},
month = {mar},
pages = {274779},
publisher = {Cold Spring Harbor Laboratory},
title = {{Uncertainpy: A Python toolbox for uncertainty quantification and sensitivity analysis in computational neuroscience.}},
url = {https://www.biorxiv.org/content/early/2018/03/05/274779},
year = {2018}
}
@article{He2013,
author = {He, Yanyan},
file = {:home/nmlemus/Documents/Mendeley Desktop/He - 2013 - Uncertainty Quantification and Data Fusion Based on Dempster-Shafer Theory.pdf:pdf},
keywords = {Aleatory uncertainty, Data fusion, Dempster-Shafer},
number = {November},
title = {{Uncertainty Quantification and Data Fusion Based on Dempster-Shafer Theory}},
year = {2013}
}
@article{Lampasi2006,
abstract = {The generalized lambda distribution family fits the probability distributions of a wide variety of data sets, including the most important distributions encountered in the measurement applications (normal, uniform, Student's t, U-shaped, exponential). This paper illustrates how the four parameters needed for such distribution can be exploited in the expression of measurement uncertainty and to extend the information related to a measurement. The obtained representation allows an immediate calculation of coverage intervals and is particularly useful to support the techniques commonly applied in the estimation of the combined uncertainty. Moreover, in order to include the classical measurement information, a novel parameterization of the distribution is proposed.},
author = {Lampasi, Domenico Alessandro and {Di Nicola}, Fabio and Podesta, Luca},
doi = {10.1109/TIM.2006.876408},
file = {:home/nmlemus/Documents/Mendeley Desktop/Lampasi, Di Nicola, Podesta - 2006 - Generalized lambda distribution for the expression of measurement uncertainty.pdf:pdf},
isbn = {0780388798},
issn = {00189456},
journal = {IEEE Transactions on Instrumentation and Measurement},
keywords = {Calibration,Digital signal processing,Measurement uncertainty,Monte Carlo method,Probability,Virtual instruments},
number = {4},
pages = {1281--1287},
title = {{Generalized lambda distribution for the expression of measurement uncertainty}},
volume = {55},
year = {2006}
}
@article{Ning2008,
abstract = {Mixture models were studied by Karl Pearson in 1894 when he fitted a mixture of two normal distributions to data consisting of measurements on the ratio of forehead to body length in 1000 crabs. Most work since that time has used mixtures of normal distributions. In this paper, we consider a model for mixtures of generalized lambda distributions (GLDs). The advantage of using the GLD family is that the GLD can fit the normal well, hence whenever a mixture of normals will fit data well, so will a mixture of at most the same number of GLDs. Meanwhile, the GLD family is a much broader family, and can do well in cases where the normal cannot. In this paper, we fit Pearson's data by using the mixture of two GLDs. We also show the change of shapes of the mixtures with different proportions. We include examples and computational considerations compared with normal mixtures by using Kullback-Leibler (KL) distance and overlapping coefficient ($\delta$).},
author = {Ning, Wei and Gao, Yunchuan and Dudewicz, Edward J.},
doi = {10.1080/01966324.2008.10737718},
file = {:home/nmlemus/Documents/Mendeley Desktop/Ning, Gao, Dudewicz - 2008 - Fitting mixture distributions using generalized lambda distributions and comparison with normal mixtures.pdf:pdf},
issn = {01966324},
journal = {American Journal of Mathematical and Management Sciences},
keywords = {Generalized lambda distributions,Mixture models,Normal mixtures},
number = {1-2},
pages = {81--99},
title = {{Fitting mixture distributions using generalized lambda distributions and comparison with normal mixtures}},
volume = {28},
year = {2008}
}
@article{Chen2008,
abstract = {We extract from the yield curve a new measure of fundamental economic uncertainty, based on McDiarmid's distance and related methods for optimal uncertainty quantification (OUQ). OUQ seeks analytical bounds on a system's behavior, even where the underlying data-generating process and system response function are incompletely specified. We use OUQ to stress test a simple fixed-income portfolio, certifying its safety—i.e., that potential losses will be " small " in an appropriate sense. The results give explicit tradeoffs between: scenario count, maximum loss, test horizon, and confidence level. Unfortunately, uncertainty peaks in late 2008, weakening certification assurances just when they are needed most.},
author = {Chen, Jingnan and Flood, Mark D and Sowers, Richard B},
doi = {10.1080/14697688.2017.1296176},
file = {:home/nmlemus/Documents/Mendeley Desktop/Chen, Flood, Sowers - 2008 - Measuring the Unmeasurable An Application of Uncertainty Quantification to Financial Portfolios Measuring t.pdf:pdf},
issn = {14697696},
journal = {Quantitative Finance},
keywords = {model risk,optimal uncertainty quantification,stress testing,uncertainty,yield curve},
number = {January},
pages = {1--18},
title = {{Measuring the Unmeasurable: An Application of Uncertainty Quantification to Financial Portfolios Measuring the Unmeasurable An application of uncertainty quantification to financial portfolios}},
url = {http://dx.doi.org/10.1080/14697688.2017.1296176},
volume = {7688},
year = {2008}
}
@article{Hausser2008,
abstract = {We present a procedure for effective estimation of entropy and mutual information from small-sample data, and apply it to the problem of inferring high-dimensional gene association networks. Specifically, we develop a James-Stein-type shrinkage estimator, resulting in a procedure that is highly efficient statistically as well as computationally. Despite its simplicity, we show that it outperforms eight other entropy estimation procedures across a diverse range of sampling scenarios and data-generating models, even in cases of severe undersampling. We illustrate the approach by analyzing E. coli gene expression data and computing an entropy-based gene-association network from gene expression data. A computer program is available that implements the proposed shrinkage estimator.},
archivePrefix = {arXiv},
arxivId = {0811.3579},
author = {Hausser, Jean and Strimmer, Korbinian},
eprint = {0811.3579},
file = {:home/nmlemus/Documents/Mendeley Desktop/Hausser, Strimmer - 2008 - Entropy inference and the James-Stein estimator, with application to nonlinear gene association networks.pdf:pdf},
isbn = {1532-4435},
issn = {{\textless}null{\textgreater}},
keywords = {acknowledgments this work was,emmy noether grant of,entropy,gene association network,james-stein estimator,large p,mutual information,partially supported by an,referees and,s,setting,shrinkage estimation,small n,the deutsche forschungsgemeinschaft,to k,we thank the anonymous},
number = {October 2008},
pages = {1--18},
title = {{Entropy inference and the James-Stein estimator, with application to nonlinear gene association networks}},
url = {http://arxiv.org/abs/0811.3579},
year = {2008}
}
@phdthesis{Mustafa2016,
author = {{Mustafa Inchasi}, Enas Atef},
file = {:home/nmlemus/Documents/Mendeley Desktop/Mustafa Inchasi - 2016 - The Generalized Lambda Distribution and Its Use in Fitting Distributions to Data.pdf:pdf},
title = {{The Generalized Lambda Distribution and Its Use in Fitting Distributions to Data}},
year = {2016}
}
@article{Pebesma2012,
abstract = {We address the relative paucity of empirical testing of learning algorithms (of any type) by introducing a new public-domain, Modular, Optimal Learning Testing Environment (MOLTE) for Bayesian ranking and selection problem, stochastic bandits or sequential experimental design problems. The Matlab-based simulator allows the comparison of a number of learning policies (represented as a series of .m modules) in the context of a wide range of problems (each represented in its own .m module) which makes it easy to add new algorithms and new test problems. State-of-the-art policies and various problem classes are provided in the package. The choice of problems and policies is guided through a spreadsheet-based interface. Different graphical metrics are included. MOLTE is designed to be compatible with parallel computing to scale up from local desktop to clusters and clouds. We offer MOLTE as an easy-to-use tool for the research community that will make it possible to perform much more comprehensive testing, spanning a broader selection of algorithms and test problems. We demonstrate the capabilities of MOLTE through a series of comparisons of policies on a starter library of test problems. We also address the problem of tuning and constructing priors that have been largely overlooked in optimal learning literature. We envision MOLTE as a modest spur to provide researchers an easy environment to study interesting questions involved in optimal learning.},
archivePrefix = {arXiv},
arxivId = {1709.04553},
author = {Pebesma, Edzer},
doi = {10.18637/jss.v000.i00},
eprint = {1709.04553},
file = {:home/nmlemus/Documents/Mendeley Desktop/Pebesma - 2012 - spacetime Spatio-Temporal Data in R.pdf:pdf},
keywords = {geographic informa-,spatial data,spatio-temporal statistics,time series analysis},
number = {7},
title = {{spacetime: Spatio-Temporal Data in R}},
url = {http://arxiv.org/abs/1709.04553},
volume = {51},
year = {2012}
}
@article{RESSTENetwork2016,
author = {{RESSTE Network}},
file = {:home/nmlemus/Documents/Mendeley Desktop/RESSTE Network - 2016 - Analyzing spatio-temporal data with R Everything you always wanted to know – but were afraid to ask.pdf:pdf},
isbn = {9781450342056},
journal = {Journal de la Soci{\'{e}}t{\'{e}} Fran{\c{c}}aise de Statistique},
keywords = {62-01,62-07,62f99,62m40,air pollution,ams 2000 subject classifications,covariance function,fonction de covariance,geostatistics,g{\'{e}}ostatistique,krigeage,kriging,mots-cl{\'{e}}s,pollution atmosph{\'{e}}rique,space-time},
title = {{Analyzing spatio-temporal data with R : Everything you always wanted to know – but were afraid to ask}},
url = {http://informatique-mia.inra.fr/resste/sites/informatique-mia.inra.fr.resste/files/analyzing-spatio-temporal.pdf},
year = {2016}
}
@article{Oladyshkin2011,
abstract = {Model-based uncertainty analysis can help to judge the potentials and hazard in many engineering applications better. This requires to specify the probability distributions of all model parameters, posing a huge demand on data availability or requiring highly subjective assumptions on distribution shapes to compensate for missing data. We present a minimally subjective approach for uncertainty quantification in data-sparse situations, based on a new and purely data-driven version of polynomial chaos expansion (PCE). It avoids the subjectivity that is otherwise introduced when choosing among a small limited number of theoretical distribution shapes to represent natural phenomena: we only demand the existence of a finite number of statistical moments, and do not require knowledge or even the existence of probability density functions for input parameters. In a small fictitious example with independent experts, otherwise, we demonstrate that this subjectivity can easily lead to substantial prediction bias, and that the subjective choice of distribution shapes has a similar relevance as uncertainties due to physical conceptualization, numerical codes and parameter uncertainty. With our approach we can directly and most flexibly use raw data sets available from global databases or soft information from experts in the form of arbitrary distributions or statistical moments. We illustrate and validate our proposed approach by a comparison with a Monte Carlo simulation using a common 3D benchmark problem for CO2injection, which is a low-parametric homogeneous system. We obtain a significant computational speed-up compared with Monte Carlo as well as high accuracy even for small orders of expansion, and show how our novel approach helps overcome subjectivity. {\textcopyright} 2011 Elsevier Ltd.},
author = {Oladyshkin, S. and Class, H. and Helmig, R. and Nowak, W.},
doi = {10.1016/j.advwatres.2011.08.005},
file = {:home/nmlemus/Documents/Mendeley Desktop/Oladyshkin et al. - 2011 - A concept for data-driven uncertainty quantification and its application to carbon dioxide storage in geologi.pdf:pdf},
isbn = {0309-1708},
issn = {03091708},
journal = {Advances in Water Resources},
keywords = {Arbitrary distribution,CO2 storage,Data-driven,Orthonormal basis,Polynomial chaos,Uncertainty},
number = {11},
pages = {1508--1518},
publisher = {Elsevier Ltd},
title = {{A concept for data-driven uncertainty quantification and its application to carbon dioxide storage in geological formations}},
url = {http://dx.doi.org/10.1016/j.advwatres.2011.08.005},
volume = {34},
year = {2011}
}
@article{Zhang2016a,
abstract = {Stochastic spectral methods have become a popular technique to quantify the uncertainties of nano-scale devices and circuits. They are much more efficient than Monte Carlo for certain design cases with a small number of random parameters. However, their computational cost significantly increases as the number of random parameters increases. This paper presents a big-data approach to solve high-dimensional uncertainty quantification problems. Specifically, we simulate integrated circuits and MEMS at only a small number of quadrature samples, then, a huge number of (e.g., {\$}1.5 \backslashtimes 10{\^{}}{\{}27{\}}{\$}) solution samples are estimated from the available small-size (e.g., {\$}500{\$}) solution samples via a low-rank and tensor-recovery method. Numerical results show that our algorithm can easily extend the applicability of tensor-product stochastic collocation to IC and MEMS problems with over 50 random parameters, whereas the traditional algorithm can only handle several random parameters.},
archivePrefix = {arXiv},
arxivId = {1603.06119},
author = {Zhang, Zheng and Weng, Tsui-Wei and Daniel, Luca},
eprint = {1603.06119},
file = {:home/nmlemus/Documents/Mendeley Desktop/Zhang, Weng, Daniel - 2016 - A Big-Data Approach to Handle Process Variations Uncertainty Quantification by Tensor Recovery(2).pdf:pdf},
number = {2},
title = {{A Big-Data Approach to Handle Process Variations: Uncertainty Quantification by Tensor Recovery}},
url = {http://arxiv.org/abs/1603.06119},
year = {2016}
}
@article{Su2007,
author = {Su, Steve},
file = {:home/nmlemus/Documents/Mendeley Desktop/Su - 2007 - Fitting Single and Mixture of Generalized Lambda Distributions to Data via Discretized and Maximum Likelihood Methods GLDEX.pdf:pdf},
journal = {Journal of Statistical Software},
keywords = {generalized lambda distributions,maximum likelihood estimation,mixtures,smooth-},
number = {9},
title = {{Fitting Single and Mixture of Generalized Lambda Distributions to Data via Discretized and Maximum Likelihood Methods: GLDEX in R}},
volume = {21},
year = {2007}
}
@article{Chen2017,
abstract = {ABSTRACTThe simultaneous-source shooting technique can accelerate field acquisition and improve spatial sampling but it will cause strong interferences in the recorded data and artifacts in the final image. The previously proposed structural smoothing operator can effectively attenuate artifacts for relatively simple reflection structures during least-squares inversion, but it will cause damage to complicated reflection events such as discontinuities. To preserve discontinuities in a seismic image, we apply the singular spectrum analysis (SSA) operator to attenuate artifacts during least-squares inversion. Considering that global SSA cannot deal with overcomplicated data very well, we use local SSA to remove noise and to better preserve the steeply dipping components. The local SSA operator corresponds to a local low-rank constraint applied in the inversion process. The migration operator used in the study is the reverse time migration (RTM) operator. Tests using the Marmousi model showed the superior per...},
author = {Chen, Yangkang and Chen, Hanming and Xiang, Kui and Chen, Xiaohong},
doi = {10.1190/geo2016-0456.1},
issn = {0016-8033},
journal = {Geophyscis},
keywords = {discontinuities,least-squares reverse time migration,noise,simultaneous source acquisition,singular spectrum analysis},
month = {may},
number = {3},
pages = {S185--S196},
publisher = {Society of Exploration Geophysicists},
title = {{Preserving the discontinuities in least-squares reverse time migration of simultaneous-source data}},
url = {http://library.seg.org/doi/10.1190/geo2016-0456.1 http://library.seg.org/doi/10.1190/segam2016-13862862.1{\%}0Ahttp://library.seg.org/doi/10.1190/geo2016-0456.1},
volume = {82},
year = {2017}
}
@article{Xue2016,
abstract = {See, stats, and : https : / / www . researchgate . net / publication / 283571070 Seismic simultaneous-source - squares reverse regularization ARTICLE Impact : 1 . 61 : 10 . 1190 / GEO2014 - 0524 . 1 CITATION 1 READS 145 4 : Zhiguang University 2 SEE Yangkang University 80 SEE Sergey University 299 , 800 SEE Junzhe University 5 SEE All - text , letting . Available : Yangkang Retrieved : 24 ABSTRACT Simultaneous - source acquisition improves the efficiency of the seismic data acquisition process . However , direct im - aging of simultaneous - source data may introduce crosstalk artifacts in the final image . Likewise , direct imaging of in - complete data avoids the step of data reconstruction , but it can suffer from migration artifacts . We have proposed to in - corporate shaping regularization into least - squares reverse time migration (LSRTM) and use it for suppressing interfer - ence noise caused by simultaneous - source data or migration artifacts caused by incomplete data . To implement LSRTM , we have applied lowrank one - step reverse time migration and its adjoint iteratively in the conjugate - gradient algorithm to minimize the data misfit . A shaping operator imposing structure constraints on the estimated model was applied at each iteration . We constructed the shaping operator as a structure - enhancing filtering to attenuate migration arti - facts and crosstalk noise while preserving structural infor - mation . We have carried out numerical tests on synthetic models in which the proposed method exhibited a fast con - vergence rate and was effective in attenuating migration ar - tifacts and crosstalk noise .},
author = {Xue, Zhiguang and Chen, Yangkang and Fomel, Sergey and Sun, Junzhe},
doi = {10.1190/GEO2014-0524.1},
issn = {0016-8033},
journal = {GEOPHYSICS},
keywords = {RTM,imaging,least squares,migration,noise},
month = {jan},
number = {1},
pages = {S11--S20},
publisher = {Society of Exploration Geophysicists},
title = {{Seismic imaging of incomplete data and simultaneous - source data using least - squares reverse time migration with shaping regularization}},
url = {http://library.seg.org/doi/10.1190/geo2014-0524.1},
volume = {81},
year = {2016}
}
@article{Chen2015,
abstract = {Simultaneous-source acquisition can help obtain better-sampled seismic data with a tremendously faster efficiency. However, the interference caused between simultaneous sources will cause strong artifacts in the migrated image. We propose to directly migrate the blended data without deblending. We treat the imaging for blended seismic data as a least-squares inversion problem, and use preconditioned conjugate gradient algorithm to iteratively solve it. The constraining operator is chosen as the structural smoothing operator, which helps to smooth the seismic image along the local structure. Compared with other available approaches, the proposed approach can obtain a true-amplitude and much cleaner image.},
author = {Chen, Yangkang and Yuan, Jiang and Zu, Shaohuan and Qu, Shan and Gan, Shuwei},
doi = {10.1016/j.jappgeo.2015.01.004},
issn = {09269851},
journal = {Journal of Applied Geophysics},
keywords = {Constrained LSRTM,Least-squares reverse time migration (LSRTM),Preconditioned conjugate gradient,Simultaneous-source acquisition},
month = {mar},
pages = {32--35},
publisher = {Elsevier},
title = {{Seismic imaging of simultaneous-source data using constrained least-squares reverse time migration}},
url = {https://www.sciencedirect.com/science/article/pii/S0926985115000117?via{\%}3Dihub},
volume = {114},
year = {2015}
}
@book{Karian2011,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Karian, Zaven A. and Dudewicz, Edward J.},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/nmlemus/Documents/Mendeley Desktop/Karian, Dudewicz - 2011 - Handbook of fitting statistical distributions with R.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
pmid = {25246403},
title = {{Handbook of fitting statistical distributions with R}},
year = {2011}
}
@article{Bilkova2014,
author = {B{\'{i}}lkov{\'{a}}, Diana and Republic, Czech},
doi = {10.11648/j.pamj.20140302.11},
file = {:home/nmlemus/Documents/Mendeley Desktop/B{\'{i}}lkov{\'{a}}, Republic - 2014 - Alternative Means of Statistical Data Analysis L-Moments and TL-Moments of Probability Distributions.pdf:pdf},
issn = {2326-9790},
keywords = {and tl-moments,distribution function,income distribution,l-moments and tl-moments of,mikrocensus,order statistics,probability density function,probability distribution,quantile function,sample l-moments},
number = {2},
pages = {77--94},
title = {{Alternative Means of Statistical Data Analysis : L-Moments and TL-Moments of Probability Distributions}},
volume = {94},
year = {2014}
}
@article{DeBaar2012,
abstract = {Techniques for Uncertainty Quantification (UQ) suffer from the 'curse of dimen-sionality': the number of required evaluations of the simulation code increases rapidly as the number of uncertain variables increases. Fluid-Structure Interaction (FSI) problems can in-volve complex physics as well as a large number of random input variables. The objective of the current work is to mitigate the curse of dimensionality by including adjoint-based gradient information from the FSI problem. For a FSI problem we increase the number of random struc-ture variables from 1 to 16. We apply a UQ response surface technique known as Kriging, and observe the computational effort that is required to obtain a certain target accuracy. When in-cluding gradient information – a technique known as Gradient-Enhanced Kriging (GEK) – we find a speedup that increases with the number of random variables. For example, for 4 random variables we observe a speedup of 3.0, while for 16 random variables we observe a speedup of 9.8. We conclude that including gradient information can lead to significant speedups.},
author = {{De Baar}, Jouke H S and Scholcz, Thomas P and Verhoosel, Clemens V and Dwight, Richard P and {Van Zuijlen}, Alexander H and Bijl, Hester},
file = {:home/nmlemus/Documents/Mendeley Desktop/De Baar et al. - 2012 - Efficient Uncertainty Quantification With Gradient-Enhanced Kriging Applications in Fsi.pdf:pdf},
isbn = {9783950353709},
journal = {European Congress on Computational Methods in Applied Sciences and Engineering},
keywords = {Fluid-structure interac-tion,Gradient-Enhanced Kriging,Uncertainty Quantification,adjoint gradients,response surfaces},
number = {Eccomas},
pages = {1--12},
title = {{Efficient Uncertainty Quantification With Gradient-Enhanced Kriging: Applications in Fsi}},
year = {2012}
}
@article{Fournier2007,
abstract = {The method of moments is a popular technique for estimating the parameters of a generalized lambda distribution (GLD), but published results suggest that the percentile method gives superior results. However, the percentile method cannot be implemented in an automatic fashion, and automatic methods, like the starship method, can lead to prohibitive execution time with large sample sizes. A new estimation method is proposed that is automatic (it does not require the use of special tables or graphs), and it reduces the computational time. Based partly on the usual percentile method, this new method also requires choosing which quantile u to use when fitting a GLD to data. The choice for u is studied and it is found that the best choice depends on the final goal of the modeling process. The sampling distribution of the new estimator is studied and compared to the sampling distribution of estimators that have been proposed. Naturally, all estimators are biased and here it is found that the bias becomes negligible with sample sizes n ≥ 2 × 103. The .025 and .975 quantiles of the sampling distribution are investigated, and the difference between these quantiles is found to decrease proportionally to 1 / sqrt(n). The same results hold for the moment and percentile estimates. Finally, the influence of the sample size is studied when a normal distribution is modeled by a GLD. Both bounded and unbounded GLDs are used and the bounded GLD turns out to be the most accurate. Indeed it is shown that, up to n = 106, bounded GLD modeling cannot be rejected by usual goodness-of-fit tests. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
author = {Fournier, B. and Rupin, N. and Bigerelle, M. and Najjar, D. and Iost, A. and Wilcox, R.},
doi = {10.1016/j.csda.2006.09.043},
file = {:home/nmlemus/Documents/Mendeley Desktop/Fournier et al. - 2007 - Estimating the parameters of a generalized lambda distribution.pdf:pdf},
isbn = {0167-9473},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {Estimating distributions,GLD,Goodness-of-fit,Percentiles,Simplex},
number = {6},
pages = {2813--2835},
title = {{Estimating the parameters of a generalized lambda distribution}},
volume = {51},
year = {2007}
}
@article{Partner2016,
author = {Partner, Lead and Revision, W U},
file = {:home/nmlemus/Documents/Mendeley Desktop/Partner, Revision - 2016 - QUICS Quantifying Uncertainty in Integrated Catchment Studies 2 . 1 Software tools for quantifying uncertain.pdf:pdf},
pages = {1--38},
title = {{QUICS : Quantifying Uncertainty in Integrated Catchment Studies 2 . 1 Software tools for quantifying uncertainty across different scales}},
year = {2016}
}
@article{Baroni2014a,
abstract = {The present study proposes a General Probabilistic Framework (GPF) for uncertainty and global sensitivity analysis of deterministic models in which, in addition to scalar inputs, non-scalar and correlated inputs can be considered as well. The analysis is conducted with the variance-based approach of Sobol/Saltelli where first and total sensitivity indices are estimated. The results of the framework can be used in a loop for model improvement, parameter estimation or model simplification. The framework is applied to SWAP, a 1D hydrological model for the transport of water, solutes and heat in unsaturated and saturated soils. The sources of uncertainty are grouped in five main classes: model structure (soil discretization), input (weather data), time-varying (crop) parameters, scalar parameters (soil properties) and observations (measured soil moisture). For each source of uncertainty, different realizations are created based on direct monitoring activities. Uncertainty of evapotranspiration, soil moisture in the root zone and bottom fluxes below the root zone are considered in the analysis. The results show that the sources of uncertainty are different for each output considered and it is necessary to consider multiple output variables for a proper assessment of the model. Improvements on the performance of the model can be achieved reducing the uncertainty in the observations, in the soil parameters and in the weather data. Overall, the study shows the capability of the GPF to quantify the relative contribution of the different sources of uncertainty and to identify the priorities required to improve the performance of the model. The proposed framework can be extended to a wide variety of modelling applications, also when direct measurements of model output are not available. {\textcopyright} 2013 Elsevier Ltd.},
author = {Baroni, G. and Tarantola, S.},
doi = {10.1016/j.envsoft.2013.09.022},
file = {:home/nmlemus/Documents/Mendeley Desktop/Baroni, Tarantola - 2014 - A General Probabilistic Framework for uncertainty and global sensitivity analysis of deterministic models A h.pdf:pdf},
isbn = {3319772805},
issn = {13648152},
journal = {Environmental Modelling and Software},
keywords = {Global sensitivity analysis,Hydrological model,Multi-variables,Non-scalar input factors},
pages = {26--34},
publisher = {Elsevier Ltd},
title = {{A General Probabilistic Framework for uncertainty and global sensitivity analysis of deterministic models: A hydrological case study}},
url = {http://dx.doi.org/10.1016/j.envsoft.2013.09.022},
volume = {51},
year = {2014}
}
@book{Higdon2017,
abstract = {These lecture notes highlight the mathematical and computational structure relating to the formulation of, and development of algorithms for, the Bayesian approach to inverse problems in differential equations. This approach is fundamental in the quantification of uncertainty within applications in volving the blending of mathematical models with data. The finite dimensional situation is described first, along with some motivational examples. Then the development of probability measures on separable Banach space is undertaken, using a random series over an infinite set of functions to construct draws; these probability measures are used as priors in the Bayesian approach to inverse problems. Regularity of draws from the priors is studied in the natural Sobolev or Besov spaces implied by the choice of functions in the random series construction, and the Kolmogorov continuity theorem is used to extend regularity considerations to the space of H{\"{o}}lder continuous functions. Bayes' theorem is de rived in this prior setting, and here interpreted as finding conditions under which the posterior is absolutely continuous with respect to the prior, and determining a formula for the Radon-Nikodym derivative in terms of the likelihood of the data. Having established the form of the posterior, we then describe various properties common to it in the infinite dimensional setting. These properties include well-posedness, approximation theory, and the existence of maximum a posteriori estimators. We then describe measure-preserving dynamics, again on the infinite dimensional space, including Markov chain-Monte C arlo and sequential Monte Carlo methods, and measure-preserving reversible stochastic differential equations. By formulating the theory and algorithms on the underlying infinite dimensional space, we obtain a framework suitable for rigorous analysis of the accuracy of reconstructions, of computational complexity, as well as naturally constructing algorithms which perform well under mesh refinement, since they are inherently well-defined in infinite dimensions.},
archivePrefix = {arXiv},
arxivId = {1507.00398},
author = {Higdon, David},
doi = {10.1007/978-3-319-12385-1},
eprint = {1507.00398},
file = {:home/nmlemus/Documents/Mendeley Desktop/Higdon - 2017 - Handbook of Uncertainty Quantification.pdf:pdf},
isbn = {978-3-319-12384-4},
title = {{Handbook of Uncertainty Quantification}},
url = {http://link.springer.com/10.1007/978-3-319-12385-1},
year = {2017}
}
@article{Zammit-Mangion2017,
abstract = {FRK is an R software package for spatial/spatio-temporal modelling and prediction with large datasets. It facilitates optimal spatial prediction (kriging) on the most commonly used manifolds (in Euclidean space and on the surface of the sphere), for both spatial and spatio-temporal fields. It differs from existing packages for spatial modelling and prediction by avoiding stationary and isotropic covariance and variogram models, instead constructing a spatial random effects (SRE) model on a discretised spatial domain. The discrete element is known as a basic areal unit (BAU), whose introduction in the software leads to several practical advantages. The software can be used to (i) integrate multiple observations with different supports with relative ease; (ii) obtain exact predictions at millions of prediction locations (without conditional simulation); and (iii) distinguish between measurement error and fine-scale variation at the resolution of the BAU, thereby allowing for improved uncertainty quantification when compared to related packages. The temporal component is included by adding another dimension. A key component of the SRE model is the specification of spatial or spatio-temporal basis functions; in the package, they can be generated automatically or by the user. The package also offers automatic BAU construction, an expectation maximisation (EM) algorithm for parameter estimation, and functionality for prediction over any user-specified polygons or BAUs. Use of the package is illustrated on several spatial and spatio-temporal datasets, and it is compared to two extensively used methods in spatial prediction and modelling, namely the package LatticeKrig and the stochastic partial differential equation tools in INLA.},
archivePrefix = {arXiv},
arxivId = {1705.08105},
author = {Zammit-Mangion, Andrew and Cressie, Noel},
eprint = {1705.08105},
file = {:home/nmlemus/Documents/Mendeley Desktop/Zammit-Mangion, Cressie - 2017 - FRK An R Package for Spatial and Spatio-Temporal Prediction with Large Datasets.pdf:pdf},
keywords = {basic areal units,em algorithm,fixed rank kriging,spatial prediction,spatial ran-},
number = {2007},
title = {{FRK: An R Package for Spatial and Spatio-Temporal Prediction with Large Datasets}},
url = {http://arxiv.org/abs/1705.08105},
year = {2017}
}
@book{Salicone2000,
author = {Salicone, Simona},
booktitle = {Methods},
isbn = {9780387306551},
title = {{Measurement Uncertainty An Approach via the Mathematical Theory of Evidence}},
year = {2000}
}
@article{Acar2008,
author = {Acar, Erdem and Rais-rohani, Masoud and Eamon, Christopher D},
doi = {10.2514/6.2008-1893},
file = {:home/nmlemus/Documents/Mendeley Desktop/a.pdf:pdf},
isbn = {978-1-60086-993-8},
number = {April},
pages = {1--15},
title = {{Reliability Estimation using Dimension Reduction and Extended Generalized Lambda Distribution}},
year = {2008}
}
@book{Andrade2003,
author = {Andrade, Eliana X L De},
booktitle = {Algoritmos Evolutivos},
doi = {10.5540/001.2012.0066.01},
isbn = {9788582150245},
pages = {103},
title = {{Quantificacao De Incertezas E Estimacao De Parametros Em Dinamica Estrutural: Uma Introducao a Partir De Exemplos Computacionais}},
volume = {4},
year = {2003}
}
@article{Kawai2014,
author = {Kawai, Soshi and Shimoyama, Koji},
doi = {10.2514/6.2014-2737},
file = {:home/nmlemus/Documents/Mendeley Desktop/Kawai, Shimoyama - 2014 - Kriging-model-based uncertainty quantification in computational fluid dynamics.pdf:pdf},
isbn = {978-1-62410-288-2},
journal = {32nd AIAA Applied Aerodynamics Conference},
number = {June},
pages = {1--16},
title = {{Kriging-model-based uncertainty quantification in computational fluid dynamics}},
url = {http://arc.aiaa.org/doi/10.2514/6.2014-2737},
year = {2014}
}
@article{Shu2017,
author = {{Jiang Shu, Layne T. Watson, Naren Ramakrishnan, Frederick A. Kamke}, Shubhangi Deshpande},
doi = {10.1007/s00366-010-0192-8},
file = {:home/nmlemus/Documents/Mendeley Desktop/Jiang Shu, Layne T. Watson, Naren Ramakrishnan, Frederick A. Kamke - 2017 - Computational steering in the problem solving environment WB.pdf:pdf},
issn = {01770667},
journal = {Engineering with Computers},
keywords = {Experiment management,Optimization,Problem solving environment,Response surface approximation,Sequential approximate optimization,Surrogate,Trust region strategy,Visualization,Wood-based composite materials},
number = {3},
pages = {211--223},
title = {{Computational steering in the problem solving environment WBCSim}},
volume = {27},
year = {2017}
}
@article{Kennedy2010,
abstract = {Estimation via sampling out of highly selective join queries is well known to be problematic, most notably in online aggregation. Without goal-directed sampling strategies, samples falling outside of the selection constraints lower estimation efficiency at best, and cause inaccurate estimates at worst This problem appears in general probabilistic database systems, where query processing is tightly coupled with sampling. By committing to a set of samples before evaluating the query, the engine wastes effort on samples that will be discarded, query processing that may need to be repeated, or unnecessarily large numbers of samples. We describe PIP, a general probabilistic database system that uses symbolic representations of probabilistic data to defer computation of expectations, moments, and other statistical measures until the expression to be measured is fully known. This approach is sufficiently general to admit both continuous and discrete distributions. Moreover, deferring sampling enables a broad range of goal-oriented sampling-based (as well as exact) integration techniques for computing expectations, allows the selection of the integration strategy most appropriate to the expression being measured, and can reduce the amount of sampling work required. We demonstrate the effectiveness of this approach by showing that even straightforward algorithms can make use of the added information. These algorithms have a profoundly positive impact on the efficiency and accuracy of expectation computations, particularly in the case of highly selective join queries.},
author = {Kennedy, Oliver and Koch, Christoph},
doi = {10.1109/ICDE.2010.5447879},
file = {:home/nmlemus/Documents/Mendeley Desktop/Kennedy, Koch - 2010 - PIP A database system for great and small expectations.pdf:pdf},
isbn = {9781424454440},
issn = {10844627},
journal = {Proceedings - International Conference on Data Engineering},
pages = {157--168},
title = {{PIP: A database system for great and small expectations}},
year = {2010}
}
@book{Schlick2006,
abstract = {The development of high performance, massively parallel computers and the increasing demands of computationally challenging applications have ne-cessitated the development of scalable solvers and preconditioners. One of the most effective ways to achieve scalability is the use of multigrid or multilevel techniques. Algebraic multigrid (AMG) is a very efficient algorithm for solving large problems on unstructured grids. While much of it can be parallelized in a straightforward way, some components of the classical algorithm, particularly the coarsening process and some of the most efficient smoothers, are highly sequential, and require new par-allel approaches. This chapter presents the basic principles of AMG and gives an overview of various parallel implementations of AMG, including descriptions of par-allel coarsening schemes and smoothers, some numerical results as well as references to existing software packages.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Schlick, Timothy J. Barth Michael Griebel David E. Keyes RistoM. Nieminen Dirk Roose Tamar},
booktitle = {Numer. Solu. Partial Differ. Equ. Parallel Comput.},
doi = {10.1007/3-540-31619-1},
eprint = {arXiv:1011.1669v3},
isbn = {3-540-29076-1},
issn = {1439-7358},
pages = {209--236},
pmid = {25246403},
title = {{Lecture Notes in Computational Science and Engineering}},
volume = {51},
year = {2006}
}
@article{Graler2016,
abstract = {We present new spatio-temporal geostatistical modelling and interpolation capabilities of the R package gstat. Various spatio-temporal covariance models have been implemented, such as the separable, product-sum, metric and sum-metric models. In a real-world application we compare spatio-temporal interpolations using these models with a purely spatial kriging approach. The target variable of the application is the daily mean PM10 concentration measured at rural air quality monitoring stations across Germany in 2005. R code for variogram fitting and interpolation is presented in this paper to illustrate the workflow of spatio-temporal interpolation using gstat. We conclude that the system works properly and that the extension of gstat facilitates and eases spatio-temporal geostatistical modelling and prediction for R users.},
author = {Graler, Benedikt and Pebesma, Edzer and Heuvelink, Gerard},
file = {:home/nmlemus/Documents/Mendeley Desktop/Graler, Pebesma, Heuvelink - 2016 - Spatio-Temporal Interpolation using gstat.pdf:pdf},
issn = {20734859},
journal = {Wp},
pages = {1--20},
title = {{Spatio-Temporal Interpolation using gstat}},
volume = {8},
year = {2016}
}
@article{Aggarwal2013,
author = {Aggarwal, Charu C.},
file = {:home/nmlemus/Documents/Mendeley Desktop/Aggarwal - 2013 - A Survey of Uncertain Data Clustering Algorithms.pdf:pdf},
journal = {Data Clustering: Algorithms and Applications},
number = {5},
pages = {455--480},
title = {{A Survey of Uncertain Data Clustering Algorithms}},
volume = {21},
year = {2013}
}
@phdthesis{Dallachiesa2014,
author = {Dallachiesa, Michele},
file = {:home/nmlemus/Documents/Mendeley Desktop/Dallachiesa - 2014 - Modeling and Querying Data Series and Data Streams with Uncertainty.pdf:pdf},
number = {March},
school = {University of Trento},
title = {{Modeling and Querying Data Series and Data Streams with Uncertainty}},
url = {http://eprints-phd.biblio.unitn.it/1223/1/FinalSubmissionPhD.pdf},
year = {2014}
}
@article{Chkrebtii2016,
abstract = {We explore probability modelling of discretization uncertainty for system states defined implicitly by ordinary or partial differential equations. Accounting for this uncertainty can avoid posterior under-coverage when likelihoods are constructed from a coarsely discretized approximation to system equations. A formalism is proposed for inferring a fixed but a priori unknown model trajectory through Bayesian updating of a prior process conditional on model information. A one-step-ahead sampling scheme for interrogating the model is described, its consistency and first order convergence properties are proved, and its computational complexity is shown to be proportional to that of numerical explicit one-step solvers. Examples illustrate the flexibility of this framework to deal with a wide variety of complex and large-scale systems. Within the calibration problem, discretization uncertainty defines a layer in the Bayesian hierarchy, and a Markov chain Monte Carlo algorithm that targets this posterior distribution is presented. This formalism is used for inference on the JAK-STAT delay differential equation model of protein dynamics from indirectly observed measurements. The discussion outlines implications for the new field of probabilistic numerics.},
archivePrefix = {arXiv},
arxivId = {1306.2365},
author = {Chkrebtii, Oksana A. and Campbell, David A. and Calderhead, Ben and Girolami, Mark A.},
doi = {10.1214/16-BAXXX},
eprint = {1306.2365},
file = {:home/nmlemus/Documents/Mendeley Desktop/Chkrebtii et al. - 2016 - Bayesian Solution Uncertainty Quantification for Differential Equations.pdf:pdf},
issn = {1936-0975},
journal = {International Society for Bayesian Analysis},
keywords = {Bayesian numerical analysis,Gaussian processes,bayesian numerical analysis,differential equation models,gaussian,processes,uncertainty in computer models,uncertainty quantification},
pages = {1--29},
title = {{Bayesian Solution Uncertainty Quantification for Differential Equations}},
url = {http://arxiv.org/abs/1306.2365},
year = {2016}
}
@article{Arnaut2008,
author = {Arnaut, L. R.},
file = {:home/nmlemus/Documents/Mendeley Desktop/Arnaut - 2008 - Measurement uncertainty in reverberation chambers - I. Sample statistics.pdf:pdf},
issn = {1754-2995},
journal = {NPL Technical Report TQE 2, 2nd. ed., sec. 4.1.2.2},
number = {2},
title = {{Measurement uncertainty in reverberation chambers - I. Sample statistics}},
volume = {TQE},
year = {2008}
}
@article{Tobergte2013,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Tobergte, David R. and Curtis, Shirley},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/nmlemus/Documents/Mendeley Desktop/Tobergte, Curtis - 2013 - Workshop on Quantification, Communication, and Interpretation of Uncertainty in Simulation and Data Science.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Journal of Chemical Information and Modeling},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{Workshop on Quantification, Communication, and Interpretation of Uncertainty in Simulation and Data Science}},
volume = {53},
year = {2013}
}
@article{Goncalves2013a,
author = {Gon{\c{c}}alves, Bernardo and Porto, Fabio},
file = {:home/nmlemus/Documents/Mendeley Desktop/Gon{\c{c}}alves, Porto - 2013 - A lattice-theoretic approach for representing and managing hypothesis-driven research.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {Data management,Large-scale science,Lattice theory,Scientific hypotheses},
title = {{A lattice-theoretic approach for representing and managing hypothesis-driven research}},
volume = {1087},
year = {2013}
}
@inproceedings{Bernecker2011,
abstract = {Many spatial query problems defined on uncertain data are computationally expensive, in particular, if in addition to spatial attributes, a time component is added. Although there exists a wide range of applications dealing with uncertain spatio-temporal data, there is no solution for efficient management of such data available yet. This paper is the first work to propose general models for spatio-temporal uncertain data that have the potential to allow efficient processing on a wide range of queries. The main challenge here is to unfold this potential by developing new algorithms based on these models. In addition, we give examples of interesting spatio-temporal queries on uncertain data.},
address = {New York, New York, USA},
author = {Bernecker, Thomas and Emrich, Tobias and Kriegel, Hans-Peter and Zuefle, Andreas and Chen, Lei and Lian, Xiang and Mamoulis, Nikos},
booktitle = {Proceedings of the 2nd ACM SIGSPATIAL International Workshop on Querying and Mining Uncertain Spatio-Temporal Data - QUeST '11},
doi = {10.1145/2064969.2064972},
file = {:home/nmlemus/Documents/Mendeley Desktop/Bernecker et al. - 2011 - Managing uncertain spatio-temporal data.pdf:pdf},
isbn = {9781450310376},
number = {c},
pages = {16--20},
publisher = {ACM Press},
title = {{Managing uncertain spatio-temporal data}},
url = {http://dl.acm.org/citation.cfm?doid=2064969.2064972 http://dl.acm.org/citation.cfm?doid=2064969.2064972{\%}5Cnhttp://dl.acm.org/citation.cfm?id=2064972},
year = {2011}
}
@article{Wang2016,
abstract = {Uncertainty quantification (UQ) refers to quantitative characterization and reduction of uncertainties present in computer model simulations. It is widely used in engineering and geophysics fields to assess and predict the likelihood of various outcomes. This paper describes a UQ platform called UQ-PyL (Uncertainty Quantification Python Laboratory), a flexible software platform designed to quantify uncertainty of complex dynamical models. UQ-PyL integrates different kinds of UQ methods, including experimental design, statistical analysis, sensitivity analysis, surrogate modeling and parameter optimization. It is written in Python language and runs on all common operating systems. UQ-PyL has a graphical user interface that allows users to enter commands via pull-down menus. It is equipped with a model driver generator that allows any computer model to be linked with the software. We illustrate the different functions of UQ-PyL by applying it to the uncertainty analysis of the Sacramento Soil Moisture Accounting Model. We will also demonstrate that UQ-PyL can be applied to a wide range of applications.},
author = {Wang, Chen and Duan, Qingyun and Tong, Charles H. and Di, Zhenhua and Gong, Wei},
doi = {10.1016/j.envsoft.2015.11.004},
file = {:home/nmlemus/Documents/Mendeley Desktop/1-s2.0-S1364815215300955-main.pdf:pdf},
isbn = {1364-8152},
issn = {13648152},
journal = {Environmental Modelling and Software},
keywords = {Design of experiments,Parameter optimization,Sensitivity analysis,Surrogate modeling,UQ-PyL,Uncertainty Quantification},
pages = {1--12},
publisher = {Elsevier Ltd},
title = {{A GUI platform for uncertainty quantification of complex dynamical models}},
url = {http://dx.doi.org/10.1016/j.envsoft.2015.11.004},
volume = {76},
year = {2016}
}
@article{Golias2012,
author = {Golia{\v{s}}, Marcel and Palen{\v{c}}{\'{a}}r, Rudolf},
file = {:home/nmlemus/Documents/Mendeley Desktop/Golia{\v{s}}, Palen{\v{c}}{\'{a}}r - 2012 - Determination of uncertainties for correlated input quantities by the Monte Carlo method.pdf:pdf},
issn = {12102709},
journal = {Acta Polytechnica},
keywords = {Correlation,Monte carlo method,Uncertainty of measurement},
number = {4},
pages = {57--61},
title = {{Determination of uncertainties for correlated input quantities by the Monte Carlo method}},
volume = {52},
year = {2012}
}
@article{Kumar2015d,
abstract = {Advanced analytics is a booming area in both industry and academia. Several projects aim to implement ML algorithms efficiently. But three key challenging and iterative practical tasks in using ML – feature engi-neering, algorithm selection, and parameter tuning, collectively called model selection – have largely been overlooked by the data management community even though these are often the most time-consuming tasks for analysts. To make the iterative process of model se-lection easier and faster, we envision a unifying abstract framework that acts a basis for a new class of analytics systems that we call model selection management sys-tems (MSMS). We discuss how time-tested ideas from database research offer new avenues to improve model selection, and outline how MSMS are a new frontier for interesting and impactful data management research.},
author = {Kumar, Arun and Mccann, Robert and Naughton, Jeffrey and Patel, Jignesh M.},
doi = {10.1145/2935694.2935698},
file = {:home/nmlemus/Documents/Mendeley Desktop/p17-kumar.pdf:pdf},
issn = {0163-5808},
journal = {ACM SIGMOD Record},
keywords = {Feature Engineering,Iterative Model Selection,Performance Optimization,Provenance for Machine Learning,Usability of Machine Learning},
month = {may},
number = {4},
pages = {17--22},
publisher = {ACM},
title = {{Model Selection Management Systems: The Next Frontier of Advanced Analytics}},
url = {http://dl.acm.org/citation.cfm?doid=2935694.2935698 http://pages.cs.wisc.edu/{~}arun/vision/ http://dl.acm.org/citation.cfm?doid=2935694.2935698{\%}5Cnhttp://pages.cs.wisc.edu/{~}arun/vision/},
volume = {44},
year = {2015}
}
@article{Iglesias2014,
abstract = {In a Bayesian setting, inverse problems and uncertainty quantification (UQ) - the propagation of uncertainty through a computational (forward) model - are strongly connected. In the form of conditional expectation the Bayesian update becomes computationally attractive. This is especially the case as together with a functional or spectral approach for the forward UQ there is no need for time-consuming and slowly convergent Monte Carlo sampling. The developed sampling-free non-linear Bayesian update is derived from the variational problem associated with conditional expectation. This formulation in general calls for further discretisation to make the computation possible, and we choose a polynomial approximation. After giving details on the actual computation in the framework of functional or spectral approximations, we demonstrate the workings of the algorithm on a number of examples of increasing complexity. At last, we compare the linear and quadratic Bayesian update on the small but taxing example of the chaotic Lorenz 84 model, where we experiment with the influence of different observation or measurement operators on the update.},
archivePrefix = {arXiv},
arxivId = {1312.5048},
author = {Litvinenko, Alexander and Matthies, Hermann G. and Iglesias, Marco A and Stuart, Andrew M},
eprint = {1312.5048},
file = {:home/nmlemus/Documents/Mendeley Desktop/Iglesias, Stuart - 2014 - Inverse problems and uncertainty quantification.pdf:pdf},
journal = {SIAM News},
keywords = {60h15,60h25,62f15,62p30,65n21,74g75,80a23,classification,identification,inverse problem,msc2010,uncertainty quantification},
pages = {2--3},
title = {{Inverse problems and uncertainty quantification}},
url = {http://arxiv.org/abs/1312.5048},
year = {2013}
}
@phdthesis{Lavril2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Melorose, J. and Perroy, R. and Careas, S. and Lavril, Thibaut Jean Philippe},
booktitle = {Statewide Agricultural Land Use Baseline 2015},
doi = {10.1145/2064942.2064944.},
eprint = {arXiv:1011.1669v3},
file = {:home/nmlemus/Documents/Mendeley Desktop/Melorose et al. - 2015 - A PROBABILISTIC FRAMEWORK FOR UNCERTAINTY QUANTIFICATION IN LARGE-SCALE SIMULATIONS APPLICATION IN SEISMIC IMAG.pdf:pdf},
isbn = {9781450311694},
issn = {1098-6596},
keywords = {icle},
pmid = {25246403},
school = {Universidade Federal do Rio de Janeiro},
title = {{A PROBABILISTIC FRAMEWORK FOR UNCERTAINTY QUANTIFICATION IN LARGE-SCALE SIMULATIONS: APPLICATION IN SEISMIC IMAGING}},
volume = {1},
year = {2015}
}
@book{Marino2009,
abstract = {Accuracy of results from mathematical and computer models of biological systems is often complicated by the presence of uncertainties in experimental data that are used to estimate parameter values. Current mathematical modeling approaches typically use either single-parameter or local sensitivity analyses. However, these methods do not accurately assess uncertainty and sensitivity in the system as, by default they hold all other parameters fixed at baseline values. Using techniques described within we demonstrate how a multi-dimensional parameter space can be studied globally so all uncertainties can be identified. Further, uncertainty and sensitivity analysis techniques can help to identify and ultimately control uncertainties. In this work we develop methods for applying existing analytical tools to perform analyses on a variety of mathematical and computer models. We compare two specific types of global sensitivity analysis indexes that have proven to be among the most robust and efficient. Through familiar and new examples of mathematical and computer models, we provide a complete methodology for performing these analyses, both in deterministic and stochastic settings, and propose novel techniques to handle problems encountered during this type of analyses.},
author = {Marino, Simeone and Hogue, Ian B and Ray, Christian J and Kirschner, Denise E},
booktitle = {Journal of Theoretical Biology},
doi = {10.1016/j.jtbi.2008.04.011.A},
file = {:home/nmlemus/Documents/Mendeley Desktop/Marino et al. - 2009 - A Methodology for Performing Global Ucertainty and Sensitivity Analysis in Systems Biology.pdf:pdf},
isbn = {7346477723},
issn = {1095-8541},
keywords = {abm,agent-based model,aleatory uncertainty,amplitude sensitivity test,efast,epistemic uncertainty,extended fourier,latin hypercube sampling,lhs,methods,monte carlo,partial rank correlation coefficient,prcc,sensitivity index},
number = {1},
pages = {178--196},
pmid = {18572196},
title = {{A Methodology for Performing Global Ucertainty and Sensitivity Analysis in Systems Biology}},
volume = {254},
year = {2009}
}
@article{Fordham2016,
abstract = {Spatially explicit demographic models are increasingly being used to forecast the effect of global change on the range dynamics of species. These models are typically complex, with the structure and parameter values often estimated with considerable uncertainty. If not properly accounted, this can lead to bias or false precision in projections of changes to species range dynamics and extinction risk. Here we present a new open-source freeware tool, "Sensitivity Analysis of Range Dynamics Models" (SARDM) that provides an all-in-one approach for: (i) determining the implications of integrating complex and often uncertain information into spatially explicit demographic models compiled in RAMAS GIS, and (ii) identifying and ranking the relative importance of different sources of parameter uncertainty. The sensitivity and uncertainty analysis techniques built into SARDM will facilitate ecologists and conservation scientists in better establishing confidence in forecasts of range movement and abundance.},
author = {Fordham, Damien A. and Haythorne, Sean and Brook, Barry W.},
doi = {10.1016/j.envsoft.2016.05.020},
file = {:home/nmlemus/Documents/Mendeley Desktop/Fordham, Haythorne, Brook - 2016 - Sensitivity Analysis of Range Dynamics Models (SARDM) Quantifying the influence of parameter uncertai.pdf:pdf},
issn = {13648152},
journal = {Environmental Modelling and Software},
keywords = {Climate change,Coupled niche-population model,Metapopulation,Population viability analysis,Propagating uncertainty,Species distribution},
pages = {193--197},
publisher = {Elsevier Ltd},
title = {{Sensitivity Analysis of Range Dynamics Models (SARDM): Quantifying the influence of parameter uncertainty on forecasts of extinction risk from global change}},
volume = {83},
year = {2016}
}
@article{Peckham2015,
abstract = {Component-based modeling frameworks make it easier for users to access, configure, couple, run and test numerical models. However, they do not typically provide tools for uncertainty quantification or data-based model verification and calibration. To better address these important issues, modeling frameworks should be integrated with existing, general-purpose toolkits for optimization, parameter estimation and uncertainty quantification.This paper identifies and then examines the key issues that must be addressed in order to make a component-based modeling framework interoperable with general-purpose packages for model analysis. As a motivating example, one of these packages, DAKOTA, is applied to a representative but nontrivial surface process problem of comparing two models for the longitudinal elevation profile of a river to observational data. Results from a new mathematical analysis of the resulting nonlinear least squares problem are given and then compared to results from several different optimization algorithms in DAKOTA.},
author = {Peckham, Scott D. and Kelbert, Anna and Hill, Mary C. and Hutton, Eric W H},
doi = {10.1016/j.cageo.2016.03.005},
file = {:home/nmlemus/Documents/Mendeley Desktop/Peckham et al. - 2016 - Towards uncertainty quantification and parameter estimation for Earth system models in a component-based modelin.pdf:pdf},
isbn = {00983004},
issn = {00983004},
journal = {Computers and Geosciences},
keywords = {Component-based modeling,Inverse problems,Longitudinal river elevation profiles,Model uncertainty,Modeling frameworks,Nonlinear least squares,Optimization,Parameter estimation},
pages = {152--161},
publisher = {Elsevier},
title = {{Towards uncertainty quantification and parameter estimation for Earth system models in a component-based modeling framework}},
volume = {90},
year = {2016}
}
@article{Baroni2014,
abstract = {The present study proposes a General Probabilistic Framework (GPF) for uncertainty and global sensitivity analysis of deterministic models in which, in addition to scalar inputs, non-scalar and correlated inputs can be considered as well. The analysis is conducted with the variance-based approach of Sobol/Saltelli where first and total sensitivity indices are estimated. The results of the framework can be used in a loop for model improvement, parameter estimation or model simplification. The framework is applied to SWAP, a 1D hydrological model for the transport of water, solutes and heat in unsaturated and saturated soils. The sources of uncertainty are grouped in five main classes: model structure (soil discretization), input (weather data), time-varying (crop) parameters, scalar parameters (soil properties) and observations (measured soil moisture). For each source of uncertainty, different realizations are created based on direct monitoring activities. Uncertainty of evapotranspiration, soil moisture in the root zone and bottom fluxes below the root zone are considered in the analysis. The results show that the sources of uncertainty are different for each output considered and it is necessary to consider multiple output variables for a proper assessment of the model. Improvements on the performance of the model can be achieved reducing the uncertainty in the observations, in the soil parameters and in the weather data. Overall, the study shows the capability of the GPF to quantify the relative contribution of the different sources of uncertainty and to identify the priorities required to improve the performance of the model. The proposed framework can be extended to a wide variety of modelling applications, also when direct measurements of model output are not available. {\textcopyright} 2013 Elsevier Ltd.},
annote = {From Duplicate 2 (A General Probabilistic Framework for uncertainty and global sensitivity analysis of deterministic models: A hydrological case study - Baroni, G.; Tarantola, S.)

From Duplicate 2 (A General Probabilistic Framework for uncertainty and global sensitivity analysis of deterministic models: A hydrological case study - Baroni, G.; Tarantola, S.)

Framework to UA and SA. Reference to UQ and SA.},
author = {Baroni, G. and Tarantola, S.},
doi = {10.1016/j.envsoft.2013.09.022},
file = {:home/nmlemus/Documents/Mendeley Desktop/Baroni, Tarantola - 2014 - A General Probabilistic Framework for uncertainty and global sensitivity analysis of deterministic models A h.pdf:pdf},
isbn = {3319772805},
issn = {13648152},
journal = {Environmental Modelling and Software},
keywords = {Global sensitivity analysis,Hydrological model,Multi-variables,Non-scalar input factors},
pages = {26--34},
publisher = {Elsevier Ltd},
title = {{A General Probabilistic Framework for uncertainty and global sensitivity analysis of deterministic models: A hydrological case study}},
url = {http://dx.doi.org/10.1016/j.envsoft.2013.09.022},
volume = {51},
year = {2014}
}
@article{Pianosi2016,
abstract = {Sensitivity Analysis (SA) investigates how the variation in the output of a numerical model can be attributed to variations of its input factors. SA is increasingly being used in environmental modelling for a variety of purposes, including uncertainty assessment, model calibration and diagnostic evaluation, dominant control analysis and robust decision-making. In this paper we review the SA literature with the goal of providing: (i) a comprehensive view of SA approaches also in relation to other methodologies for model identification and application; (ii) a systematic classification of the most commonly used SA methods; (iii) practical guidelines for the application of SA. The paper aims at delivering an introduction to SA for non-specialist readers, as well as practical advice with best practice examples from the literature; and at stimulating the discussion within the community of SA developers and users regarding the setting of good practices and on defining priorities for future research.},
author = {Pianosi, Francesca and Beven, Keith and Freer, Jim and Hall, Jim W. and Rougier, Jonathan and Stephenson, David B. and Wagener, Thorsten},
doi = {10.1016/j.envsoft.2016.02.008},
file = {:home/nmlemus/Documents/Mendeley Desktop/1-s2.0-S1364815216300287-main.pdf:pdf},
issn = {13648152},
journal = {Environmental Modelling {\&} Software},
keywords = {Calibration,Evaluation,Robust decision-making,Sensitivity Analysis,Spatio-tenporal Sensitivity Analysis,Uncertainty Analysis},
mendeley-tags = {Spatio-tenporal Sensitivity Analysis},
pages = {214--232},
publisher = {Elsevier Ltd},
title = {{Sensitivity analysis of environmental models: A systematic review with practical workflow}},
url = {http://dx.doi.org/10.1016/j.envsoft.2016.02.008 http://www.sciencedirect.com/science/article/pii/S1364815216300287},
volume = {79},
year = {2016}
}
@article{Oberkampf2004,
abstract = {Developers of computer codes, analysts who use the codes, and decision makers who rely on the results of the analyses face a critical question: How should confidence in modeling and simulation be critically assessed? Verification and validation (V{\&}V) of computational simulations are the primary methods for building and quantifying this confidence. Briefly, verification is the assessment of the accuracy of the solution to a computational model. Validation is the assessment of the accuracy of a computational simulation by comparison with experimental data. In verification, the relationship of the simulation to the real world is not an issue. In validation, the relationship between computation and the real world, ie, experimental data, is the issue. This paper presents our viewpoint of the state of the art in V{\&}V in computational physics. (In this paper we refer to all fields of computational engineering and physics, eg, computational fluid dynamics, computational solid mechanics, structural dynamics, shock wave physics, computational chemistry, etc, as computational physics.) We describe our view of the framework in which predictive capability relies on V{\&}V, as well as other factors that affect predictive capability. Our opinions about the research needs and management issues in V{\&}V are very practical: What methods and techniques need to be developed and what changes in the views of management need to occur to increase the usefulness, reliability, and impact of computational physics for decision making about engineering systems? We review the state of the art in V{\&}V over a wide range of topics, for example, prioritization of V{\&}V activities using the Phenomena Identification and Ranking Table (PIRT), code verification, software quality assurance (SQA), numerical error estimation, hierarchical experiments for validation, characteristics of validation experiments, the need to perform nondeterministic computational simulations in comparisons with experimental data, and validation metrics. We then provide an extensive discussion of V{\&}V research and implementation issues that we believe must be addressed for V{\&}V to be more effective in improving confidence in computational predictive capability. Some of the research topics addressed are development of improved procedures for the use of the PIRT for prioritizing V{\&}V activities, the method of manufactured solutions for code verification, development and use of hierarchical validation diagrams, and the construction and use of validation metrics incorporating statistical measures. Some of the implementation topics addressed are the needed management initiatives to better align and team computationalists and experimentalists in conducting validation activities, the perspective of commercial software companies, the key role of analysts and decision makers as code customers, obstacles to the improved effectiveness of V{\&}V, effects of cost and schedule constraints on practical applications in industrial settings, and the role of engineering standards committees in documenting best practices for V{\&}V. There are 207 references cited in this review article.},
author = {Oberkampf, William L. and Trucano, Timothy G. and Hirsch, Charles},
doi = {10.1115/1.1767847},
file = {:home/nmlemus/Documents/Mendeley Desktop/Oberkampf, Trucano, Hirsch - 2004 - Verification, validation, and predictive capability in computational engineering and physics.pdf:pdf},
isbn = {0003-6900},
issn = {00036900},
journal = {Applied Mechanics Reviews},
number = {5},
pages = {345},
title = {{Verification, validation, and predictive capability in computational engineering and physics}},
volume = {57},
year = {2004}
}
@article{Alvin1998,
annote = {From Duplicate 2 (Uncertainty quantification in computational structural dynamics: a new paradigm for model validation - Alvin, K F; Oberkampf, William L; Diegert, K V; Rutherford, B M)

From Duplicate 1 (Uncertainty quantification in computational structural dynamics: a new paradigm for model validation - Alvin, K F; Oberkampf, William L; Diegert, K V; Rutherford, B M)

Uncertainty when we have different models. They use the same approach as Bernardo, Bayesian model to propagate de uncertainty.},
author = {Alvin, K F and Oberkampf, William L and Diegert, K V and Rutherford, B M},
file = {:home/nmlemus/Documents/Mendeley Desktop/Alvin et al. - 1998 - Uncertainty quantification in computational structural dynamics a new paradigm for model validation.pdf:pdf},
journal = {Society for Experimental Mechanics, Inc, 16 th International Modal Analysis Conference.},
keywords = {uncertainty - general},
pages = {1191--1198},
title = {{Uncertainty quantification in computational structural dynamics: a new paradigm for model validation}},
volume = {2},
year = {1998}
}
@inproceedings{Lavril2016,
author = {Lavril, Thibault and Mattoso, Marta and Costa, Danilo and Rochinha, Fernando A and Miras, Thomas},
booktitle = {Proceedings of the XXXVII Iberian Latin-American Congress on Computational Methods in Engineering},
file = {:home/nmlemus/Documents/Mendeley Desktop/Lavril et al. - 2016 - Controlling Parallel Adaptive Sparse Grid Stochastic Collocation Simulations With.pdf:pdf},
title = {{CONTROLLING PARALLEL ADAPTIVE SPARSE GRID STOCHASTIC COLLOCATION SIMULATIONS WITH}},
year = {2016}
}
@article{Lee2009,
abstract = {A wide variety of uncertainty propagation methods exist in literature; however, there is a lack of good understanding of their relative merits. In this paper, a comparative study on the performances of several representative uncertainty propagation methods, including a few newly developed methods that have received growing attention, is performed. The full factorial numerical integration, the univariate dimension reduction method, and the polynomial chaos expansion method are implemented and applied to several test problems. They are tested under different settings of the performance nonlinearity, distribution types of input random variables, and the magnitude of input uncertainty. The performances of those methods are compared in moment estimation, tail probability calculation, and the probability density function construction, corresponding to a wide variety of scenarios of design under uncertainty, such as robust design, and reliability-based design optimization. The insights gained are expected to direct designers for choosing the most applicable uncertainty propagation technique in design under uncertainty.},
author = {Lee, S. H. and Chen, W.},
doi = {10.1007/s00158-008-0234-7},
file = {:home/nmlemus/Documents/Mendeley Desktop/Lee, Chen - 2009 - A comparative study of uncertainty propagation methods for black-box-type problems.pdf:pdf},
isbn = {1615-147X},
issn = {1615147X},
journal = {Structural and Multidisciplinary Optimization},
keywords = {Comparative study,Design under uncertainty,Dimension reduction method,Full factorial numerical integration,Polynomial chaos expansion,Uncertainty propagation},
number = {3},
pages = {239--253},
title = {{A comparative study of uncertainty propagation methods for black-box-type problems}},
volume = {37},
year = {2009}
}
@article{Hoare2008,
abstract = {SaSAT (Sampling and Sensitivity Analysis Tools) is a user-friendly software package for applying uncertainty and sensitivity analyses to mathematical and computational models of arbitrary complexity and context. The toolbox is built in Matlab, a numerical mathematical software package, and utilises algorithms contained in the Matlab Statistics Toolbox. However, Matlab is not required to use SaSAT as the software package is provided as an executable file with all the necessary supplementary files. The SaSAT package is also designed to work seamlessly with Microsoft Excel but no functionality is forfeited if that software is not available. A comprehensive suite of tools is provided to enable the following tasks to be easily performed: efficient and equitable sampling of parameter space by various methodologies; calculation of correlation coefficients; regression analysis; factor prioritisation; and graphical output of results, including response surfaces, tornado plots, and scatterplots. Use of SaSAT is exemplified by application to a simple epidemic model. To our knowledge, a number of the methods available in SaSAT for performing sensitivity analyses have not previously been used in epidemiological modelling and their usefulness in this context is demonstrated.},
author = {Hoare, Alexander and Regan, David G and Wilson, David P},
doi = {10.1186/1742-4682-5-4},
file = {:home/nmlemus/Documents/Mendeley Desktop/Hoare, Regan, Wilson - 2008 - Sampling and sensitivity analyses tools (SaSAT) for computational modelling.pdf:pdf},
isbn = {1742-4682 (Electronic)$\backslash$n1742-4682 (Linking)},
issn = {1742-4682},
journal = {Theoretical biology {\&} medical modelling},
pages = {4},
pmid = {18304361},
title = {{Sampling and sensitivity analyses tools (SaSAT) for computational modelling.}},
volume = {5},
year = {2008}
}
@book{Bettencourt2012a,
author = {Bettencourt, Ricardo and Bulska, Ewa and Godlewska-{\.{z}}y{\l}kiewicz, Beata and Papadakis, Ioannis and Patriarca, Marina and Vassileva, Emilia and Taylor, Philip},
doi = {10.2787/5825},
file = {:home/nmlemus/Documents/Mendeley Desktop/Bettencourt et al. - 2012 - Analytical measurement measurement uncertainty and statistics.pdf:pdf},
isbn = {9789279230714},
pages = {240},
title = {{Analytical measurement : measurement uncertainty and statistics}},
url = {http://www.jrc.ec.europa.eu},
year = {2012}
}
@article{Santonja2012,
abstract = {Mathematical models based on ordinary differential equations are a useful tool to study the processes involved in epidemiology. Many models consider that the parameters are deterministic variables. But in practice, the transmission parameters present large variability and it is not possible to determine them exactly, and it is necessary to introduce randomness. In this paper, we present an application of the polynomial chaos approach to epidemiological mathematical models based on ordinary differential equations with random coefficients. Taking into account the variability of the transmission parameters of the model, this approach allows us to obtain an auxiliary system of differential equations, which is then integrated numerically to obtain the first-and the second-order moments of the output stochastic processes. A sensitivity analysis based on the polynomial chaos approach is also performed to determine which parameters have the greatest influence on the results. As an example, we will apply the approach to an obesity epidemic model.},
author = {Santonja, F. and Chen-Charpentier, B.},
doi = {10.1155/2012/742086},
file = {:home/nmlemus/Documents/Mendeley Desktop/Santonja, Chen-Charpentier - 2012 - Uncertainty quantification in simulations of epidemics using polynomial chaos.pdf:pdf},
issn = {1748670X},
journal = {Computational and Mathematical Methods in Medicine},
pmid = {22927889},
title = {{Uncertainty quantification in simulations of epidemics using polynomial chaos}},
volume = {2012},
year = {2012}
}
@article{Citac2000,
abstract = {This Guide gives detailed guidance for the evaluation and expression of uncertainty in quantitative chemical analysis, based on the approach taken in the ISO Guide to the Expression of Uncertainty in Measurement H.2. It is applicable at all levels of accuracy and in all fields - from routine analysis to basic research and to empirical and rational methods (see section 5.3.). Some common areas in which chemical measurements are needed, and in which the principles of this Guide may be applied, are: Quality control and quality assurance in manufacturing industries. Testing for regulatory compliance. Testing utilising an agreed method. Calibration of standards and equipment. Measurements associated with the development and certification of reference materials. Research and development.},
author = {Citac and Eurachem},
doi = {0 948926 15 5},
file = {:home/nmlemus/Documents/Mendeley Desktop/Citac, Eurachem - 2000 - Quantifying Uncertainty in Analytical Measurement.pdf:pdf},
isbn = {0948926155},
journal = {English},
pages = {126},
title = {{Quantifying Uncertainty in Analytical Measurement}},
url = {http://www.measurementuncertainty.org/mu/QUAM2000-1.pdf},
volume = {2nd},
year = {2000}
}
@article{Johnstone2015,
abstract = {Cardiac electrophysiology models have been developed for over 50 years, and now include detailed descriptions of individual ion currents and sub-cellular calcium handling. It is commonly accepted that there are many uncertainties in these systems, with quantities such as ion channel kinetics or expression levels being difficult to measure or variable between samples. Until recently, the original approach of describing model parameters using single values has been retained, and consequently the majority of mathematical models in use today provide point predictions, with no associated uncertainty. In recent years, statistical techniques have been developed and applied in many scientific areas to capture uncertainties in the quantities that determine model behaviour, and to provide a distribution of predictions which accounts for this uncertainty. In this paper we discuss this concept, which is termed uncertainty quantification, and consider how it might be applied to cardiac electrophysiology models. We present two case studies in which probability distributions, instead of individual numbers, are inferred from data to describe quantities such as maximal current densities. Then we show how these probabilistic representations of model parameters enable probabilities to be placed on predicted behaviours. We demonstrate how changes in these probability distributions across data sets offer insight into which currents cause beat-to-beat variability in canine APs. We conclude with a discussion of the challenges that this approach entails, and how it provides opportunities to improve our understanding of electrophysiology.},
author = {Johnstone, Ross H. and Chang, Eugene T Y and Bardenet, R{\'{e}}mi and de Boer, Teun P. and Gavaghan, David J. and Pathmanathan, Pras and Clayton, Richard H. and Mirams, Gary R.},
doi = {10.1016/j.yjmcc.2015.11.018},
issn = {10958584},
journal = {Journal of Molecular and Cellular Cardiology},
keywords = {Cardiac electrophysiology,Mathematical model,Probability,Uncertainty quantification},
pages = {49--62},
pmid = {26611884},
publisher = {The Authors},
title = {{Uncertainty and variability in models of the cardiac action potential: Can we build trustworthy models?}},
url = {http://dx.doi.org/10.1016/j.yjmcc.2015.11.018},
volume = {96},
year = {2016}
}
@article{Jampani2011,
abstract = {The application of stochastic models and analysis techniques to large datasets is now commonplace. Unfortunately, in practice this usually means extracting data from a database system into an external tool (such as SAS, R, Arena, or Matlab), and then running the analysis there. This extract-and-model paradigm is typically error-prone, slow, does not support fine-grained modeling, and discourages what-if and sensitivity analyses. In this article we describe MCDB, a database system that permits a wide spectrum of stochastic models to be used in conjunction with the data stored in a large database, without ever extracting the data. MCDB facilitates in-database execution of tasks such as risk assessment, prediction, and imputation of missing data, as well as management of errors due to data integration, information extraction, and privacy-preserving data anonymization. MCDB allows a user to define random relations whose contents are determined by stochastic models. The models can then be queried using standard SQL. Monte Carlo techniques are used to analyze the probability distribution of the result of an SQL query over random relations. Novel tuple-bundle processing techniques can effectively control the Monte Carlo overhead, as shown in our experiments.},
author = {Jampani, Ravi and Xu, Fei and Wu, Mingxi and Perez, Luis and Jermaine, Chris and Haas, Peter J},
doi = {10.1145/2000824.2000828},
file = {:home/nmlemus/Documents/Mendeley Desktop/Jampani et al. - 2011 - The monte carlo database system.pdf:pdf},
issn = {03625915},
journal = {ACM Transactions on Database Systems},
keywords = {MCDB,relational database systems,uncertainty},
number = {3},
pages = {1--41},
title = {{The monte carlo database system}},
url = {http://dl.acm.org/citation.cfm?doid=2000824.2000828},
volume = {36},
year = {2011}
}
@article{Helton2010a,
abstract = {Procedures are described for the representation of results in analyses that involve both aleatory uncertainty and epistemic uncertainty, with aleatory uncertainty deriving from an inherent randomness in the behaviour of the system under study and epistemic uncertainty deriving from a lack of knowledge about the appropriate values to use for quantities that are assumed to have fixed but poorly known values in the context of a specific study. Aleatory uncertainty is usually represented with probability and leads to cumulative distribution functions (CDFs) or complementary CDFs (CCDFs) for analysis results of interest. Several mathematical structures are available for the representation of epistemic uncertainty, including interval analysis, possibility theory, evidence theory and probability theory. In the presence of epistemic uncertainty, there is not a single CDF or CCDF for a given analysis result. Rather, there is a family of CDFs and a corresponding family of CCDFs that derive from epistemic uncertainty and have an uncertainty structure that derives from the particular uncertainty structure (e.g. interval analysis, possibility theory, evidence theory or probability theory) used to represent epistemic uncertainty. Graphical formats for the representation of epistemic uncertainty in families of CDFs and CCDFs are investigated and presented for the indicated characterisations of epistemic uncertainty.$\backslash$nProcedures are described for the representation of results in analyses that involve both aleatory uncertainty and epistemic uncertainty, with aleatory uncertainty deriving from an inherent randomness in the behaviour of the system under study and epistemic uncertainty deriving from a lack of knowledge about the appropriate values to use for quantities that are assumed to have fixed but poorly known values in the context of a specific study. Aleatory uncertainty is usually represented with probability and leads to cumulative distribution functions (CDFs) or complementary CDFs (CCDFs) for analysis results of interest. Several mathematical structures are available for the representation of epistemic uncertainty, including interval analysis, possibility theory, evidence theory and probability theory. In the presence of epistemic uncertainty, there is not a single CDF or CCDF for a given analysis result. Rather, there is a family of CDFs and a corresponding family of CCDFs that derive from epistemic uncertainty and have an uncertainty structure that derives from the particular uncertainty structure (e.g. interval analysis, possibility theory, evidence theory or probability theory) used to represent epistemic uncertainty. Graphical formats for the representation of epistemic uncertainty in families of CDFs and CCDFs are investigated and presented for the indicated characterisations of epistemic uncertainty.},
author = {Helton, Jon C. and Johnson, Jay D. and Oberkampf, William L. and Sallaberry, C{\'{e}}dric J.},
doi = {10.1080/03081079.2010.486664},
file = {:home/nmlemus/Documents/Mendeley Desktop/Helton et al. - 2010 - Representation of analysis results involving aleatory and epistemic uncertainty.pdf:pdf},
issn = {0308-1079},
journal = {International Journal of General Systems},
keywords = {aleatory uncertainty,epistemic uncertainty,evidence theory,interval analysis,possibility theory,probability theory},
number = {6},
pages = {605--646},
publisher = {Taylor {\&} Francis Group},
title = {{Representation of analysis results involving aleatory and epistemic uncertainty}},
volume = {39},
year = {2010}
}
@article{Kidane2012,
abstract = {This work is concerned with establishing the feasibility of a data-on-demand (DoD) uncertainty quantification (UQ) protocol based on concentration-of-measure inequalities. Specific aims are to establish the feasibility of the protocol and its basic properties, including the tightness of the predictions afforded by the protocol. The assessment is based on an application to terminal ballistics and a specific system configuration consisting of 6061-T6 aluminum plates struck by spherical S-2 tool steel projectiles at ballistic impact speeds. The systems inputs are the plate thickness and impact velocity and the perforation area is chosen as the sole performance measure of the system. The objective of the UQ analysis is to certify the lethality of the projectile, i.e., that the projectile perforates the plate with high probability over a prespecified range of impact velocities and plate thicknesses. The net outcome of the UQ analysis is an M/U ratio, or confidence factor, of 2.93, indicative of a small probability of no perforation of the plate over its entire operating range. The high-confidence ({\textgreater}99.9{\%}) in the successful operation of the system afforded the analysis and the small number of tests (40) required for the determination of the modeling-error diameter, establishes the feasibility of the DoD UQ protocol as a rigorous yet practical approach for model-based certification of complex systems. ?? 2011 Elsevier Ltd. All rights reserved.},
author = {Kidane, A. and Lashgari, A. and Li, B. and McKerns, M. and Ortiz, M. and Owhadi, H. and Ravichandran, G. and Stalzer, M. and Sullivan, T. J.},
doi = {10.1016/j.jmps.2011.12.001},
file = {:home/nmlemus/Documents/Mendeley Desktop/Kidane et al. - 2012 - Rigorous model-based uncertainty quantification with application to terminal ballistics, part I Systems with cont.pdf:pdf},
isbn = {0022-5096},
issn = {00225096},
journal = {Journal of the Mechanics and Physics of Solids},
keywords = {Certification,Concentration of measure,Terminal ballistics,Uncertainty quantification},
number = {5},
pages = {983--1001},
title = {{Rigorous model-based uncertainty quantification with application to terminal ballistics, part I: Systems with controllable inputs and small scatter}},
volume = {60},
year = {2012}
}
@misc{Smith2013,
author = {Smith, Ralph C.},
file = {:home/nmlemus/Documents/Mendeley Desktop/Smith - 2013 - Uncertainty Quantification Theory, Implementation, and Applications.pdf:pdf;:home/nmlemus/Documents/Mendeley Desktop/Smith - 2013 - Uncertainty Quantification Theory, Implementation, and Applications(2).pdf:pdf},
title = {{Uncertainty Quantification Theory, Implementation, and Applications}},
year = {2013}
}
@techreport{Energy2009,
author = {{U.S. Department of Energy}},
file = {:home/nmlemus/Documents/Mendeley Desktop/U.S. Department of Energy - 2009 - Scientific Grand Challenges in National Security The Role of Computing at the Extreme Scale.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
pages = {255},
title = {{Scientific Grand Challenges in National Security: The Role of Computing at the Extreme Scale}},
year = {2009}
}
@article{Yi2016a,
abstract = {Sensitivity analysis is a primary approach used in mathematical modeling to identify important factors that control the response dynamics in a model. In this paper, we applied the Morris sensitivity analysis method to identify the important factors governing the dynamics in a complex 3-dimensional water quality model. The water quality model was developed using the Environmental fluid dynamics code (EFDC) to simulate the fate and transport of nutrients and algal dynamics in Lake Dianchi, one of the most polluted large lakes in China. The analysis focused on the response of four water quality constituents, including chlorophyll-a, dissolved oxygen, total nitrogen, and total phosphorus, to 47 parameters and 7 external driving forces. We used Morris sensitivity analysis with different sample sizes and factor perturbation ranges to study the sensitivity with regard to different output metrics of the water quality model, and we analyzed the consistency between different sensitivity scenarios. In addition to the analysis with aggregate outputs, a spatiotemporal variability analysis was performed to understand the spatial heterogeneity and temporal distribution of sensitivities. Our results indicated that it is important to consider multiple characteristics in a sensitivity analysis, and we have identified a robust set of sensitive factors in the water quality model that will be useful for systematic model parameter identification and uncertainty analysis.},
author = {Yi, Xuan and Zou, Rui and Guo, Huaicheng},
doi = {10.1016/j.ecolmodel.2016.01.005},
file = {:home/nmlemus/Documents/Mendeley Desktop/1-s2.0-S030438001600017X-main.pdf:pdf},
isbn = {03043800},
issn = {03043800},
journal = {Ecological Modelling},
keywords = {EFDC model,Morris screening,Sensitivity analysis,Spatiotemporal sensitivity indices,Water quality model},
pages = {74--84},
publisher = {Elsevier B.V.},
title = {{Global sensitivity analysis of a three-dimensional nutrients-algae dynamic model for a large shallow lake}},
url = {http://dx.doi.org/10.1016/j.ecolmodel.2016.01.005},
volume = {327},
year = {2016}
}
@article{Guerra2009c,
author = {Guerra, Gabriel and Rochinha, Fernando and Elias, Renato and Coutinho, Alvaro and Braganholo, Vanessa and de Oliveira, Daniel and Ogasawara, Eduardo and Chirigati, Fernando and Mattoso, Marta},
file = {:home/nmlemus/Documents/Mendeley Desktop/Guerra et al. - 2009 - Scientific Workflow Management System Applied to Uncertainty Quantification in Large Eddy Simulation.pdf:pdf},
number = {November},
title = {{Scientific Workflow Management System Applied to Uncertainty Quantification in Large Eddy Simulation}},
year = {2009}
}
@article{Zio2013,
author = {Zio, Enrico and Pedroni, Nicola},
doi = {10.3406/mcm.1987.945},
file = {:home/nmlemus/Documents/Mendeley Desktop/Zio, Pedroni - 2013 - Literature review of methods for representing uncertainty.pdf:pdf},
issn = {2100-3874},
journal = {Fondation pour une Culture de S{\'{e}}curit{\'{e}} Industrielle, Toulouse, France},
number = {1},
pages = {77--88},
title = {{Literature review of methods for representing uncertainty}},
url = {http://www.persee.fr/web/revues/home/prescript/article/mcm{\_}0755-8287{\_}1987{\_}num{\_}5{\_}1{\_}945},
volume = {5},
year = {2013}
}
@book{Sullivan2015,
author = {Sullivan, T. J.},
editor = {Springer},
file = {:home/nmlemus/Documents/Mendeley Desktop/Sullivan - 2015 - Introduction to Uncertainty Quantification.pdf:pdf},
isbn = {9783319233949},
publisher = {Springer},
title = {{Introduction to Uncertainty Quantification}},
url = {http://www.springer.com/series/1214},
year = {2015}
}
@article{NICHOLLS2004,
author = {NICHOLLS, GK},
journal = {Kuopio, Finland: Workshop on Bayesian {\ldots}},
keywords = {and phrases,and ville kolehmainen for,bayesian inference,examples,i thank jari kaipio,inviting me to give,lecture notes,mcmc,these lectures},
title = {{Bayesian Inference and Markov Chain Monte Carlo by Example}},
url = {http://www.math.auckland.ac.nz/{~}nicholls/707/NichollsKuopio04.pdf},
year = {2004}
}
@article{Lee2009a,
abstract = {A wide variety of uncertainty propagation methods exist in literature; however, there is a lack of good understanding of their relative merits. In this paper, a comparative study on the performances of several representative uncertainty propagation methods, including a few newly developed methods that have received growing attention, is performed. The full factorial numerical integration, the univariate dimension reduction method, and the polynomial chaos expansion method are implemented and applied to several test problems. They are tested under different settings of the performance nonlinearity, distribution types of input random variables, and the magnitude of input uncertainty. The performances of those methods are compared in moment estimation, tail probability calculation, and the probability density function construction, corresponding to a wide variety of scenarios of design under uncertainty, such as robust design, and reliability-based design optimization. The insights gained are expected to direct designers for choosing the most applicable uncertainty propagation technique in design under uncertainty.},
author = {Lee, S. H. and Chen, W.},
doi = {10.1007/s00158-008-0234-7},
file = {:home/nmlemus/Documents/Mendeley Desktop/Lee, Chen - 2009 - A comparative study of uncertainty propagation methods for black-box-type problems.pdf:pdf},
isbn = {1615-147X},
issn = {1615147X},
journal = {Structural and Multidisciplinary Optimization},
keywords = {Comparative study,Design under uncertainty,Dimension reduction method,Full factorial numerical integration,Polynomial chaos expansion,Uncertainty propagation},
number = {3},
pages = {239--253},
title = {{A comparative study of uncertainty propagation methods for black-box-type problems}},
volume = {37},
year = {2009}
}
@article{Frank2012,
author = {Frank, Prof M},
number = {d},
pages = {3--4},
title = {{Uncertainty quantification}},
year = {2012}
}
@article{Wood-Schultz2011,
author = {Wood-Schultz, David H. Sharp and Merri M.},
number = {1},
pages = {55--82},
title = {{QMU and Nuclear Weapons Certification What's under the hood?}},
volume = {44},
year = {2011}
}
@book{Bettencourt2012,
author = {Bettencourt, Ricardo and Bulska, Ewa and Godlewska-{\.{z}}y{\l}kiewicz, Beata and Papadakis, Ioannis and Patriarca, Marina and Vassileva, Emilia and Taylor, Philip},
doi = {10.2787/5825},
file = {:home/nmlemus/Documents/Mendeley Desktop/Bettencourt et al. - 2012 - Analytical measurement measurement uncertainty and statistics.pdf:pdf},
isbn = {9789279230714},
pages = {240},
title = {{Analytical measurement : measurement uncertainty and statistics}},
url = {http://www.jrc.ec.europa.eu},
year = {2012}
}
@article{Citac2000a,
abstract = {This Guide gives detailed guidance for the evaluation and expression of uncertainty in quantitative chemical analysis, based on the approach taken in the ISO Guide to the Expression of Uncertainty in Measurement H.2. It is applicable at all levels of accuracy and in all fields - from routine analysis to basic research and to empirical and rational methods (see section 5.3.). Some common areas in which chemical measurements are needed, and in which the principles of this Guide may be applied, are: Quality control and quality assurance in manufacturing industries. Testing for regulatory compliance. Testing utilising an agreed method. Calibration of standards and equipment. Measurements associated with the development and certification of reference materials. Research and development.},
author = {Citac and Eurachem},
doi = {0 948926 15 5},
isbn = {0948926155},
journal = {English},
pages = {126},
title = {{Quantifying Uncertainty in Analytical Measurement}},
url = {http://www.measurementuncertainty.org/mu/QUAM2000-1.pdf},
volume = {2nd},
year = {2000}
}
@inproceedings{Lavril2016,
author = {Lavril, Thibault and Mattoso, Marta and Costa, Danilo and Rochinha, Fernando A and Miras, Thomas},
booktitle = {Proceedings of the XXXVII Iberian Latin-American Congress on Computational Methods in Engineering},
file = {:home/nmlemus/Documents/Mendeley Desktop/Lavril et al. - 2016 - Controlling Parallel Adaptive Sparse Grid Stochastic Collocation Simulations With.pdf:pdf},
title = {{CONTROLLING PARALLEL ADAPTIVE SPARSE GRID STOCHASTIC COLLOCATION SIMULATIONS WITH}},
year = {2016}
}
@article{Santonja2012,
abstract = {Mathematical models based on ordinary differential equations are a useful tool to study the processes involved in epidemiology. Many models consider that the parameters are deterministic variables. But in practice, the transmission parameters present large variability and it is not possible to determine them exactly, and it is necessary to introduce randomness. In this paper, we present an application of the polynomial chaos approach to epidemiological mathematical models based on ordinary differential equations with random coefficients. Taking into account the variability of the transmission parameters of the model, this approach allows us to obtain an auxiliary system of differential equations, which is then integrated numerically to obtain the first-and the second-order moments of the output stochastic processes. A sensitivity analysis based on the polynomial chaos approach is also performed to determine which parameters have the greatest influence on the results. As an example, we will apply the approach to an obesity epidemic model.},
author = {Santonja, F. and Chen-Charpentier, B.},
doi = {10.1155/2012/742086},
file = {:home/nmlemus/Documents/Mendeley Desktop/Santonja, Chen-Charpentier - 2012 - Uncertainty quantification in simulations of epidemics using polynomial chaos.pdf:pdf},
issn = {1748670X},
journal = {Computational and Mathematical Methods in Medicine},
pmid = {22927889},
title = {{Uncertainty quantification in simulations of epidemics using polynomial chaos}},
volume = {2012},
year = {2012}
}
@article{Oden2010,
abstract = {Having developed and calibrated a model to be used for a particular prediction (see Part I of this article, SIAM News, November 2010, page 1), we are ready to begin the validation process—that is, to assess the suitability of the calibrated model for the prediction.},
author = {Oden, J. Tinsley and Moser, Robert and Ghattas, Omar},
journal = {SIAM News},
number = {10},
pages = {2008--2011},
title = {{Computer predictions with quantified uncertainty, Part II}},
volume = {43},
year = {2010}
}
@article{Alvin1998,
annote = {From Duplicate 1 (Uncertainty quantification in computational structural dynamics: a new paradigm for model validation - Alvin, K F; Oberkampf, William L; Diegert, K V; Rutherford, B M)

Uncertainty when we have different models. They use the same approach as Bernardo, Bayesian model to propagate de unsertainty.},
author = {Alvin, K F and Oberkampf, William L and Diegert, K V and Rutherford, B M},
file = {:home/nmlemus/Documents/Mendeley Desktop/Alvin et al. - 1998 - Uncertainty quantification in computational structural dynamics a new paradigm for model validation.pdf:pdf},
journal = {Society for Experimental Mechanics, Inc, 16 th International Modal Analysis Conference.},
keywords = {uncertainty - general},
pages = {1191--1198},
title = {{Uncertainty quantification in computational structural dynamics: a new paradigm for model validation}},
volume = {2},
year = {1998}
}
@book{DeCursi2015a,
abstract = {Formerly CIP.},
annote = {From Duplicate 2 (Uncertainty Quantification and Stochastic Modeling with Matlab - de Cursi, Eduardo Souza; Sampaio, Rubens)

From Duplicate 2 (Uncertainty Quantification and Stochastic Modeling with Matlab - de Cursi, Eduardo Souza; Sampaio, Rubens)

Muy dificil porque usa Analisis Funcional.},
author = {de Cursi, Eduardo Souza and Sampaio, Rubens},
doi = {10.1016/B978-1-78548-005-8.50009-8},
file = {:home/nmlemus/Documents/Mendeley Desktop/de Cursi, Sampaio - 2015 - Uncertainty Quantification and Stochastic Modeling with Matlab.pdf:pdf},
isbn = {9780081004715},
pages = {442},
title = {{Uncertainty Quantification and Stochastic Modeling with Matlab}},
url = {http://www.amazon.com/Uncertainty-Quantification-Stochastic-Modeling-Matlab/dp/1785480057},
year = {2015}
}
@article{Wang2016,
abstract = {Uncertainty quantification (UQ) refers to quantitative characterization and reduction of uncertainties present in computer model simulations. It is widely used in engineering and geophysics fields to assess and predict the likelihood of various outcomes. This paper describes a UQ platform called UQ-PyL (Uncertainty Quantification Python Laboratory), a flexible software platform designed to quantify uncertainty of complex dynamical models. UQ-PyL integrates different kinds of UQ methods, including experimental design, statistical analysis, sensitivity analysis, surrogate modeling and parameter optimization. It is written in Python language and runs on all common operating systems. UQ-PyL has a graphical user interface that allows users to enter commands via pull-down menus. It is equipped with a model driver generator that allows any computer model to be linked with the software. We illustrate the different functions of UQ-PyL by applying it to the uncertainty analysis of the Sacramento Soil Moisture Accounting Model. We will also demonstrate that UQ-PyL can be applied to a wide range of applications.},
author = {Wang, Chen and Duan, Qingyun and Tong, Charles H. and Di, Zhenhua and Gong, Wei},
doi = {10.1016/j.envsoft.2015.11.004},
isbn = {1364-8152},
issn = {13648152},
journal = {Environmental Modelling and Software},
keywords = {Design of experiments,Parameter optimization,Sensitivity analysis,Surrogate modeling,UQ-PyL,Uncertainty Quantification},
pages = {1--12},
publisher = {Elsevier Ltd},
title = {{A GUI platform for uncertainty quantification of complex dynamical models}},
url = {http://dx.doi.org/10.1016/j.envsoft.2015.11.004},
volume = {76},
year = {2016}
}
@book{Marino2009,
abstract = {Accuracy of results from mathematical and computer models of biological systems is often complicated by the presence of uncertainties in experimental data that are used to estimate parameter values. Current mathematical modeling approaches typically use either single-parameter or local sensitivity analyses. However, these methods do not accurately assess uncertainty and sensitivity in the system as, by default they hold all other parameters fixed at baseline values. Using techniques described within we demonstrate how a multi-dimensional parameter space can be studied globally so all uncertainties can be identified. Further, uncertainty and sensitivity analysis techniques can help to identify and ultimately control uncertainties. In this work we develop methods for applying existing analytical tools to perform analyses on a variety of mathematical and computer models. We compare two specific types of global sensitivity analysis indexes that have proven to be among the most robust and efficient. Through familiar and new examples of mathematical and computer models, we provide a complete methodology for performing these analyses, both in deterministic and stochastic settings, and propose novel techniques to handle problems encountered during this type of analyses.},
author = {Marino, Simeone and Hogue, Ian B and Ray, Christian J and Kirschner, Denise E},
booktitle = {Journal of Theoretical Biology},
doi = {10.1016/j.jtbi.2008.04.011.A},
file = {:home/nmlemus/Documents/Mendeley Desktop/Marino et al. - 2009 - A Methodology for Performing Global Ucertainty and Sensitivity Analysis in Systems Biology.pdf:pdf},
isbn = {7346477723},
issn = {1095-8541},
keywords = {abm,agent-based model,aleatory uncertainty,amplitude sensitivity test,efast,epistemic uncertainty,extended fourier,latin hypercube sampling,lhs,methods,monte carlo,partial rank correlation coefficient,prcc,sensitivity index},
number = {1},
pages = {178--196},
pmid = {18572196},
title = {{A Methodology for Performing Global Ucertainty and Sensitivity Analysis in Systems Biology}},
volume = {254},
year = {2009}
}
@article{Kidane2012a,
abstract = {This work is concerned with establishing the feasibility of a data-on-demand (DoD) uncertainty quantification (UQ) protocol based on concentration-of-measure inequalities. Specific aims are to establish the feasibility of the protocol and its basic properties, including the tightness of the predictions afforded by the protocol. The assessment is based on an application to terminal ballistics and a specific system configuration consisting of 6061-T6 aluminum plates struck by spherical S-2 tool steel projectiles at ballistic impact speeds. The systems inputs are the plate thickness and impact velocity and the perforation area is chosen as the sole performance measure of the system. The objective of the UQ analysis is to certify the lethality of the projectile, i.e., that the projectile perforates the plate with high probability over a prespecified range of impact velocities and plate thicknesses. The net outcome of the UQ analysis is an M/U ratio, or confidence factor, of 2.93, indicative of a small probability of no perforation of the plate over its entire operating range. The high-confidence ({\textgreater}99.9{\%}) in the successful operation of the system afforded the analysis and the small number of tests (40) required for the determination of the modeling-error diameter, establishes the feasibility of the DoD UQ protocol as a rigorous yet practical approach for model-based certification of complex systems. ?? 2011 Elsevier Ltd. All rights reserved.},
author = {Kidane, A. and Lashgari, A. and Li, B. and McKerns, M. and Ortiz, M. and Owhadi, H. and Ravichandran, G. and Stalzer, M. and Sullivan, T. J.},
doi = {10.1016/j.jmps.2011.12.001},
isbn = {0022-5096},
issn = {00225096},
journal = {Journal of the Mechanics and Physics of Solids},
keywords = {Certification,Concentration of measure,Terminal ballistics,Uncertainty quantification},
number = {5},
pages = {983--1001},
title = {{Rigorous model-based uncertainty quantification with application to terminal ballistics, part I: Systems with controllable inputs and small scatter}},
volume = {60},
year = {2012}
}
@article{Oberkampf2004a,
abstract = {Developers of computer codes, analysts who use the codes, and decision makers who rely on the results of the analyses face a critical question: How should confidence in modeling and simulation be critically assessed? Verification and validation (V{\&}V) of computational simulations are the primary methods for building and quantifying this confidence. Briefly, verification is the assessment of the accuracy of the solution to a computational model. Validation is the assessment of the accuracy of a computational simulation by comparison with experimental data. In verification, the relationship of the simulation to the real world is not an issue. In validation, the relationship between computation and the real world, ie, experimental data, is the issue. This paper presents our viewpoint of the state of the art in V{\&}V in computational physics. (In this paper we refer to all fields of computational engineering and physics, eg, computational fluid dynamics, computational solid mechanics, structural dynamics, shock wave physics, computational chemistry, etc, as computational physics.) We describe our view of the framework in which predictive capability relies on V{\&}V, as well as other factors that affect predictive capability. Our opinions about the research needs and management issues in V{\&}V are very practical: What methods and techniques need to be developed and what changes in the views of management need to occur to increase the usefulness, reliability, and impact of computational physics for decision making about engineering systems? We review the state of the art in V{\&}V over a wide range of topics, for example, prioritization of V{\&}V activities using the Phenomena Identification and Ranking Table (PIRT), code verification, software quality assurance (SQA), numerical error estimation, hierarchical experiments for validation, characteristics of validation experiments, the need to perform nondeterministic computational simulations in comparisons with experimental data, and validation metrics. We then provide an extensive discussion of V{\&}V research and implementation issues that we believe must be addressed for V{\&}V to be more effective in improving confidence in computational predictive capability. Some of the research topics addressed are development of improved procedures for the use of the PIRT for prioritizing V{\&}V activities, the method of manufactured solutions for code verification, development and use of hierarchical validation diagrams, and the construction and use of validation metrics incorporating statistical measures. Some of the implementation topics addressed are the needed management initiatives to better align and team computationalists and experimentalists in conducting validation activities, the perspective of commercial software companies, the key role of analysts and decision makers as code customers, obstacles to the improved effectiveness of V{\&}V, effects of cost and schedule constraints on practical applications in industrial settings, and the role of engineering standards committees in documenting best practices for V{\&}V. There are 207 references cited in this review article.},
author = {Oberkampf, William L. and Trucano, Timothy G. and Hirsch, Charles},
doi = {10.1115/1.1767847},
isbn = {0003-6900},
issn = {00036900},
journal = {Applied Mechanics Reviews},
number = {5},
pages = {345},
title = {{Verification, validation, and predictive capability in computational engineering and physics}},
volume = {57},
year = {2004}
}
@article{Guerra2009,
author = {Guerra, Gabriel and Rochinha, Fernando and Elias, Renato and Coutinho, Alvaro and Braganholo, Vanessa and de Oliveira, Daniel and Ogasawara, Eduardo and Chirigati, Fernando and Mattoso, Marta},
file = {:home/nmlemus/Documents/Mendeley Desktop/Guerra et al. - 2009 - Scientific Workflow Management System Applied to Uncertainty Quantification in Large Eddy Simulation.pdf:pdf},
number = {November},
title = {{Scientific Workflow Management System Applied to Uncertainty Quantification in Large Eddy Simulation}},
year = {2009}
}
@article{Alden2013,
abstract = {Integrating computer simulation with conventional wet-lab research has proven to have much potential in furthering the understanding of biological systems. Success requires the relationship between simulation and the real-world system to be established: substantial aspects of the biological system are typically unknown, and the abstract nature of simulation can complicate interpretation of in silico results in terms of the biology. Here we present spartan (Simulation Parameter Analysis R Toolkit ApplicatioN), a package of statistical techniques specifically designed to help researchers understand this relationship and provide novel biological insight. The tools comprising spartan help identify which simulation results can be attributed to the dynamics of the modelled biological system, rather than artefacts of biological uncertainty or parametrisation, or simulation stochasticity. Statistical analyses reveal the influence that pathways and components have on simulation behaviour, offering valuable biological insight into aspects of the system under study. We demonstrate the power of spartan in providing critical insight into aspects of lymphoid tissue development in the small intestine through simulation. Spartan is released under a GPLv2 license, implemented within the open source R statistical environment, and freely available from both the Comprehensive R Archive Network (CRAN) and http://www.cs.york.ac.uk/spartan. The techniques within the package can be applied to traditional ordinary or partial differential equation simulations as well as agent-based implementations. Manuals, comprehensive tutorials, and example simulation data upon which spartan can be applied are available from the website.},
author = {Alden, Kieran and Read, Mark and Timmis, Jon and Andrews, Paul S. and Veiga-Fernandes, Henrique and Coles, Mark},
doi = {10.1371/journal.pcbi.1002916},
file = {:home/nmlemus/Documents/Mendeley Desktop/Alden et al. - 2013 - Spartan A Comprehensive Tool for Understanding Uncertainty in Simulations of Biological Systems.pdf:pdf},
isbn = {1553-7358},
issn = {1553734X},
journal = {PLoS Computational Biology},
number = {2},
pmid = {23468606},
title = {{Spartan: A Comprehensive Tool for Understanding Uncertainty in Simulations of Biological Systems}},
volume = {9},
year = {2013}
}
@article{Alvin1998a,
author = {Alvin, K F and Oberkampf, William L and Diegert, K V and Rutherford, B M},
journal = {Society for Experimental Mechanics, Inc, 16 th International Modal Analysis Conference.},
keywords = {uncertainty - general},
pages = {1191--1198},
title = {{Uncertainty quantification in computational structural dynamics: a new paradigm for model validation}},
volume = {2},
year = {1998}
}
@article{Hoare2008a,
abstract = {SaSAT (Sampling and Sensitivity Analysis Tools) is a user-friendly software package for applying uncertainty and sensitivity analyses to mathematical and computational models of arbitrary complexity and context. The toolbox is built in Matlab, a numerical mathematical software package, and utilises algorithms contained in the Matlab Statistics Toolbox. However, Matlab is not required to use SaSAT as the software package is provided as an executable file with all the necessary supplementary files. The SaSAT package is also designed to work seamlessly with Microsoft Excel but no functionality is forfeited if that software is not available. A comprehensive suite of tools is provided to enable the following tasks to be easily performed: efficient and equitable sampling of parameter space by various methodologies; calculation of correlation coefficients; regression analysis; factor prioritisation; and graphical output of results, including response surfaces, tornado plots, and scatterplots. Use of SaSAT is exemplified by application to a simple epidemic model. To our knowledge, a number of the methods available in SaSAT for performing sensitivity analyses have not previously been used in epidemiological modelling and their usefulness in this context is demonstrated.},
author = {Hoare, Alexander and Regan, David G and Wilson, David P},
doi = {10.1186/1742-4682-5-4},
isbn = {1742-4682 (Electronic)$\backslash$n1742-4682 (Linking)},
issn = {1742-4682},
journal = {Theoretical biology {\&} medical modelling},
pages = {4},
pmid = {18304361},
title = {{Sampling and sensitivity analyses tools (SaSAT) for computational modelling.}},
volume = {5},
year = {2008}
}
@article{Goncalves2013a,
author = {Gon{\c{c}}alves, Bernardo and Porto, Fabio},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {Data management,Large-scale science,Lattice theory,Scientific hypotheses},
title = {{A lattice-theoretic approach for representing and managing hypothesis-driven research}},
volume = {1087},
year = {2013}
}
@article{Fordham2016a,
abstract = {Spatially explicit demographic models are increasingly being used to forecast the effect of global change on the range dynamics of species. These models are typically complex, with the structure and parameter values often estimated with considerable uncertainty. If not properly accounted, this can lead to bias or false precision in projections of changes to species range dynamics and extinction risk. Here we present a new open-source freeware tool, "Sensitivity Analysis of Range Dynamics Models" (SARDM) that provides an all-in-one approach for: (i) determining the implications of integrating complex and often uncertain information into spatially explicit demographic models compiled in RAMAS GIS, and (ii) identifying and ranking the relative importance of different sources of parameter uncertainty. The sensitivity and uncertainty analysis techniques built into SARDM will facilitate ecologists and conservation scientists in better establishing confidence in forecasts of range movement and abundance.},
author = {Fordham, Damien A. and Haythorne, Sean and Brook, Barry W.},
doi = {10.1016/j.envsoft.2016.05.020},
issn = {13648152},
journal = {Environmental Modelling and Software},
keywords = {Climate change,Coupled niche-population model,Metapopulation,Population viability analysis,Propagating uncertainty,Species distribution},
pages = {193--197},
publisher = {Elsevier Ltd},
title = {{Sensitivity Analysis of Range Dynamics Models (SARDM): Quantifying the influence of parameter uncertainty on forecasts of extinction risk from global change}},
volume = {83},
year = {2016}
}
@article{Tobergte2013a,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Tobergte, David R. and Curtis, Shirley},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Journal of Chemical Information and Modeling},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{Workshop on Quantification, Communication, and Interpretation of Uncertainty in Simulation and Data Science}},
volume = {53},
year = {2013}
}
@article{Thakur2010,
abstract = {The life of turbine blades is central to the integrity of an aircraft engine. Turbine blades, when manufactured, inevitably exhibit some deviations in shape from the desired design specifications due to the influence of manufacturing variability. This manufacturing variability may in turn lead to variations in the expected life and performance of these blades. It becomes important therefore to understand and model the effect of manufacturing variability on turbine blade life. The present work proposes a methodology which employs an existing geometry manipulation technique, namely Free Form Deformation (FFD), to generate 3-d models of the probable manufactured blade shapes. FFD is employed in conjunction with optimization for morphing the base geometry to generate different probable manufactured blade shapes in a case where a limited number of measurements are available per blade to characterize these differences. Lifing estimations on these perturbed geometries show that the presence of variability due to manufacturing processes may result in a reduction of around 1.6{\%} in mean life relative to the designed life, and, a maximum relative reduction of around 3.6{\%}, for turbine blades manufactured over a span of one year.},
author = {Thakur, Nikita and Keane, A.J. and Nair, P.B.},
doi = {10.3850/978-981-08-5118-7},
isbn = {9789810851187},
journal = {4th International Workshop on Reliable Engineering Computing (REC 2010)},
keywords = {fuzzy analysis,fuzzy finite elements,interval analysis,interval fields},
number = {Rec},
pages = {978--981},
title = {{Estimating the effect of Manufacturing Variability on Turbine Blade Life}},
url = {http://eprints.soton.ac.uk/141630/},
year = {2010}
}
@misc{deLaPuente2015,
author = {{Josep de la Puente}, Alvaro Coutinho},
pages = {1--9},
title = {{Website deploying a suite of geophysical tests for wave propagation problems on extreme scale machines}},
year = {2015}
}
@article{Sawicka2016,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {{K. Sawicka}, G.B.M. Heuvelink and Soil},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/nmlemus/Documents/Mendeley Desktop/K. Sawicka, Soil - 2016 - spup- an R package for uncertainty propagation in spatial environmental modelling.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {International symposium on "Spatial Accuracy Assessment in Natural Resources and Environmental Sciences"},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{spup- an R package for uncertainty propagation in spatial environmental modelling}},
url = {http://spatial-accuracy.org/Accuracy2016},
volume = {53},
year = {2016}
}
@book{Lorenz2011,
abstract = {This book focuses on computational methods for large-scale statistical inverse problems and provides an introduction to statistical Bayesian and frequentist methodologies. Recent research advances for approximation methods are discussed, along with Kalman filtering methods and optimization-based approaches to solving inverse problems. The aim is to cross-fertilize the perspectives of researchers in the areas of data assimilation, statistics, large-scale optimization, applied and computational mathematics, high performance computing, and cutting-edge applications. The solution to large-scale inverse problems critically depends on methods to reduce computational cost. Recent research approaches tackle this challenge in a variety of different ways. Many of the computational frameworks highlighted in this book build upon state-of-the-art methods for simulation of the forward problem, such as, fast Partial Differential Equation (PDE) solvers, reduced-order models and emulators of the forward problem, stochastic spectral approximations, and ensemble-based approximations, as well as exploiting the machinery for large-scale deterministic optimization through adjoint and other sensitivity analysis methods. Key Features: • Brings together the perspectives of researchers in areas of inverse problems and data assimilation. • Assesses the current state-of-the-art and identify needs and opportunities for future research. • Focuses on the computational methods used to analyze and simulate inverse problems. • Written by leading experts of inverse problems and uncertainty quantification. Graduate students and researchers working in statistics, mathematics and engineering will benefit from this book.},
author = {{Lorenz Biegler, George Biros}, Omar Ghattas},
doi = {10.1002/9780470685853},
file = {:home/nmlemus/Documents/Mendeley Desktop/Lorenz Biegler, George Biros - 2011 - Large-Scale Inverse Problems and Quantification of Uncertainty.pdf:pdf},
isbn = {1119957583},
pages = {388},
title = {{Large-Scale Inverse Problems and Quantification of Uncertainty}},
url = {https://books.google.com/books?hl=en{\&}lr={\&}id=YPPKipz9qccC{\&}pgis=1},
year = {2011}
}
@article{Iglesias2014,
abstract = {In a Bayesian setting, inverse problems and uncertainty quantification (UQ) - the propagation of uncertainty through a computational (forward) model - are strongly connected. In the form of conditional expectation the Bayesian update becomes computationally attractive. This is especially the case as together with a functional or spectral approach for the forward UQ there is no need for time-consuming and slowly convergent Monte Carlo sampling. The developed sampling-free non-linear Bayesian update is derived from the variational problem associated with conditional expectation. This formulation in general calls for further discretisation to make the computation possible, and we choose a polynomial approximation. After giving details on the actual computation in the framework of functional or spectral approximations, we demonstrate the workings of the algorithm on a number of examples of increasing complexity. At last, we compare the linear and quadratic Bayesian update on the small but taxing example of the chaotic Lorenz 84 model, where we experiment with the influence of different observation or measurement operators on the update.},
archivePrefix = {arXiv},
arxivId = {1312.5048},
author = {Iglesias, Marco A and Stuart, Andrew M},
eprint = {1312.5048},
file = {:home/nmlemus/Documents/Mendeley Desktop/Iglesias, Stuart - 2014 - Inverse problems and uncertainty quantification.pdf:pdf},
journal = {SIAM News},
keywords = {60h15,60h25,62f15,62p30,65n21,74g75,80a23,classification,identification,inverse problem,msc2010,uncertainty quantification},
pages = {2--3},
title = {{Inverse problems and uncertainty quantification}},
url = {http://arxiv.org/abs/1312.5048},
year = {2014}
}
@book{VVUQ2012,
author = {Council, National Research},
doi = {10.17226/13395},
file = {:home/nmlemus/Documents/Mendeley Desktop/Council - 2012 - Assessing the Reliability of Complex Models Mathematical and Statistical Foundations of Verification, Validation, and U.pdf:pdf},
isbn = {978-0-309-25634-6},
number = {February},
publisher = {National Academies Press},
title = {{Assessing the Reliability of Complex Models: Mathematical and Statistical Foundations of Verification, Validation, and Uncertainty Quantification}},
year = {2012}
}
@article{Guerra2009a,
author = {Guerra, Gabriel and Rochinha, Fernando and Elias, Renato and Coutinho, Alvaro and Braganholo, Vanessa and de Oliveira, Daniel and Ogasawara, Eduardo and Chirigati, Fernando and Mattoso, Marta},
file = {:home/nmlemus/Documents/Mendeley Desktop/Guerra et al. - 2009 - Scientific Workflow Management System Applied to Uncertainty Quantification in Large Eddy Simulation.pdf:pdf},
number = {November},
title = {{Scientific Workflow Management System Applied to Uncertainty Quantification in Large Eddy Simulation}},
year = {2009}
}
@book{Banks2014a,
author = {Banks, HT and Hu, S and Thompson, WC},
booktitle = {CRC Press},
file = {:home/nmlemus/Documents/Mendeley Desktop/H.{\_}T.{\_}Banks,{\_}Shuhua{\_}Hu,{\_}W.{\_}Clayton{\_}Thompson{\_}Modeling{\_}and{\_}Inverse{\_}Problems{\_}in{\_}the{\_}Presence{\_}of{\_}Uncertainty{\_}{\_}2014.pdf:pdf},
isbn = {9781482206432},
title = {{Modeling and Inverse Problems in the Presence of Uncertainty}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=jZA-AwAAQBAJ{\&}oi=fnd{\&}pg=PP1{\&}dq=Modeling+and+Inverse+Problems+in+the+Presence+of+Uncertainty{\&}ots=HsI{\_}aNVlHR{\&}sig=4pbvydoz4JeqqEXXEYEqfl7uTvY},
year = {2014}
}
@article{Singh2007,
abstract = {The inherent uncertainty of data present in numerous applications such as sensor databases, text annotations, and information retrieval motivate the need to handle imprecise data at the database level. Uncertainty can be at the attribute or tuple level and is present in both continuous and discrete data domains. This paper presents a model for handling arbitrary probabilistic uncertain data (both discrete and continuous) natively at the database level. Our approach leads to a natural and efficient representation for probabilistic data. We develop a model that is consistent with possible worlds semantics and closed under basic relational operators. This is the first model that accurately and efficiently handles both continuous and discrete uncertainty. The model is implemented in a real database system (PostgreSQL) and the effectiveness and efficiency of our approach is validated experimentally.},
author = {Singh, Sarvjeet and Mayfield, Chris and Shah, Rahul and Prabhakar, Sunil and Hambrusch, Susanne and Neville, Jennifer and Cheng, Reynold},
doi = {10.1109/ICDE.2008.4497514},
isbn = {9781424418374},
issn = {10844627},
journal = {Proceedings - International Conference on Data Engineering},
pages = {1053--1061},
title = {{Database support for probabilistic attributes and tuples}},
year = {2008}
}
@article{Ailamaki2010,
abstract = {Needed are generic, rather than one-off, DBMS solutions automating storage and analysis of data from scientific collaborations.},
author = {Ailamaki, Anastasia and Kantere, Verena and Dash, Debabrata},
doi = {10.1145/1743546.1743568},
file = {:home/nmlemus/Documents/Mendeley Desktop/Ailamaki, Kantere, Dash - 2010 - Managing scientific data.pdf:pdf},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {archiving},
number = {6},
pages = {68},
title = {{Managing scientific data}},
volume = {53},
year = {2010}
}
@misc{Lataniotis2015,
author = {Lataniotis, C and Marelli, S and Sudret, B},
keywords = {Computational Model,Model,Plugin,UQLab,Uncertainty Quantification,Wrapper},
title = {{UQLAB USER MANUAL THE MODEL MODULE}},
year = {2015}
}
@article{Stratos2011,
abstract = {Database management systems (DBMS) provide incredible flexibility and performance when it comes to query processing, scalability and accuracy. To fully exploit DBMS features, however, the user must define a schema, load the data, tune the system for the expected workload, and answer several questions. Should the database use a column-store, a row-store or some hybrid format? What indices should be created? All these questions make for a formidable and time-consuming hurdle, often deterring new applications or imposing high cost to existing ones. A characteristic example is that of scientific databases with huge data sets. The prohibitive initialization cost and complexity still forces scientists to rely on "ancient" tools for their data management tasks, delaying scientific understanding and progress. Users and applications collect their data in flat files, which have traditionally been considered to be "outside" a DBMS. A DBMS wants control: always bring all data "inside", replicate it and format it in its own "secret" way. The problem has been recognized and current efforts extend existing systems with abilities such as reading information from flat files and gracefully incorporating it into the processing engine. This paper proposes a new generation of systems where the only requirement from the user is a link to the raw data files. Queries can then immediately be fired without preparation steps in between. Internally and in an abstract way, the system takes care of selectively, adaptively and incrementally providing the proper environment given the queries at hand. Only part of the data is loaded at any given time and it is being stored and accessed in the format suitable for the current workload.},
author = {{Idreos Ioannis Alagiannis Ryan Johnson Anastasia Ailamaki CWI}, Stratos and Idreos, Stratos and Alagiannis, Ioannis and Johnson, Ryan and Ailamaki, Anastasia},
file = {:home/nmlemus/Documents/Mendeley Desktop/Idreos Ioannis Alagiannis Ryan Johnson Anastasia Ailamaki CWI et al. - 2011 - Here are my Data Files. Here are my Queries.Where are my R.pdf:pdf},
journal = {CIDR '11: Fifth Biennial Conference on Innovative Data Systems Research},
pages = {57--68},
title = {{Here are my Data Files. Here are my Queries.Where are my Results?}},
year = {2011}
}
@article{Helton2010,
abstract = {Procedures are described for the representation of results in analyses that involve both aleatory uncertainty and epistemic uncertainty, with aleatory uncertainty deriving from an inherent randomness in the behaviour of the system under study and epistemic uncertainty deriving from a lack of knowledge about the appropriate values to use for quantities that are assumed to have fixed but poorly known values in the context of a specific study. Aleatory uncertainty is usually represented with probability and leads to cumulative distribution functions (CDFs) or complementary CDFs (CCDFs) for analysis results of interest. Several mathematical structures are available for the representation of epistemic uncertainty, including interval analysis, possibility theory, evidence theory and probability theory. In the presence of epistemic uncertainty, there is not a single CDF or CCDF for a given analysis result. Rather, there is a family of CDFs and a corresponding family of CCDFs that derive from epistemic uncertainty and have an uncertainty structure that derives from the particular uncertainty structure (e.g. interval analysis, possibility theory, evidence theory or probability theory) used to represent epistemic uncertainty. Graphical formats for the representation of epistemic uncertainty in families of CDFs and CCDFs are investigated and presented for the indicated characterisations of epistemic uncertainty.$\backslash$nProcedures are described for the representation of results in analyses that involve both aleatory uncertainty and epistemic uncertainty, with aleatory uncertainty deriving from an inherent randomness in the behaviour of the system under study and epistemic uncertainty deriving from a lack of knowledge about the appropriate values to use for quantities that are assumed to have fixed but poorly known values in the context of a specific study. Aleatory uncertainty is usually represented with probability and leads to cumulative distribution functions (CDFs) or complementary CDFs (CCDFs) for analysis results of interest. Several mathematical structures are available for the representation of epistemic uncertainty, including interval analysis, possibility theory, evidence theory and probability theory. In the presence of epistemic uncertainty, there is not a single CDF or CCDF for a given analysis result. Rather, there is a family of CDFs and a corresponding family of CCDFs that derive from epistemic uncertainty and have an uncertainty structure that derives from the particular uncertainty structure (e.g. interval analysis, possibility theory, evidence theory or probability theory) used to represent epistemic uncertainty. Graphical formats for the representation of epistemic uncertainty in families of CDFs and CCDFs are investigated and presented for the indicated characterisations of epistemic uncertainty.},
author = {Helton, Jon C. and Johnson, Jay D. and Oberkampf, William L. and Sallaberry, C{\'{e}}dric J.},
doi = {10.1080/03081079.2010.486664},
issn = {0308-1079},
journal = {International Journal of General Systems},
number = {6},
pages = {605--646},
title = {{Representation of analysis results involving aleatory and epistemic uncertainty}},
volume = {39},
year = {2010}
}
@book{Quak2007,
author = {Quak, Ewald},
booktitle = {Geometric Modelling, Numerical Simulation, and Optimization: Applied Mathematics at SINTEF},
doi = {10.1007/978-3-540-68783-2},
file = {:home/nmlemus/Documents/Mendeley Desktop/Quak - 2007 - An Introduction to the Numerics of Flow in Porous Media using Matlab.pdf:pdf},
isbn = {9783540687825},
number = {December},
pages = {5--10},
title = {{An Introduction to the Numerics of Flow in Porous Media using Matlab}},
year = {2007}
}
@article{Baxter2016,
abstract = {A simple, simulation-based model of temporal uncertainty is presented that embraces other approaches recently proposed in the literature, including those more usually involving mathematical calculation rather than simulation. More specifically, it is shown how the random generation of dates for events, conditioned by uncertain temporal knowledge of the true date, can be adapted to what has been called the chronological apportioning of artefact assemblages and aoristic analysis (as a temporal rather than spatio-temporal method). The methodology is in the same spirit - though there are differences - as that underpinning the use of summed radiocarbon dates. A possibly novel approach to representing temporal change is suggested. Ideas are illustrated using data extracted from a large corpus of late Iron Age and Roman brooches, where the focus of interest was on their temporal distribution over a period of about 450 years.},
author = {Baxter, M J and Cool, H E M},
doi = {10.1016/j.jas.2015.12.007},
file = {:home/nmlemus/Documents/Mendeley Desktop/Baxter, Cool - 2016 - Reinventing the wheel Modelling temporal uncertainty with applications to brooch distributions in Roman Britain.pdf:pdf},
issn = {10959238},
journal = {Journal of Archaeological Science},
keywords = {Aoristic,Brooches,Modelling,Roman,Simulation,Temporal change,Temporal uncertainty},
pages = {120--127},
publisher = {Elsevier Ltd},
title = {{Reinventing the wheel? Modelling temporal uncertainty with applications to brooch distributions in Roman Britain}},
volume = {66},
year = {2016}
}
@article{Li2016,
abstract = {Stratigraphic (or lithological) uncertainty refers to the uncertainty of boundaries between different soil layers and lithological units, which has received increasing attention in geotechnical engineering. In this paper, an effective stochastic geological modeling framework is proposed based on Markov random field theory, which is conditional on site investigation data, such as observations of soil types from ground surface, borehole logs, and strata orientation from geophysical tests. The proposed modeling method is capable of accounting for the inherent heterogeneous and anisotropic characteristics of geological structure. In this method, two modeling approaches are introduced to simulate subsurface geological structures to accommodate different confidence levels on geological structure type (i.e., layered vs others). The sensitivity analysis for two modeling approaches is conducted to reveal the influence of mesh density and the model parameter on the simulation results. Illustrative examples using borehole data are presented to elucidate the ability to quantify the geological structure uncertainty. Furthermore, the applicability of two modeling approaches and the behavior of the proposed model under different model parameters are discussed in detail. Finally, Bayesian inferential framework is introduced to allow for the estimation of the posterior distribution of model parameter, when additional or subsequent borehole information becomes available. Practical guidance of using the proposed stochastic geological modeling technique for engineering practice is given.},
annote = {The paper present two different models ICM and MCMC. The authors evaluate the results and compare the models based in the information entropy.

They use Bayes inference too, to update the probability of some parameters. To do this they use information of boreholes.

This paper could be used to exemplify the needs of Bayesian Inference (Inverse Problem) into our framework.},
author = {Li, Zhao and Wang, Xiangrong and Wang, Hui and Liang, Robert Y},
doi = {10.1016/j.enggeo.2015.12.017},
issn = {00137952},
journal = {Engineering Geology},
keywords = {Geological modeling,Markov random field,Soil heterogeneity,Stratigraphic uncertainty,Uncertainty quantification},
pages = {106--122},
publisher = {Elsevier B.V.},
title = {{Quantifying stratigraphic uncertainties by stochastic simulation techniques based on Markov random field}},
url = {http://dx.doi.org/10.1016/j.enggeo.2015.12.017},
volume = {201},
year = {2016}
}
@article{Caers2014,
annote = {This paper could be used as a reference in the selection of a case of study.},
author = {Li, Lewis and Caers, Jef and Sava, Paul},
doi = {10.1190/segam2014-1402.1},
file = {:home/nmlemus/Documents/Mendeley Desktop/Li, Caers, Sava - 2014 - Uncertainty maps for seismic images through geostatistical model randomization.pdf:pdf},
journal = {SEG Technical Program Expanded Abstracts 2014},
keywords = {depth,fractals,imaging,interpretation,subsalt},
pages = {1496--1500},
title = {{Uncertainty maps for seismic images through geostatistical model randomization}},
url = {http://library.seg.org/doi/abs/10.1190/segam2014-1402.1},
year = {2014}
}
@article{Artijn2010,
author = {Artijn, I Mramsjah M and Om, J O H N V a N D E R B},
file = {:home/nmlemus/Documents/Mendeley Desktop/Artijn, Om - 2010 - ASSESSING SEISMIC UNCERTAINTY VIA GEOSTATISTICAL VELOCITY MODEL PERTURBATION AND IMAGE REGISTRATION AN APPLICATION T.pdf:pdf},
isbn = {9789039354513},
number = {3},
pages = {765--772},
title = {{ASSESSING SEISMIC UNCERTAINTY VIA GEOSTATISTICAL VELOCITY MODEL PERTURBATION AND IMAGE REGISTRATION: AN APPLICATION TO SUB-SALT IMAGING}},
volume = {7},
year = {2010}
}
@article{Zehner2010,
abstract = {Characterization of the earth's subsurface involves the construction of 3D models from sparse data and so leads to simulation results that involve some degree of uncertainty. This uncertainty is often neglected in the subsequent visualization, due to the fact that no established methods or available software exist. We describe a visualization method to render scalar fields with a probability density function at each data point. We render these data as isosurfaces and make use of a colour scheme, which intuitively gives the viewer an idea of which parts of the surface are more reliable than others. We further show how to extract an envelope that indicates within which volume the isosurface will lie with a certain confidence, and augment the isosurfaces with additional geometry in order to show this information. The resulting visualization is easy and intuitive to understand and is suitable for rendering multiple distinguishable isosurfaces at a time. It can moreover be easily used together with other visualized objects, such as the geological context. Finally we show how we have integrated this into a visualization pipeline that is based on the Visualization Toolkit (VTK) and the open source scenegraph OpenSG, allowing us to render the results on a desktop and in different kinds of virtual environments. ?? 2010 Elsevier Ltd.},
author = {Zehner, Bj{\"{o}}rn and Watanabe, Norihiro and Kolditz, Olaf},
doi = {10.1016/j.cageo.2010.02.010},
file = {:home/nmlemus/Documents/Mendeley Desktop/Zehner, Watanabe, Kolditz - 2010 - Visualization of gridded scalar data with uncertainty in geosciences.pdf:pdf},
isbn = {0098-3004},
issn = {00983004},
journal = {Computers and Geosciences},
keywords = {3D,Monte carlo simulation,Scalar fields,Uncertainty,Visualisation,Visualization},
number = {10},
pages = {1268--1275},
title = {{Visualization of gridded scalar data with uncertainty in geosciences}},
volume = {36},
year = {2010}
}
@article{Wellmann2012,
abstract = {Analyzing, visualizing and communicating uncertainties are important issues as geological models can never be fully determined. To date, there exists no general approach to quantify uncertainties in geological modeling. We propose here to use information entropy as an objective measure to compare and evaluate model and observational results. Information entropy was introduced in the 50s and defines a scalar value at every location in the model for predictability. We show that this method not only provides a quantitative insight into model uncertainties but, due to the underlying concept of information entropy, can be related to questions of data integration (i.e. how is the model quality interconnected with the used input data) and model evolution (i.e. does new data - or a changed geological hypothesis - optimize the model). In other words information entropy is a powerful measure to be used for data assimilation and inversion.As a first test of feasibility, we present the application of the new method to the visualization of uncertainties in geological models, here understood as structural representations of the subsurface. Applying the concept of information entropy on a suite of simulated models, we can clearly identify (a) uncertain regions within the model, even for complex geometries; (b) the overall uncertainty of a geological unit, which is, for example, of great relevance in any type of resource estimation; (c) a mean entropy for the whole model, important to track model changes with one overall measure. These results cannot easily be obtained with existing standard methods.The results suggest that information entropy is a powerful method to visualize uncertainties in geological models, and to classify the indefiniteness of single units and the mean entropy of a model quantitatively. Due to the relationship of this measure to the missing information, we expect the method to have a great potential in many types of geoscientific data assimilation problems - beyond pure visualization. {\textcopyright} 2011 Elsevier B.V.},
annote = {In general, they present a method to quantify uncertainty using information entropy. They apply the method just to spatial models (no time evolution). In the general case of spatio-temporal models, then we can use maxEnt (described in the conclusions). This could be another contribution.

The use of information entropy could be very good to compare models. With this idea we have many models in play. Then we need to create a DB structure to store the uncertainty asociated to those models and we need to implement the operators to perform the entropy evaluation above those models. To, evaluate a model itself, or to compare with other model.},
author = {Wellmann, J. Florian and Regenauer-Lieb, Klaus},
doi = {10.1016/j.tecto.2011.05.001},
file = {:home/nmlemus/Documents/Mendeley Desktop/Wellmann, Regenauer-Lieb - 2012 - Uncertainties have a meaning Information entropy as a quality measure for 3-D geological models.pdf:pdf},
isbn = {0040-1951},
issn = {00401951},
journal = {Tectonophysics},
keywords = {3-D geological modeling,Fuzziness,Information entropy,Simulation,Uncertainty,Visualization},
pages = {207--216},
publisher = {Elsevier B.V.},
title = {{Uncertainties have a meaning: Information entropy as a quality measure for 3-D geological models}},
url = {http://dx.doi.org/10.1016/j.tecto.2011.05.001},
volume = {526-529},
year = {2012}
}
@article{Scheidt2009,
abstract = {Assessing uncertainty of a spatial phenomenon requires the analysis$\backslash$nof a large number of parameters which must be processed by a transfer$\backslash$nfunction. To capture the possibly of a wide range of uncertainty$\backslash$nin the transfer function response, a large set of geostatistical$\backslash$nmodel realizations needs to be processed. Stochastic spatial simulation$\backslash$ncan rapidly provide multiple, equally probable realizations. However,$\backslash$nsince the transfer function is often computationally demanding, only$\backslash$na small number of models can be evaluated in practice, and are usually$\backslash$nselected through a ranking procedure. Traditional ranking techniques$\backslash$nfor selection of probabilistic ranges of response (P10, P50 and P90)$\backslash$nare highly dependent on the static property used. In this paper,$\backslash$nwe propose to parameterize the spatial uncertainty represented by$\backslash$na large set of geostatistical realizations through a distance function$\backslash$nmeasuring dissimilarity between any two geostatistical realizations.$\backslash$nThe distance function allows a mapping of the space of uncertainty.$\backslash$nThe distance can be tailored to the particular problem. The multi-dimensional$\backslash$nspace of uncertainty can be modeled using kernel techniques, such$\backslash$nas kernel principal component analysis (KPCA) or kernel clustering.$\backslash$nThese tools allow for the selection of a subset of representative$\backslash$nrealizations containing similar properties to the larger set. Without$\backslash$nlosing accuracy, decisions and strategies can then be performed applying$\backslash$na transfer function on the subset without the need to exhaustively$\backslash$nevaluate each realization. This method is applied to a synthetic$\backslash$noil reservoir, where spatial uncertainty of channel facies is modeled$\backslash$nthrough multiple realizations generated using a multi-point geostatistical$\backslash$nalgorithm and several training images.},
annote = {In general they porpouse a method to select a set of representative realizations. This is, using a distance function we can clusterize all the realizations and select just one representative realization by cluster.},
author = {Scheidt, C{\'{e}}line and Caers, Jef},
doi = {10.1007/s11004-008-9186-0},
file = {:home/nmlemus/Documents/Mendeley Desktop/Scheidt, Caers - 2009 - Representing spatial uncertainty using distances and kernels.pdf:pdf},
isbn = {1100400891860},
issn = {18748961},
journal = {Mathematical Geosciences},
keywords = {Distance,Geostatistics,Kernel methods,Ranking,Uncertainty quantification},
number = {4},
pages = {397--419},
title = {{Representing spatial uncertainty using distances and kernels}},
volume = {41},
year = {2009}
}
@book{DeCursi2015,
abstract = {Formerly CIP.},
annote = {From Duplicate 2 (Uncertainty Quantification and Stochastic Modeling with Matlab - de Cursi, Eduardo Souza; Sampaio, Rubens)

Muy dificil porque usa Analisis Funcional.},
author = {de Cursi, Eduardo Souza and Sampaio, Rubens},
doi = {10.1016/B978-1-78548-005-8.50009-8},
file = {:home/nmlemus/Documents/Mendeley Desktop/de Cursi, Sampaio - 2015 - Uncertainty Quantification and Stochastic Modeling with Matlab.pdf:pdf},
isbn = {9780081004715},
pages = {442},
title = {{Uncertainty Quantification and Stochastic Modeling with Matlab}},
url = {http://www.amazon.com/Uncertainty-Quantification-Stochastic-Modeling-Matlab/dp/1785480057},
year = {2015}
}
@article{Bernecker,
author = {Bernecker, Thomas and Emrich, Tobias and Kriegel, Hans-peter and Zuefle, Andreas and Chen, Lei and Lian, Xiang and Mamoulis, Nikos},
file = {:home/nmlemus/Documents/Mendeley Desktop/Bernecker et al. - 2011 - Managing uncertain spatio-temporal data.pdf:pdf},
isbn = {9781450310376},
title = {{Managing Uncertain Spatio-Temporal Data}}
}
@inproceedings{Bernecker2011a,
abstract = {Many spatial query problems defined on uncertain data are computationally expensive, in particular, if in addition to spatial attributes, a time component is added. Although there exists a wide range of applications dealing with uncertain spatio-temporal data, there is no solution for efficient management of such data available yet. This paper is the first work to propose general models for spatio-temporal uncertain data that have the potential to allow efficient processing on a wide range of queries. The main challenge here is to unfold this potential by developing new algorithms based on these models. In addition, we give examples of interesting spatio-temporal queries on uncertain data.},
address = {New York, New York, USA},
author = {Bernecker, Thomas and Emrich, Tobias and Kriegel, Hans-Peter and Zuefle, Andreas and Chen, Lei and Lian, Xiang and Mamoulis, Nikos},
booktitle = {Proceedings of the 2nd ACM SIGSPATIAL International Workshop on Querying and Mining Uncertain Spatio-Temporal Data - QUeST '11},
doi = {10.1145/2064969.2064972},
file = {:home/nmlemus/Documents/Mendeley Desktop/Bernecker et al. - 2011 - Managing uncertain spatio-temporal data.pdf:pdf},
isbn = {9781450310376},
number = {c},
pages = {16--20},
publisher = {ACM Press},
title = {{Managing uncertain spatio-temporal data}},
url = {http://dl.acm.org/citation.cfm?doid=2064969.2064972 http://dl.acm.org/citation.cfm?doid=2064969.2064972{\%}5Cnhttp://dl.acm.org/citation.cfm?doid=2064969.2064972{\%}5Cnhttp://dl.acm.org/citation.cfm?id=2064972},
year = {2011}
}
@article{Zhang2016,
abstract = {Stochastic spectral methods have become a popular technique to quantify the uncertainties of nano-scale devices and circuits. They are much more efficient than Monte Carlo for certain design cases with a small number of random parameters. However, their computational cost significantly increases as the number of random parameters increases. This paper presents a big-data approach to solve high-dimensional uncertainty quantification problems. Specifically, we simulate integrated circuits and MEMS at only a small number of quadrature samples, then, a huge number of (e.g., {\$}1.5 \backslashtimes 10{\^{}}{\{}27{\}}{\$}) solution samples are estimated from the available small-size (e.g., {\$}500{\$}) solution samples via a low-rank and tensor-recovery method. Numerical results show that our algorithm can easily extend the applicability of tensor-product stochastic collocation to IC and MEMS problems with over 50 random parameters, whereas the traditional algorithm can only handle several random parameters.},
archivePrefix = {arXiv},
arxivId = {1603.06119},
author = {Zhang, Zheng and Weng, Tsui-Wei and Daniel, Luca},
eprint = {1603.06119},
file = {:home/nmlemus/Documents/Mendeley Desktop/Zhang, Weng, Daniel - 2016 - A Big-Data Approach to Handle Process Variations Uncertainty Quantification by Tensor Recovery.pdf:pdf},
number = {2},
title = {{A Big-Data Approach to Handle Process Variations: Uncertainty Quantification by Tensor Recovery}},
url = {http://arxiv.org/abs/1603.06119},
year = {2016}
}
@article{Singh2008,
abstract = {Orion is a state-of-the-art uncertain database management system with built-in support for probabilistic data as first class data types. In contrast to other uncertain databases, Orion supports both attribute and tuple uncertainty with arbitrary correlations. This enables the database engine to handle both discrete and continuous pdfs in a natural and accurate manner. The underlying model is closed under the basic relational operators and is consistent with Possible Worlds Semantics. We demonstrate how Orion simplifies the design and enhances the capabilities of two example applications: managing sensor data (continuous uncertainty) and inferring missing values (discrete uncertainty).},
author = {Singh, Sarvjeet and Mayfield, Chris and Mittal, Sagar},
doi = {10.1145/1376616.1376744},
file = {:home/nmlemus/Documents/Mendeley Desktop/Singh, Mayfield, Mittal - 2008 - Orion 2.0 native support for uncertain data.pdf:pdf},
isbn = {9781605581026},
issn = {07308078},
journal = {In Proc. of the ACM Special Interest Group on Management of Data (SIGMOD 2008)},
pages = {1239--1241},
title = {{Orion 2.0: native support for uncertain data}},
url = {http://dl.acm.org/citation.cfm?id=1376744},
year = {2008}
}
@article{Beskales2008,
abstract = {Uncertainty pervades many domains in our lives. Current real-life applications, e.g., location tracking using GPS devices or cell phones, multimedia feature extraction, and sensor data management, deal with different kinds of uncertainty. Finding the nearest neighbor objects to a given query point is an important query type in these applications. In this paper, we study the problem of finding objects with the highest marginal probability of being the nearest neighbors to a query object. We adopt a general uncertainty model allowing for data and query uncertainty. Under this model, we define new query semantics, and provide several efficient evaluation algorithms. We analyze the cost factors involved in query evaluation, and present novel techniques to address the trade-offs among these factors. We give multiple extensions to our techniques including handling dependencies among data objects, and answering threshold queries. We conduct an extensive experimental study to evaluate our techniques on both real and synthetic data. {\textcopyright} 2008 VLDB Endowment.},
author = {Beskales, George and Soliman, M.a. Mohamed A and Ilyas, I.F. Ihab F},
doi = {10.14778/1453856.1453895},
file = {:home/nmlemus/Documents/Mendeley Desktop/Beskales, Soliman, Ilyas - 2008 - Efficient search for the Top-k probable nearest neighbors in uncertain databases.pdf:pdf},
isbn = {0000000000000},
issn = {21508097 (ISSN)},
journal = {Proceedings of the VLDB Endowment},
number = {1},
pages = {326--339},
title = {{Efficient search for the Top-k probable nearest neighbors in uncertain databases}},
url = {http://www.vldb.org/pvldb/1/1453895.pdf{\%}5Cnhttp://www.scopus.com/inward/record.url?eid=2-s2.0-84859170480{\&}partnerID=40{\&}md5=7e1d372e3a2771480ff9f393dd13314e},
volume = {1},
year = {2008}
}
@article{Jampani2011,
abstract = {The application of stochastic models and analysis techniques to large datasets is now commonplace. Unfortunately, in practice this usually means extracting data from a database system into an external tool (such as SAS, R, Arena, or Matlab), and then running the analysis there. This extract-and-model paradigm is typically error-prone, slow, does not support fine-grained modeling, and discourages what-if and sensitivity analyses. In this article we describe MCDB, a database system that permits a wide spectrum of stochastic models to be used in conjunction with the data stored in a large database, without ever extracting the data. MCDB facilitates in-database execution of tasks such as risk assessment, prediction, and imputation of missing data, as well as management of errors due to data integration, information extraction, and privacy-preserving data anonymization. MCDB allows a user to define random relations whose contents are determined by stochastic models. The models can then be queried using standard SQL. Monte Carlo techniques are used to analyze the probability distribution of the result of an SQL query over random relations. Novel tuple-bundle processing techniques can effectively control the Monte Carlo overhead, as shown in our experiments.},
author = {Jampani, Ravi and Xu, Fei and Wu, Mingxi and Perez, Luis and Jermaine, Chris and Haas, Peter J},
doi = {10.1145/2000824.2000828},
issn = {03625915},
journal = {ACM Transactions on Database Systems},
keywords = {MCDB,relational database systems,uncertainty},
number = {3},
pages = {1--41},
title = {{The monte carlo database system}},
url = {http://dl.acm.org/citation.cfm?doid=2000824.2000828},
volume = {36},
year = {2011}
}
@article{Haas,
author = {Haas, Peter J and Jermaine, Chris},
file = {:home/nmlemus/Documents/Mendeley Desktop/Haas, Jermaine - 2014 - MCDB and SimSQL Scalable Stochastic Analytics within the Database.pdf:pdf},
title = {{MCDB and SimSQL : Scalable Stochastic Analytics within the Database}},
year = {2014}
}
@article{Cai2013,
abstract = {This paper describes the SimSQL system, which allows for SQL-based specification, simulation, and querying of database-valued Markov chains, i.e., chains whose value at any time step comprises the contents of an entire database. SimSQL extends the earlier Monte Carlo database system (MCDB), which permitted Monte Carlo simulation of static database-valued random variables. Like MCDB, SimSQL uses user-specified "VG functions" to generate the simulated data values that are the building blocks of a simulated database. The enhanced functionality of SimSQL is enabled by the ability to parametrize VG functions using stochastic tables, so that one stochastic database can be used to parametrize the generation of another stochastic database, which can parametrize another, and so on. Other key extensions include the ability to explicitly define recursive versions of a stochastic table and the ability to execute the simulation in a MapReduce environment. We focus on applying SimSQL to Bayesian machine learning. Copyright {\textcopyright} 2013 ACM.},
author = {Cai, Z.a and Vagena, Z.b and Perez, L.c and Arumugam, S.a and Haas, P.J.c and Jermaine, C.a},
doi = {10.1145/2463676.2465283},
file = {:home/nmlemus/Documents/Mendeley Desktop/Cai et al. - 2013 - Simulation of database-valued Markov chains using SimSQL.pdf:pdf},
isbn = {9781450320375},
issn = {07308078},
journal = {Proceedings of the ACM SIGMOD International Conference on Management of Data},
keywords = {any time step comprises,chains,chains whose value at,database,databases,e,i,machine learning,markov chains,simsql employs many of,tents of an entire,the con-,the ideas},
pages = {637--648},
title = {{Simulation of database-valued Markov chains using SimSQL}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880568698{\&}partnerID=40{\&}md5=756efc38f2843d16507a920c0a4feec9},
year = {2013}
}
@book{Sirovich1996,
abstract = {This is the first of three volumes on partial differential equations. It introduces basic examples of partial differential equations, arising in continuum mechanics, electromagnetism, complex analysis and other areas, and develops a number of tools for their solution, including particularly Fourier analysis, distribution theory, and Sobolev spaces. These tools are applied to the treatment of basic problems in linear PDE, including the Laplace equation, heat equation, and wave equation, as well as more general elliptic, parabolic, and hyperbolic equations. Volume I prepares the way for studies of more advanced topics in linear PDE, in Volume 2, and for studies of nonlinear PDE, in Volume 3. The book is addressed to graduate students in mathematics and to professional mathematicians, with an interest in partial differential equations, mathematical physics, differential geometry, harmonic analysis, and complex analysis.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Sirovich, Antman J E Marsden L and Holmes, Hale P and Keller, J Keener J and Mielke, B J Matkowsky A and Sreenivasan, C S Peskin K R S},
booktitle = {Applied Mathematical Sciences},
doi = {10.1007/978-1-4419-7055-8},
eprint = {arXiv:1011.1669v3},
isbn = {0387404376},
issn = {00255572},
number = {399},
pages = {80},
pmid = {25246403},
title = {{Applied Mathematical Sciences}},
url = {http://books.google.com/books?id=0xtSyLjsphkC{\&}pgis=1},
volume = {115},
year = {1996}
}
@article{Feinberg2015,
abstract = {The paper describes the philosophy, design, functionality, and usage of the Python software toolbox Chaospy for performing uncertainty quantification via polynomial chaos expansions and Monte Carlo simulation. The paper compares Chaospy to similar packages and demonstrates a stronger focus on defining reusable software building blocks that can easily be assembled to construct new, tailored algorithms for uncertainty quantification. For example, a Chaospy user can in a few lines of high-level computer code define custom distributions, polynomials, integration rules, sampling schemes, and statistical metrics for uncertainty analysis. In addition, the software introduces some novel methodological advances, like a framework for computing Rosenblatt transformations and a new approach for creating polynomial chaos expansions with dependent stochastic variables.},
author = {Feinberg, Jonathan and Langtangen, Hans Petter},
doi = {10.1016/j.jocs.2015.08.008},
file = {:home/nmlemus/Documents/Mendeley Desktop/Feinberg, Langtangen - 2015 - Chaospy An open source tool for designing methods of uncertainty quantification.pdf:pdf},
issn = {18777503},
journal = {Journal of Computational Science},
keywords = {Monte Carlo simulation,Polynomial chaos expansions,Python package,Rosenblatt transformations,Uncertainty quantification},
pages = {46--57},
publisher = {Elsevier B.V.},
title = {{Chaospy: An open source tool for designing methods of uncertainty quantification}},
volume = {11},
year = {2015}
}
@article{Roy2011,
abstract = {An overview of a comprehensive framework is given for estimating the predictive uncertainty of scientific computing applications. The framework is comprehensive in the sense that it treats both types of uncertainty (aleatory and epistemic), incorporates uncertainty due to the mathematical form of the model, and it provides a procedure for including estimates of numerical error in the predictive uncertainty. Aleatory (random) uncertainties in model inputs are treated as random variables, while epistemic (lack of knowledge) uncertainties are treated as intervals with no assumed probability distributions. Approaches for propagating both types of uncertainties through the model to the system response quantities of interest are briefly discussed. Numerical approximation errors (due to discretization, iteration, and computer round off) are estimated using verification techniques, and the conversion of these errors into epistemic uncertainties is discussed. Model form uncertainty is quantified using (a) model validation procedures, i.e., statistical comparisons of model predictions to available experimental data, and (b) extrapolation of this uncertainty structure to points in the application domain where experimental data do not exist. Finally, methods for conveying the total predictive uncertainty to decision makers are presented. The different steps in the predictive uncertainty framework are illustrated using a simple example in computational fluid dynamics applied to a hypersonic wind tunnel. ?? 2011 Elsevier B.V.},
annote = {All the concepts, definitions, source of uncertainty, types of uncertainty, a full workflow. Very usefull to reference all the steps of the UQ process.},
author = {Roy, Christopher J. and Oberkampf, William L.},
doi = {10.1016/j.cma.2011.03.016},
file = {:home/nmlemus/Documents/Mendeley Desktop/Roy, Oberkampf - 2011 - A comprehensive framework for verification, validation, and uncertainty quantification in scientific computing.pdf:pdf},
isbn = {0045-7825},
issn = {00457825},
journal = {Computer Methods in Applied Mechanics and Engineering},
keywords = {Computational simulation,Modeling,Uncertainty quantification,Validation,Verification},
number = {25-28},
pages = {2131--2144},
publisher = {Elsevier B.V.},
title = {{A comprehensive framework for verification, validation, and uncertainty quantification in scientific computing}},
volume = {200},
year = {2011}
}
@article{Guerra2009b,
author = {Guerra, Gabriel and Rochinha, Fernando and Elias, Renato and Coutinho, Alvaro and Braganholo, Vanessa and de Oliveira, Daniel and Ogasawara, Eduardo and Chirigati, Fernando and Mattoso, Marta},
file = {:home/nmlemus/Documents/Mendeley Desktop/Guerra et al. - 2009 - Scientific Workflow Management System Applied to Uncertainty Quantification in Large Eddy Simulation.pdf:pdf},
number = {November},
title = {{Scientific Workflow Management System Applied to Uncertainty Quantification in Large Eddy Simulation}},
year = {2009}
}
@inproceedings{Lavril2016,
author = {Lavril, Thibault and Mattoso, Marta and Costa, Danilo and Rochinha, Fernando A and Miras, Thomas},
booktitle = {Proceedings of the XXXVII Iberian Latin-American Congress on Computational Methods in Engineering},
title = {{CONTROLLING PARALLEL ADAPTIVE SPARSE GRID STOCHASTIC COLLOCATION SIMULATIONS WITH}},
year = {2016}
}
@phdthesis{Goncalves2015,
author = {Gon{\c{c}}alves, Bernardo},
title = {{Managing large-scale scientific hypotheses as uncertain and probabilistic data}},
year = {2015}
}
@article{Ericson2010,
author = {Ericson, Douglas},
file = {:home/nmlemus/Documents/Mendeley Desktop/Ericson - 2010 - QEF - Manual do Usu{\'{a}}rio Douglas Ericson Fabio Porto.pdf:pdf},
journal = {Scenario},
title = {{QEF - Manual do Usu{\'{a}}rio Douglas Ericson Fabio Porto}},
year = {2010}
}
@misc{Marelli2014,
author = {Marelli, Stefano and Sudret, Bruno},
file = {:home/nmlemus/Documents/Mendeley Desktop/Marelli, Sudret - 2014 - UQLAB a framework for Uncertainty Quantification in MATLAB.pdf:pdf},
number = {Bourinet 2009},
pages = {2554--2563},
title = {{UQLAB: a framework for Uncertainty Quantification in MATLAB}},
year = {2014}
}
@article{Guerra2012,
abstract = {Computational simulation of complex engineered systems requires intensive computation and a significant amount of data management. Today, this management is often carried out on a case-by-case basis and requires great effort to track it. This is due to the complexity of controlling a large amount of data flowing along a chain of simulations. Moreover, many times there is a need to explore parameter variability for the same set of data. On a case-by-case basis, there is no register of data involved in the simulation, making this process prone to errors. In addition, if the user wants to analyze the behavior of a simulation sample, then he/she must wait until the end of the whole simulation. In this context, techniques and methodologies of scientific workflows can improve the management of simulations. Parameter variability can be put in the general context of uncertainty quantification (UQ), which provides a rational perspective for analysts and decision makers. The objective of this work is to use scientific workflows to provide a systematic approach in: (i) modeling UQ numerical experiments as scientific workflows, (ii) offering query tools to evaluate UQ processes at runtime, (iii) managing theUQanalysis, and (iv) managingUQin parallel executions. When using scientific workflow engines, one can collect data in a transparent manner, allowing execution steering, the postassessment of results, and providing the information for reexecuting the experiment, thereby ensuring reproducibility, an essential characteristic in a scientific or engineering computational experiment.},
author = {Guerra, Gabriel and Rochinha, Fernando A. and Elias, Renato and de Oliveira, Daniel and Ogasawara, Eduardo and Dias, Jonas Furtado and Mattoso, Marta and Coutinho, Alvaro L. G. A.},
doi = {10.1615/Int.J.UncertaintyQuantification.2012003593},
file = {:home/nmlemus/Documents/Mendeley Desktop/Guerra et al. - 2012 - Uncertainty Quantification in Computational Predictive Models for Fluid Dynamics Using a Workflow Management Engi.pdf:pdf},
issn = {2152-5080},
journal = {International Journal for Uncertainty Quantification},
keywords = {adaptive sparse grid,computational fluid dynamics,method,parallelization,provenance,scientific workflows,sparse grid stochastic collocation},
number = {1},
pages = {53--71},
title = {{Uncertainty Quantification in Computational Predictive Models for Fluid Dynamics Using a Workflow Management Engine}},
url = {http://www.begellhouse.com/journals/52034eb04b657aea,69f226067bce0f5b,1b427aac4a956792.html},
volume = {2},
year = {2012}
}
@article{Porto2011,
author = {Porto, Fabio and Moura, Ana Maria De C and Gon{\c{c}}alves, Bernardo and Costa, Ramon and Lustosa, Hermano and Corr{\^{e}}a, Frederico},
doi = {10.4018/JDM.2015040101},
file = {:home/nmlemus/Documents/Mendeley Desktop/Porto et al. - 2011 - Modeling and Implementing Scientific Hypotheses.pdf:pdf},
issn = {15338010},
keywords = {conceptual model,escience,scientific hypothesis},
title = {{Modeling and Implementing Scientific Hypotheses}},
year = {2011}
}
@article{Ji2014,
abstract = {An earth system model has been developed at Beijing Normal University (Beijing Normal University Earth System Model, BNU-ESM); the model is based on several widely evaluated climate model components and is used to study mechanisms of ocean-atmosphere interactions, natural climate variability and carbon-climate feedbacks at interannual to interdecadal time scales. In this paper, the model structure and individual components are described briefly. Further, results for the CMIP5 (Coupled Model Intercomparison Project phase 5) pre-industrial control and historical simulations are presented to demonstrate the model's performance in terms of the mean model state and the internal variability. It is illustrated that BNU-ESM can simulate many observed features of the earth climate system, such as the climatological annual cycle of surface-air temperature and precipitation, annual cycle of tropical Pacific sea surface temperature (SST), the overall patterns and positions of cells in global ocean meridional overturning circulation. For example, the El Ni{\~{n}}o-Southern Oscillation (ENSO) simulated in BNU-ESM exhibits an irregular oscillation between 2 and 5 years with the seasonal phase locking feature of ENSO. Important biases with regard to observations are presented and discussed, including warm SST discrepancies in the major upwelling regions, an equatorward drift of midlatitude westerly wind bands, and tropical precipitation bias over the ocean that is related to the double Intertropical Convergence Zone (ITCZ).},
author = {Ji, D. and Wang, L. and Feng, J. and Wu, Q. and Cheng, H. and Zhang, Q. and Yang, J. and Dong, W. and Dai, Y. and Gong, D. and Zhang, R. H. and Wang, X. and Liu, J. and Moore, J. C. and Chen, D. and Zhou, M.},
doi = {10.5194/gmd-7-2039-2014},
file = {:home/nmlemus/Documents/Mendeley Desktop/Ji et al. - 2014 - Description and basic evaluation of Beijing Normal University Earth System Model (BNU-ESM) version 1.pdf:pdf},
isbn = {1991-9603},
issn = {19919603},
journal = {Geoscientific Model Development},
number = {5},
pages = {2039--2064},
title = {{Description and basic evaluation of Beijing Normal University Earth System Model (BNU-ESM) version 1}},
volume = {7},
year = {2014}
}
@article{Estacio-Hiroms2012,
author = {Estacio-Hiroms, Kemelli C and Prudencio, Ernesto E},
file = {:home/nmlemus/Documents/Mendeley Desktop/Estacio-Hiroms, Prudencio - 2012 - User's Manual Quantification of Uncertainty for Estimation, Simulation, and Optimization (QUESO).pdf:pdf},
keywords = {research,statistical,uncertainty quantification},
title = {{User's Manual: Quantification of Uncertainty for Estimation, Simulation, and Optimization (QUESO)}},
year = {2012}
}
@article{Crespo2014,
abstract = {ABSTRACT In this work, an integrated framework to deal with scarce data, aleatory and epistemic uncertainties is presented. Generally, dealing with the uncertainty requires the availability of efficient and scalable computational tools. For this reason, the proposed strategies have been implemented in an open general purpose computational framework for uncertainty quantification and management that allows for a significant reduction of the computational time required by adopting efficient techniques for uncertainty quantification and resorting to the computational power of a cluster computing. The proposed framework has been adopted to solve the NASA Langley multidisciplinary uncertainty quantification challenge. All the five subproblems have been tacked, i.e. uncertainty characterization, sensitivity analysis, uncertainty quantification, extreme case analysis and robust design. All the subproblems have been solved using different approaches based on different hypotheses and assumption in order to cross-validate the results and showing the flexibility and potentiality and computational framework.},
author = {Crespo, Luis G. and Kenny, Sean P. and Giesy, Daniel P.},
doi = {10.2514/6.2014-1347},
file = {:home/nmlemus/Documents/Mendeley Desktop/Crespo, Kenny, Giesy - 2014 - The NASA Langley Multidisciplinary Uncertainty Quantification Challenge.pdf:pdf},
isbn = {978-1-62410-312-4},
journal = {16th AIAA Non-Deterministic Approaches Conference},
number = {January},
pages = {1--9},
title = {{The NASA Langley Multidisciplinary Uncertainty Quantification Challenge}},
url = {http://arc.aiaa.org/doi/abs/10.2514/6.2014-1347},
year = {2014}
}
@article{Oden2013,
abstract = {We address general approaches to the rational selection and validation of mathematical and computational models of tumor growth using methods of Bayesian inference. The model classes are derived from a general diffuse-interface, continuum mixture theory and focus on mass conservation of mixtures with up to four species. Synthetic data are generated using higher-order base models. We discuss general approaches to model cal- ibration, validation, plausibility, and selection based on Bayesian-based methods, infor- mation theory, and maximum information entropy.We also address computational issues and provide numerical experiments based on Markov chain Monte Carlo algorithms and high performance computing implementations.},
author = {Oden, J Tinsley and Prudencio, Ernesto E and Hawkins-Daarud, Andrea},
doi = {10.1142/S0218202513500103},
issn = {0218-2025},
journal = {Mathematical Models and Methods in Applied Sciences},
keywords = {Bayesian statistics,Markov chain Monte Carlo methods.,diffuse-interface models,model selec- tion,model validation},
number = {7},
pages = {1309--1338},
title = {{Selection and Assessment of Phenomenological Models of Tumor Growth}},
url = {http://www.worldscientific.com/doi/abs/10.1142/S0218202513500103},
volume = {23},
year = {2013}
}
@article{Paper2016,
author = {Paper, Conference and Gon, Bernardo and Machado, Fabio and Laborat, Porto},
isbn = {9781450319218},
number = {March},
title = {{Research Lattice: Towards a Data Model for Scientific Hypotheses}},
year = {2016}
}
@inproceedings{Goncalves2013,
address = {New York, New York, USA},
author = {Gon{\c{c}}alves, Bernardo and Porto, Fabio},
booktitle = {Proceedings of the 25th International Conference on Scientific and Statistical Database Management},
doi = {10.1145/2484838.2484861},
isbn = {978-1-4503-1921-8},
keywords = {large-scale science,lattice theory,research progress,scientific databases,scientific hypothesis},
pages = {41:1----41:4},
publisher = {ACM Press},
title = {{Research Lattices: Towards a Scientific Hypothesis Data Model}},
url = {http://dl.acm.org/citation.cfm?doid=2484838.2484861 http://doi.acm.org/10.1145/2484838.2484861},
year = {2013}
}
@article{Farrell2015a,
author = {Farrell, Kathryn Anne},
file = {:home/nmlemus/Documents/Mendeley Desktop/Farrell - 2015 - Selection , Calibration , and Validation of Coarse-Grained Models of Atomistic Systems.pdf:pdf},
title = {{Selection , Calibration , and Validation of Coarse-Grained Models of Atomistic Systems}},
year = {2015}
}
@article{Farrell2015,
abstract = {A general adaptive modeling algorithm for selection and validation of coarse-grained models of atomistic systems is presented. A Bayesian framework is developed to address uncertainties in parameters, data, and model selection. Algorithms for computing output sensitivities to parameter variances, model evidence and posterior model plausibilities for given data, and for computing what are referred to as Occam Categories in reference to a rough measure of model simplicity, make up components of the overall approach. Computational results are provided for representative applications.},
author = {Farrell, Kathryn and Oden, J. Tinsley and Faghihi, Danial},
doi = {10.1016/j.jcp.2015.03.071},
file = {:home/nmlemus/Documents/Mendeley Desktop/Farrell, Oden, Faghihi - 2015 - A Bayesian framework for adaptive selection, calibration, and validation of coarse-grained models of ato.pdf:pdf},
issn = {10902716},
journal = {Journal of Computational Physics},
keywords = {Bayesian inference,Coarse graining models,Model plausibility,Model validation,Output sensitivities},
pages = {189--208},
publisher = {Elsevier Inc.},
title = {{A Bayesian framework for adaptive selection, calibration, and validation of coarse-grained models of atomistic systems}},
url = {http://dx.doi.org/10.1016/j.jcp.2015.03.071},
volume = {295},
year = {2015}
}
@phdthesis{Melorose2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Melorose, J. and Perroy, R. and Careas, S.},
booktitle = {Statewide Agricultural Land Use Baseline 2015},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/nmlemus/Documents/Mendeley Desktop/Melorose et al. - 2015 - A PROBABILISTIC FRAMEWORK FOR UNCERTAINTY QUANTIFICATION IN LARGE-SCALE SIMULATIONS APPLICATION IN SEISMIC IMAG.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
pmid = {25246403},
title = {{A PROBABILISTIC FRAMEWORK FOR UNCERTAINTY QUANTIFICATION IN LARGE-SCALE SIMULATIONS: APPLICATION IN SEISMIC IMAGING}},
volume = {1},
year = {2015}
}
@article{Guerra2016,
author = {Guerra, Gabriel M. and Zio, Souleymane and Camata, Jose J. and Dias, Jonas and Elias, Renato N. and Mattoso, Marta and {B. Paraizo}, Paulo L. and {G. A. Coutinho}, Alvaro L. and Rochinha, Fernando A.},
doi = {10.1007/s10596-016-9563-6},
file = {:home/nmlemus/Documents/Mendeley Desktop/Guerra et al. - 2016 - Uncertainty quantification in numerical simulation of particle-laden flows.pdf:pdf},
issn = {1420-0597},
journal = {Computational Geosciences},
number = {1},
pages = {265--281},
title = {{Uncertainty quantification in numerical simulation of particle-laden flows}},
url = {http://link.springer.com/10.1007/s10596-016-9563-6},
volume = {20},
year = {2016}
}
@phdthesis{Sankararaman2012,
author = {Sankararaman, Shankar},
booktitle = {PhD Dissertation},
file = {:home/nmlemus/Documents/Mendeley Desktop/Sankararaman - 2012 - Uncertainty Quantification and Integration.pdf:pdf},
school = {Vanderbilt University},
title = {{Uncertainty Quantification and Integration}},
year = {2012}
}
@article{Noh2010,
abstract = {In RBDO, input uncertainty models such as marginal and joint cumulative$\backslash$ndistribution functions (CDFs) need to be used. However, only limited$\backslash$ndata exists in industry applications. Thus, identification of the$\backslash$ninput uncertainty model is challenging especially when input variables$\backslash$nare correlated. Since input random variables, such as fatigue material$\backslash$nproperties, are correlated in many industrial problems, the joint$\backslash$nCDF of correlated input variables needs to be correctly identified$\backslash$nfrom given data. In this paper, a Bayesian method is proposed to$\backslash$nidentify the marginal and joint CDFs from given data where a copula,$\backslash$nwhich only requires marginal CDFs and correlation parameters, is$\backslash$nused to model the joint CDF of input variables. Using simulated data$\backslash$nsets, performance of the Bayesian method is tested for different$\backslash$nnumbers of samples and is compared with the goodness-of-fit (GOF)$\backslash$ntest. Two examples are used to demonstrate how the Bayesian method$\backslash$nis used to identify correct marginal CDFs and copula.},
author = {Noh, Yoojeong and Choi, K. K. and Lee, Ikjin},
doi = {10.1007/s00158-009-0385-1},
file = {:home/nmlemus/Documents/Mendeley Desktop/Noh, Choi, Lee - 2010 - Identification of marginal and joint CDFs using Bayesian method for RBDO.pdf:pdf},
isbn = {0015800903},
issn = {1615147X},
journal = {Structural and Multidisciplinary Optimization},
keywords = {Bayesian method,Copula,Correlated input variables,Goodness-of-fit test,Identification of marginal and joint CDFs,Input model uncertainty,Reliability-based design optimization},
number = {1-6},
pages = {35--51},
title = {{Identification of marginal and joint CDFs using Bayesian method for RBDO}},
volume = {40},
year = {2010}
}
@article{Ferretti2016,
abstract = {The majority of published sensitivity analyses (SAs) are either local or one factor-at-a-time (OAT) analyses, relying on unjustified assumptions of model linearity and additivity. Global approaches to sensitivity analyses (GSA) which would obviate these shortcomings, are applied by a minority of researchers. By reviewing the academic literature on SA, we here present a bibliometric analysis of the trends of different SA practices in last decade. The review has been conducted both on some top ranking journals (Nature and Science) and through an extended analysis in the Elsevier's Scopus database of scientific publications. After correcting for the global growth in publications, the amount of papers performing a generic SA has notably increased over the last decade. Even if OAT is still the most largely used technique in SA, there is a clear increase in the use of GSA with preference respectively for regression and variance-based techniques. Even after adjusting for the growth of publications in the sole modelling field, to which SA and GSA normally apply, the trend is confirmed. Data about regions of origin and discipline are also briefly discussed. The results above are confirmed when zooming on the sole articles published in chemical modelling, a field historically proficient in the use of SA methods.},
annote = {Solo es una revision bibliografica, habla del aumento del uso de los metodos de analisis de sensibilidad en los ultimos anos. De como se usa mucho el analisis de un solo parametro a la vez, lo que no funciona correctamente.},
author = {Ferretti, Federico and Saltelli, Andrea and Tarantola, Stefano},
doi = {10.1016/j.scitotenv.2016.02.133},
issn = {00489697},
journal = {Science of the Total Environment},
keywords = {Bibliometric analysis,Chemical modelling,Global sensitivity analysis,Sensitivity analysis,bibliometric analysis,global sensitivity analysis,sensitivity analysis},
pages = {2--6},
pmid = {26934843},
publisher = {Elsevier B.V.},
title = {{Trends in Sensitivity Analysis practice in the last decade}},
url = {http://dx.doi.org/10.1016/j.scitotenv.2016.02.133},
volume = {21027},
year = {2016}
}
@book{Geris2016,
author = {Geris, Liesbet and Gomez-Cabrero, David},
doi = {10.1007/978-3-319-21296-8},
isbn = {978-3-319-21295-1},
pages = {471},
title = {{Uncertainty in Biology}},
url = {http://link.springer.com/10.1007/978-3-319-21296-8},
volume = {17},
year = {2016}
}
@article{Yi2016,
abstract = {Sensitivity analysis is a primary approach used in mathematical modeling to identify important factors that control the response dynamics in a model. In this paper, we applied the Morris sensitivity analysis method to identify the important factors governing the dynamics in a complex 3-dimensional water quality model. The water quality model was developed using the Environmental fluid dynamics code (EFDC) to simulate the fate and transport of nutrients and algal dynamics in Lake Dianchi, one of the most polluted large lakes in China. The analysis focused on the response of four water quality constituents, including chlorophyll-a, dissolved oxygen, total nitrogen, and total phosphorus, to 47 parameters and 7 external driving forces. We used Morris sensitivity analysis with different sample sizes and factor perturbation ranges to study the sensitivity with regard to different output metrics of the water quality model, and we analyzed the consistency between different sensitivity scenarios. In addition to the analysis with aggregate outputs, a spatiotemporal variability analysis was performed to understand the spatial heterogeneity and temporal distribution of sensitivities. Our results indicated that it is important to consider multiple characteristics in a sensitivity analysis, and we have identified a robust set of sensitive factors in the water quality model that will be useful for systematic model parameter identification and uncertainty analysis.},
author = {Yi, Xuan and Zou, Rui and Guo, Huaicheng},
doi = {10.1016/j.ecolmodel.2016.01.005},
issn = {03043800},
journal = {Ecological Modelling},
keywords = {EFDC model,Morris screening,Sensitivity analysis,Spatio-tenporal Sensitivity Analysis,Spatiotemporal sensitivity indices,Water quality model},
mendeley-tags = {Spatio-tenporal Sensitivity Analysis},
pages = {74--84},
publisher = {Elsevier B.V.},
title = {{Global sensitivity analysis of a three-dimensional nutrients-algae dynamic model for a large shallow lake}},
url = {http://dx.doi.org/10.1016/j.ecolmodel.2016.01.005},
volume = {327},
year = {2016}
}
@article{Wick2010,
abstract = {Incorporating probabilities into the semantics of incomplete databases has posed many challenges, forcing systems to sacrifice modeling power, scalability, or treatment of relational algebra operators. We propose an alternative approach where the underlying relational database always represents a single world, and an external factor graph encodes a distribution over possible worlds; Markov chain Monte Carlo (MCMC) inference is then used to recover this uncertainty to a desired level of fidelity. Our approach allows the efficient evaluation of arbitrary queries over probabilistic databases with arbitrary dependencies expressed by graphical models with structure that changes during inference. MCMC sampling provides efficiency by hypothesizing modifications to possible worlds rather than generating entire worlds from scratch. Queries are then run over the portions of the world that change, avoiding the onerous cost of running full queries over each sampled world. A significant innovation of this work is the connection between MCMC sampling and materialized view maintenance techniques: we find empirically that using view maintenance techniques is several orders of magnitude faster than naively querying each sampled world. We also demonstrate our system's ability to answer relational queries with aggregation, and demonstrate additional scalability through the use of parallelization on a real-world complex model of information extraction. This framework is sufficiently expressive to support probabilistic inference not only for answering queries, but also for inferring missing database content from raw evidence.},
archivePrefix = {arXiv},
arxivId = {1005.1934},
author = {Wick, Michael and McCallum, A and Miklau, Gerome},
doi = {10.14778/1920841.1920942},
eprint = {1005.1934},
issn = {21508097},
journal = {Proceedings of the VLDB Endowment},
number = {1-2},
pages = {794--804},
title = {{Scalable probabilistic databases with factor graphs and mcmc}},
url = {papers2://publication/uuid/6327ACAE-8C67-4F02-80AD-ACD061F4E477},
volume = {3},
year = {2010}
}
@article{Helton2010,
abstract = {Procedures are described for the representation of results in analyses that involve both aleatory uncertainty and epistemic uncertainty, with aleatory uncertainty deriving from an inherent randomness in the behaviour of the system under study and epistemic uncertainty deriving from a lack of knowledge about the appropriate values to use for quantities that are assumed to have fixed but poorly known values in the context of a specific study. Aleatory uncertainty is usually represented with probability and leads to cumulative distribution functions (CDFs) or complementary CDFs (CCDFs) for analysis results of interest. Several mathematical structures are available for the representation of epistemic uncertainty, including interval analysis, possibility theory, evidence theory and probability theory. In the presence of epistemic uncertainty, there is not a single CDF or CCDF for a given analysis result. Rather, there is a family of CDFs and a corresponding family of CCDFs that derive from epistemic uncertainty and have an uncertainty structure that derives from the particular uncertainty structure (e.g. interval analysis, possibility theory, evidence theory or probability theory) used to represent epistemic uncertainty. Graphical formats for the representation of epistemic uncertainty in families of CDFs and CCDFs are investigated and presented for the indicated characterisations of epistemic uncertainty.$\backslash$nProcedures are described for the representation of results in analyses that involve both aleatory uncertainty and epistemic uncertainty, with aleatory uncertainty deriving from an inherent randomness in the behaviour of the system under study and epistemic uncertainty deriving from a lack of knowledge about the appropriate values to use for quantities that are assumed to have fixed but poorly known values in the context of a specific study. Aleatory uncertainty is usually represented with probability and leads to cumulative distribution functions (CDFs) or complementary CDFs (CCDFs) for analysis results of interest. Several mathematical structures are available for the representation of epistemic uncertainty, including interval analysis, possibility theory, evidence theory and probability theory. In the presence of epistemic uncertainty, there is not a single CDF or CCDF for a given analysis result. Rather, there is a family of CDFs and a corresponding family of CCDFs that derive from epistemic uncertainty and have an uncertainty structure that derives from the particular uncertainty structure (e.g. interval analysis, possibility theory, evidence theory or probability theory) used to represent epistemic uncertainty. Graphical formats for the representation of epistemic uncertainty in families of CDFs and CCDFs are investigated and presented for the indicated characterisations of epistemic uncertainty.},
author = {Helton, Jon C. and Johnson, Jay D. and Oberkampf, William L. and Sallaberry, C{\'{e}}dric J.},
doi = {10.1080/03081079.2010.486664},
issn = {0308-1079},
journal = {International Journal of General Systems},
keywords = {aleatory uncertainty,epistemic uncertainty,evidence theory,interval analysis,possibility theory,probability theory},
number = {6},
pages = {605--646},
publisher = {Taylor {\&} Francis Group},
title = {{Representation of analysis results involving aleatory and epistemic uncertainty}},
volume = {39},
year = {2010}
}
@article{Mullins2016,
abstract = {This paper investigates model validation under a variety of different data scenarios and clarifies how different validation metrics may be appropriate for different scenarios. In the presence of multiple uncertainty sources, model validation metrics that compare the distributions of model prediction and observation are considered. Both ensemble validation and point-by-point approaches are discussed, and it is shown how applying the model reliability metric point-by-point enables the separation of contributions from aleatory and epistemic uncertainty sources. After individual validation assessments are made at different input conditions, it may be desirable to obtain an overall measure of model validity across the entire domain. This paper proposes an integration approach that assigns weights to the validation results according to the relevance of each validation test condition to the overall intended use of the model in prediction. Since uncertainty propagation for probabilistic validation is often unaffordable for complex computational models, surrogate models are often used; this paper proposes an approach to account for the additional uncertainty introduced in validation by the uncertain fit of the surrogate model. The proposed methods are demonstrated with a microelectromechanical system (MEMS) example.},
author = {Mullins, Joshua and Ling, You and Mahadevan, Sankaran and Sun, Lin and Strachan, Alejandro},
doi = {10.1016/j.ress.2015.10.003},
file = {:home/nmlemus/Documents/Mendeley Desktop/Mullins et al. - 2016 - Separation of aleatory and epistemic uncertainty in probabilistic model validation.pdf:pdf},
issn = {09518320},
journal = {Reliability Engineering and System Safety},
keywords = {Epistemic uncertainty,Imprecise data,Model validation,Reliability,Validation metrics},
pages = {49--59},
title = {{Separation of aleatory and epistemic uncertainty in probabilistic model validation}},
volume = {147},
year = {2016}
}
@article{Kumar2015d,
abstract = {Advanced analytics is a booming area in both industry and academia. Several projects aim to implement ML algorithms efficiently. But three key challenging and iterative practical tasks in using ML – feature engi-neering, algorithm selection, and parameter tuning, collectively called model selection – have largely been overlooked by the data management community even though these are often the most time-consuming tasks for analysts. To make the iterative process of model se-lection easier and faster, we envision a unifying abstract framework that acts a basis for a new class of analytics systems that we call model selection management sys-tems (MSMS). We discuss how time-tested ideas from database research offer new avenues to improve model selection, and outline how MSMS are a new frontier for interesting and impactful data management research.},
author = {Kumar, Arun and Mccann, Robert and Naughton, Jeffrey and Patel, Jignesh M.},
doi = {10.1145/2935694.2935698},
file = {:home/nmlemus/Documents/Mendeley Desktop/p17-kumar.pdf:pdf},
issn = {0163-5808},
journal = {ACM SIGMOD Record},
keywords = {Feature Engineering,Iterative Model Selection,Performance Optimization,Provenance for Machine Learning,Usability of Machine Learning},
month = {may},
number = {4},
pages = {17--22},
publisher = {ACM},
title = {{Model Selection Management Systems: The Next Frontier of Advanced Analytics}},
url = {http://dl.acm.org/citation.cfm?doid=2935694.2935698 http://pages.cs.wisc.edu/{~}arun/vision/},
volume = {44},
year = {2015}
}
@article{Gaganis2001,
author = {Gaganis, Petros and Smith, Leslie},
file = {:home/nmlemus/Documents/Mendeley Desktop/Gaganis, Smith - 2001 - A Bayesian approach to the quantification of the effect of model error on the predictions of groundwater models.pdf:pdf},
journal = {Water Resources},
keywords = {doi:10.102,http://dx.doi.org/10.1029/2000WR000001},
number = {9},
pages = {2309 --2322},
title = {{A Bayesian approach to the quantification of the effect of model error on the predictions of groundwater models}},
volume = {37},
year = {2001}
}
@article{Zhang2008,
abstract = {Uncertain data are inherent in many important applications. Recently, considerable research efforts have been put into the field of managing uncertain data. In this paper, we summarize existing techniques to query and model uncertain data and systems that effectively manage uncertain data, mainly from a probabilistic point of view.},
author = {Zhang, Wenjie and Lin, Xuemin and Pei, Jian and Zhang, Ying},
doi = {10.1109/WAIM.2008.42},
file = {:home/nmlemus/Documents/Mendeley Desktop/Zhang et al. - 2008 - Managing uncertain data Probabilistic approaches.pdf:pdf},
isbn = {9780769531854},
journal = {Proceedings - The 9th International Conference on Web-Age Information Management, WAIM 2008},
pages = {405--412},
title = {{Managing uncertain data: Probabilistic approaches}},
year = {2008}
}
@article{Ceylan2016,
author = {Ceylan, Ismail Ilkan and Darwiche, Adnan and Broeck, Guy Van Den},
file = {:home/nmlemus/Documents/Mendeley Desktop/Ceylan, Darwiche, Broeck - 2016 - Open-World Probabilistic Databases.pdf:pdf},
journal = {Proc.$\backslash$ of KR'16},
title = {{Open-World Probabilistic Databases}},
year = {2016}
}
@article{Bretthorst1996,
abstract = {Probability theory as logic is founded on three simple desiderata: that degrees of belief should be represented by real numbers, that one should reason consistently, and that the theory should reduce to Aristotelian logic when the truth values of the hypotheses are known. Because this theory represents a probability as a state of knowledge, not a state of nature, hypotheses such as $\backslash$The frequency of oscillation of a sinusoidal signal had value ! when the data were taken," or $\backslash$Model x is a better description of the data than model y" make perfect sense. Problems of the first type are generally thought of as parameter estimation problems, while problems of the second type are thought of as model selection problems. However, in probability theory there is no essential distinction between these two types of problems. They are both solved by application of the sum and product rules of probability theory. Model selection problems are conceptually more difficult, because the models may have different functional forms. Consequently, conceptual difficulties enter the problem that are not present in parameter estimation. This paper is a tutorial on model selection. The conceptual problems that arise in model selection will be illustrated in such a way as to automatically avoid any difficulties. A simple example is worked in detail. This example,(radar target identification) illustrates all of the points of principle that must be faced in more complex model selection problems, including how to handle nuisance parameters, uninformative prior probabilities, and incomplete sets of models.},
author = {Bretthorst, G Larry},
journal = {Maximum Entropy and Bayesian Methods},
keywords = {Bayes},
pages = {1--42},
title = {{An Introduction to model selection using probability theory as logic}},
year = {1996}
}
@article{Dalvi2009,
abstract = {A wide range of applications have recently emerged that need to manage large, imprecise data sets. The reasons for imprecision in data are as diverse as the applications them- selves: in sensor and RFID data, imprecision is due to mea- surement errors [28,66]; in information extraction, impreci- sion comes from the inherent ambiguity in natural-language text [32,40]; and in business intelligence, imprecision is used to reduce the cost of data cleaning [12]. In some applications, such as privacy, it is a requirement that the data be less pre- cise. For example, imprecision is purposely inserted to hide sensitive attributes of individuals so that the data may be published [29,55,62]. Imprecise data has no place in tradi- tional, precise database applications like payroll and inven- tory, and so, current database management systems are not prepared to deal with it. In contrast, these newly emerging applications offer value precisely because they query, search, and aggregate large volumes of imprecise data to find the“di- amonds in the dirt”. This wide-variety of applications points to the need for generic tools to manage imprecise data. In this paper, we survey the state of the art techniques to han- dle imprecise data which models imprecision as probabilistic data [4,8,11,14,21,28,45,51,71]. A probabilistic database management system, or Prob- DMS, is a system that stores large volumes of probabilis- tic data and supports complex queries. A ProbDMS may also need to perform some additional tasks, such as updates or recovery, but these do not differ from those in conven- tional database management systems and will not be dis- cussed here. The major challenge in a ProbDMS is that it needs both to scale to large data volumes, a core com- petence of database management systems, and to do prob- abilistic inference, which is a problem studied in AI. While many scalable data management systems exists, probabilis- tic inference is in general a hard problem [68], and current systems do not scale to the same extent as datamanagement systems do. To address this challenge, researchers have fo- cused on the specific nature of relational probabilistic data, and exploited the special form of probabilistic inference that occurs during query evaluation. A number of such results have emerged recently: lineage-based representations [11], safe plans [18], algorithms for top-k queries [63,82], and rep- resentations of views over probabilistic data [65,67]. What is common to all these results is that they apply and extend well known concepts that are fundamental to data manage- ment, such as the separation of query and data when analyz- ing complexity [75], incomplete databases [44], the threshold algorithm [31], and the use of materialized views to answer queries [42, 74]. In this paper, we briefly survey the key concepts in probabilistic database systems, and explain the intellectual roots of these concepts in data management.},
author = {Dalvi, Nilesh and R{\'{e}}, Christopher and Suciu, Dan},
doi = {10.1145/1538788.1538810},
issn = {00010782},
journal = {Communications of the ACM},
number = {7},
pages = {86--94},
title = {{Probabilistic databases: diamonds in the dirt}},
url = {http://portal.acm.org/citation.cfm?doid=1538788.1538810{\%}5Cnhttp://dl.acm.org/citation.cfm?id=1538810},
volume = {52},
year = {2009}
}
@article{Deshpande,
author = {Deshpande, Amol},
keywords = {attribute-level uncertainty,cleaning of data,complexity class,conditional table,graphical model,inference algorithms,model counting problem,p,pos-,probabilistic conditional table},
title = {{Probabilistic Databases}}
}
@article{Papadimitriou2014,
author = {Papadimitriou, Costas},
file = {:home/nmlemus/Documents/Mendeley Desktop/Papadimitriou - 2014 - Bayesian Uncertainty Quantification and Propagation in Structural Dynamics.pdf:pdf},
isbn = {9789727521654},
journal = {Proceedings of the 9th International Conference on Structural Dynamics, EURODYN 2014},
keywords = {component mode synthesis,hpc,laplace asymptotics,large-order models,mcmc,surrogate models},
number = {July},
pages = {111--124},
title = {{Bayesian Uncertainty Quantification and Propagation in Structural Dynamics}},
year = {2014}
}
@book{Banks2014,
author = {Banks, HT and Hu, S and Thompson, WC},
booktitle = {CRC Press},
isbn = {9781482206432},
title = {{Modeling and Inverse Problems in the Presence of Uncertainty}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=jZA-AwAAQBAJ{\&}oi=fnd{\&}pg=PP1{\&}dq=Modeling+and+Inverse+Problems+in+the+Presence+of+Uncertainty{\&}ots=HsI{\_}aNVlHR{\&}sig=4pbvydoz4JeqqEXXEYEqfl7uTvY},
year = {2014}
}
@article{Borgonovo2007,
abstract = {Uncertainty in parameters is present in many risk assessment problems and leads to uncertainty in model predictions. In this work, we introduce a global sensitivity indicator which looks at the influence of input uncertainty on the entire output distribution without reference to a specific moment of the output (moment independence) and which can be defined also in the presence of correlations among the parameters. We discuss its mathematical properties and highlight the differences between the present indicator, variance-based uncertainty importance measures and a moment independent sensitivity indicator previously introduced in the literature. Numerical results are discussed with application to the probabilistic risk assessment model on which Iman [A matrix-based approach to uncertainty and sensitivity analysis for fault trees. Risk Anal 1987;7(1):22-33] first introduced uncertainty importance measures. {\textcopyright} 2006 Elsevier Ltd. All rights reserved.},
annote = {They introduce a new global SA parameter to estimate the influence of any parameter in the output. They make a ranking of the influence of the parameters in the output. The method is independent of the correlations of the input parameters.},
author = {Borgonovo, E.},
doi = {10.1016/j.ress.2006.04.015},
file = {:home/nmlemus/Documents/Mendeley Desktop/Borgonovo - 2007 - A new uncertainty importance measure.pdf:pdf},
isbn = {0951-8320},
issn = {09518320},
journal = {Reliability Engineering and System Safety},
keywords = {Global sensitivity analysis,Importance measures,Probabilistic risk assessment,Uncertainty analysis,Uncertainty importance measures,moment independence},
mendeley-tags = {moment independence},
number = {6},
pages = {771--784},
title = {{A new uncertainty importance measure}},
volume = {92},
year = {2007}
}
@article{Kidane2012,
abstract = {This work is concerned with establishing the feasibility of a data-on-demand (DoD) uncertainty quantification (UQ) protocol based on concentration-of-measure inequalities. Specific aims are to establish the feasibility of the protocol and its basic properties, including the tightness of the predictions afforded by the protocol. The assessment is based on an application to terminal ballistics and a specific system configuration consisting of 6061-T6 aluminum plates struck by spherical S-2 tool steel projectiles at ballistic impact speeds. The systems inputs are the plate thickness and impact velocity and the perforation area is chosen as the sole performance measure of the system. The objective of the UQ analysis is to certify the lethality of the projectile, i.e., that the projectile perforates the plate with high probability over a prespecified range of impact velocities and plate thicknesses. The net outcome of the UQ analysis is an M/U ratio, or confidence factor, of 2.93, indicative of a small probability of no perforation of the plate over its entire operating range. The high-confidence ({\textgreater}99.9{\%}) in the successful operation of the system afforded the analysis and the small number of tests (40) required for the determination of the modeling-error diameter, establishes the feasibility of the DoD UQ protocol as a rigorous yet practical approach for model-based certification of complex systems. ?? 2011 Elsevier Ltd. All rights reserved.},
author = {Kidane, A. and Lashgari, A. and Li, B. and McKerns, M. and Ortiz, M. and Owhadi, H. and Ravichandran, G. and Stalzer, M. and Sullivan, T. J.},
doi = {10.1016/j.jmps.2011.12.001},
isbn = {0022-5096},
issn = {00225096},
journal = {Journal of the Mechanics and Physics of Solids},
keywords = {Certification,Concentration of measure,Terminal ballistics,Uncertainty quantification},
number = {5},
pages = {983--1001},
title = {{Rigorous model-based uncertainty quantification with application to terminal ballistics, part I: Systems with controllable inputs and small scatter}},
volume = {60},
year = {2012}
}
@article{Helton2009,
abstract = {In 2001, the National Nuclear Security Administration of the U.S. Department of Energy in conjunction with the national security laboratories (i.e, Los Alamos National Laboratory, Lawrence Livermore National Laboratory and Sandia National Laboratories) initiated development of a process designated Quantification of Margins and Uncer- tainty (QMU) for the use of risk assessment methodologies in the certification of the reliability and safety of the nation's nuclear weapons stockpile. This presentation discusses and illustrates the conceptual and computational basis of QMU in analyses that use computational models to predict the behavior of complex systems. Topics consid- ered include (i) the role of aleatory and epistemic uncertainty in QMU, (ii) the representation of uncertainty with probability, (iii) the probabilistic representation of uncertainty in QMU analyses involving only epistemic uncer- tainty, (iv) the probabilistic representation of uncertainty in QMU analyses involving aleatory and epistemic uncer- tainty, (v) procedures for sampling-based uncertainty and sensitivity analysis, (vi) the representation of uncertainty with alternatives to probability such as interval analysis, possibility theory and evidence theory, (vii) the representa- tion of uncertainty with alternatives to probability in QMU analyses involving only epistemic uncertainty, and (viii) the representation of uncertainty with alternatives to probability in QMU analyses involving aleatory and epistemic uncertainty. Concepts and computational procedures are illustrated with both notional examples and examples from reactor safety and radioactive waste disposal.},
annote = {Explica bastante bien todo el proceso, solo que habla de insertezas parametricas basicamente. Habla de las formas alternaticas de cuantificar la inserteza, como teoria de las posibilidades, etc.
Es algo viejo por lo que hay que buscar referencias mas recientes.},
author = {Helton, JC},
file = {:home/nmlemus/Documents/Mendeley Desktop/Helton - 2009 - Conceptual and computational basis for the quantification of margins and uncertainty.pdf:pdf},
keywords = {Aleatory uncertainty,Epistemic uncertainty,Performance assessment,Quantification of margins and uncertainty,Risk assessment,Sensitivity analysis,Uncertainty analysis},
number = {June},
title = {{Conceptual and computational basis for the quantification of margins and uncertainty.}},
url = {http://www.osti.gov/energycitations/product.biblio.jsp?osti{\_}id=958189},
year = {2009}
}
@article{Weber2011,
author = {Wood-Schultz, David H. Sharp {\&} Merri M.},
journal = {Los Alamos Science},
number = {1},
pages = {55--82},
title = {{QMU and Nuclear Weapons Certification-What's under the Hood?}},
volume = {44},
year = {2011}
}
@article{Ob2000,
author = {Oberkampf, W L and Deland, Sharon M and Rutherford, Brian M and Diegert, Kathleen V and Alvin, Kenneth F},
journal = {Reliability Engineering and System Safety},
number = {April},
pages = {333--357},
title = {{Estimation of total uncertainty in modeling and simulation}},
volume = {75},
year = {2002}
}
@article{Leader2005,
abstract = {JASON.Quantificationsofmarginsanduncertainties(QMU).JSR-04-3330. McLean, VA:TheMitreCorporation;2005.},
author = {Eardley},
isbn = {9780309128537},
journal = {JASON -The Mitre Corporation JASON report JSR-04-330},
title = {{Quantifications of Margins and Uncertainties}},
year = {2005}
}
@techreport{Pilch2006,
author = {Pilch, Martin and Trucano, T.G.},
booktitle = {Sandia Report},
number = {SAND2006-5001},
title = {{Ideas Underlying Quantification of Margins and Uncertainties (QMU): A White Paper}},
url = {http://www.stanford.edu/group/uq/docs/qmu{\_}ideas.pdf},
year = {2006}
}
@article{Kennedy2001a,
author = {Kennedy, Marc C. and O'Hagan, Anthony},
doi = {10.1111/1467-9868.00294},
file = {:home/nmlemus/Documents/Mendeley Desktop/1467-9868.00294.pdf:pdf},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {Calibration,Computer experiments,Deterministic models,Gaussian process,Interpolation,Model inadequacy,Sensitivity analysis,Uncertainty analysis},
month = {aug},
number = {3},
pages = {425--464},
publisher = {Blackwell Publishers Ltd.},
title = {{Bayesian calibration of computer models}},
url = {http://doi.wiley.com/10.1111/1467-9868.00294},
volume = {63},
year = {2001}
}
@article{Lee2009,
abstract = {A wide variety of uncertainty propagation methods exist in literature; however, there is a lack of good understanding of their relative merits. In this paper, a comparative study on the performances of several representative uncertainty propagation methods, including a few newly developed methods that have received growing attention, is performed. The full factorial numerical integration, the univariate dimension reduction method, and the polynomial chaos expansion method are implemented and applied to several test problems. They are tested under different settings of the performance nonlinearity, distribution types of input random variables, and the magnitude of input uncertainty. The performances of those methods are compared in moment estimation, tail probability calculation, and the probability density function construction, corresponding to a wide variety of scenarios of design under uncertainty, such as robust design, and reliability-based design optimization. The insights gained are expected to direct designers for choosing the most applicable uncertainty propagation technique in design under uncertainty.},
author = {Lee, S. H. and Chen, W.},
doi = {10.1007/s00158-008-0234-7},
isbn = {1615-147X},
issn = {1615147X},
journal = {Structural and Multidisciplinary Optimization},
keywords = {Comparative study,Design under uncertainty,Dimension reduction method,Full factorial numerical integration,Polynomial chaos expansion,Uncertainty propagation},
number = {3},
pages = {239--253},
title = {{A comparative study of uncertainty propagation methods for black-box-type problems}},
volume = {37},
year = {2009}
}
@article{Arnaut2008,
author = {Arnaut, L. R.},
issn = {1754-2995},
journal = {NPL Technical Report TQE 2, 2nd. ed., sec. 4.1.2.2},
number = {2},
title = {{Measurement uncertainty in reverberation chambers - I. Sample statistics}},
volume = {TQE},
year = {2008}
}
@article{Shirangi2016,
abstract = {Parameterization based on truncated singular value decomposition (TSVD) of the dimensionless sensitivity matrix has been shown to be an efficient approach for history matching. With TSVD parameterization, the search direction is computed as a linear combination of a few principal right singular vectors. As the sensitivity matrix is not explicitly computed, this parameterization is appropriate for large-scale history-matching problems. Moreover, previous work presented theoretical evidence that TSVD of the dimensionless sensitivity matrix provides the optimal parameterization in terms of uncertainty reduction. TSVD has been used in the randomized maximum likelihood (RML) framework to generate multiple conditional realizations of reservoir models. In this work, we investigate the effect of TSVD in the search direction obtained by the application of the Gauss–Newton and the Levenberg–Marquardt (LM) methods. In particular, we show that the TSVD-based LM algorithm converges to appropriate estimates because it gradually resolves the important features of the true model. We also introduce an improved implementation of a TSVD-based LM algorithm for generating multiple realizations of reservoir models conditioned to production data. Our experiments indicate that the computational cost of the new implementation is on the order of 2/3 of the cost of the previous implementation.},
author = {Shirangi, Mehrdad G. and Emerick, Alexandre A.},
doi = {10.1016/j.petrol.2016.02.026},
issn = {09204105},
journal = {Journal of Petroleum Science and Engineering},
pages = {258--271},
title = {{An improved TSVD-based Levenberg-Marquardt algorithm for history matching and comparison with Gauss-Newton}},
volume = {143},
year = {2016}
}
@article{GharibShirangi2014,
abstract = {For large-scale history matching problems, applying the Gauss–Newton (GN) or the Levenberg–Marquardt (LM) algorithm is computationally expensive. However, these algorithms can be efficiently applied with parameterization based on a truncated singular value decomposition (SVD) of a dimensionless sensitivity matrix, where a truncated SVD is computed by using the Lanczos method. The SVD parameterization algorithm has been previously combined with randomized maximum likelihood (RML) to simultaneously generate multiple realizations of the reservoir model. The resulting algorithm, called SVD-EnRML, has been applied for simulation of permeability fields of 2D synthetic reservoirs. In this work, the SVD-EnRML algorithm is extended for the simulation of both porosity and permeability fields of 3D reservoirs. In the proposed extension, a dimensionless sensitivity matrix is defined for each set of correlated model parameters. A limitation of the original algorithm is due to the fact that a square root of the covariance matrix is required as a transformation from the original space to a dimensionless space. In this work, this limitation is resolved by introducing ensemble-based regularization based on utilizing an ensemble of unconditional realizations of the reservoir model. Although the proposed extension fits well within the original algorithm, a modified SVD-EnRML algorithm is introduced to mainly improve the computational efficiency. Computational results, composed of two different examples, show that the algorithm can be efficiently applied for the simulation of rock property fields and performance predictions of 3D reservoirs.},
author = {{Gharib Shirangi}, Mehrdad},
doi = {10.1016/j.petrol.2013.11.025},
issn = {09204105},
journal = {Journal of Petroleum Science and Engineering},
pages = {54--71},
title = {{History matching production data and uncertainty assessment with an efficient TSVD parameterization algorithm}},
volume = {113},
year = {2014}
}
@article{Kennedy2001,
author = {Kennedy, Marc C. and O'Hagan, Anthony},
doi = {10.1111/1467-9868.00294},
file = {:home/nmlemus/Documents/Mendeley Desktop/1467-9868.00294.pdf:pdf},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {Calibration,Computer experiments,Deterministic models,Gaussian process,Interpolation,Model inadequacy,Sensitivity analysis,Uncertainty analysis},
month = {aug},
number = {3},
pages = {425--464},
publisher = {Blackwell Publishers Ltd.},
title = {{Bayesian calibration of computer models}},
url = {http://doi.wiley.com/10.1111/1467-9868.00294},
volume = {63},
year = {2001}
}
@article{Williamson2015,
author = {Williamson, D.},
doi = {10.1002/env.2335},
file = {:home/nmlemus/Documents/Mendeley Desktop/Williamson - 2015 - Exploratory ensemble designs for environmental models using k-extended Latin Hypercubes.pdf:pdf},
issn = {11804009},
journal = {Environmetrics},
month = {jun},
number = {4},
pages = {268--283},
title = {{Exploratory ensemble designs for environmental models using k-extended Latin Hypercubes}},
url = {http://doi.wiley.com/10.1002/env.2335},
volume = {26},
year = {2015}
}
@article{Williamson2014a,
abstract = {We develop Bayesian dynamic linear model Gaussian processes for emulation of time series output for computer models that may exhibit chaotic behavior, but where this behavior retains some underlying structure. The statistical technology is particularly suited to emulating the time series output of large climate models that exhibit this feature and where we want samples from the posterior of the emulator to evolve in the same way as dynamic processes in the computer model do. The methodology combines key features of good uncertainty quantification (UQ) methods such as using complex mean functions to capture large-scale signals within parameter space, with dynamic linear models in a way that allows UQ to borrow strength from the Bayesian time series literature. We present an MCMC algorithm for sampling from the posterior of the emulator parameters when the roughness lengths of the Gaussian process are unknown. We discuss an interpretation of the results of this algorithm that allows us to use MCMC to fix th...},
author = {Williamson, Daniel and Blaker, Adam T.},
file = {:home/nmlemus/Documents/Mendeley Desktop/Williamson, Blaker - 2014 - Evolving Bayesian Emulators for Structured Chaotic Time Series, with Application to Large Climate Models.pdf:pdf},
journal = {SIAM/ASA Journal on Uncertainty Quantification},
keywords = {37N10,60G15,60Gxx,Bayesian analysis,climate models,dynamic emulation,uncertainty quantification},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Evolving Bayesian Emulators for Structured Chaotic Time Series, with Application to Large Climate Models}},
year = {2014}
}
@article{Williamson2015a,
author = {Williamson, Daniel and Blaker, Adam T. and Hampton, Charlotte and Salter, James},
doi = {10.1007/s00382-014-2378-z},
file = {:home/nmlemus/Documents/Mendeley Desktop/Williamson et al. - 2015 - Identifying and removing structural biases in climate models with history matching.pdf:pdf},
issn = {0930-7575},
journal = {Climate Dynamics},
month = {sep},
number = {5-6},
pages = {1299--1324},
title = {{Identifying and removing structural biases in climate models with history matching}},
url = {http://link.springer.com/10.1007/s00382-014-2378-z},
volume = {45},
year = {2015}
}
@incollection{Matthies2007,
address = {Dordrecht},
author = {Matthies, Hermann G.},
booktitle = {Extreme Man-Made and Natural Hazards in Dynamics of Structures},
doi = {10.1007/978-1-4020-5656-7_4},
pages = {105--135},
publisher = {Springer Netherlands},
title = {{QUANTIFYING UNCERTAINTY: MODERN COMPUTATIONAL REPRESENTATION OF PROBABILITY AND APPLICATIONS}},
url = {http://link.springer.com/10.1007/978-1-4020-5656-7{\_}4},
year = {2007}
}
@book{Marino2009,
abstract = {Accuracy of results from mathematical and computer models of biological systems is often complicated by the presence of uncertainties in experimental data that are used to estimate parameter values. Current mathematical modeling approaches typically use either single-parameter or local sensitivity analyses. However, these methods do not accurately assess uncertainty and sensitivity in the system as, by default they hold all other parameters fixed at baseline values. Using techniques described within we demonstrate how a multi-dimensional parameter space can be studied globally so all uncertainties can be identified. Further, uncertainty and sensitivity analysis techniques can help to identify and ultimately control uncertainties. In this work we develop methods for applying existing analytical tools to perform analyses on a variety of mathematical and computer models. We compare two specific types of global sensitivity analysis indexes that have proven to be among the most robust and efficient. Through familiar and new examples of mathematical and computer models, we provide a complete methodology for performing these analyses, both in deterministic and stochastic settings, and propose novel techniques to handle problems encountered during this type of analyses.},
author = {Marino, Simeone and Hogue, Ian B and Ray, Christian J and Kirschner, Denise E},
booktitle = {Journal of Theoretical Biology},
doi = {10.1016/j.jtbi.2008.04.011.A},
file = {:home/nmlemus/Documents/Mendeley Desktop/Marino et al. - 2009 - A Methodology for Performing Global Ucertainty and Sensitivity Analysis in Systems Biology.pdf:pdf},
isbn = {7346477723},
issn = {1095-8541},
keywords = {abm,agent-based model,aleatory uncertainty,amplitude sensitivity test,efast,epistemic uncertainty,extended fourier,latin hypercube sampling,lhs,methods,monte carlo,partial rank correlation coefficient,prcc,sensitivity index},
number = {1},
pages = {178--196},
pmid = {18572196},
title = {{A Methodology for Performing Global Ucertainty and Sensitivity Analysis in Systems Biology}},
volume = {254},
year = {2009}
}
@article{Do2015,
author = {Do, Overno and Do, Stado and De, I O and De, Ecretaria and De, Stado and De, Entro and Em, Rofissional},
file = {:home/nmlemus/Documents/Mendeley Desktop/Do et al. - 2015 - Proposta para um ambiente de gerenciamento de dados de predi{\c{c}}{\~{a}}o.pdf:pdf},
title = {{Proposta para um ambiente de gerenciamento de dados de predi{\c{c}}{\~{a}}o}},
year = {2015}
}
@article{Ghanem2013,
author = {Ghanem, Roger},
pages = {1--42},
title = {{Uncertainty Quantification}},
year = {2013}
}
@article{Hoare2008,
abstract = {SaSAT (Sampling and Sensitivity Analysis Tools) is a user-friendly software package for applying uncertainty and sensitivity analyses to mathematical and computational models of arbitrary complexity and context. The toolbox is built in Matlab, a numerical mathematical software package, and utilises algorithms contained in the Matlab Statistics Toolbox. However, Matlab is not required to use SaSAT as the software package is provided as an executable file with all the necessary supplementary files. The SaSAT package is also designed to work seamlessly with Microsoft Excel but no functionality is forfeited if that software is not available. A comprehensive suite of tools is provided to enable the following tasks to be easily performed: efficient and equitable sampling of parameter space by various methodologies; calculation of correlation coefficients; regression analysis; factor prioritisation; and graphical output of results, including response surfaces, tornado plots, and scatterplots. Use of SaSAT is exemplified by application to a simple epidemic model. To our knowledge, a number of the methods available in SaSAT for performing sensitivity analyses have not previously been used in epidemiological modelling and their usefulness in this context is demonstrated.},
author = {Hoare, Alexander and Regan, David G and Wilson, David P},
doi = {10.1186/1742-4682-5-4},
isbn = {1742-4682 (Electronic)$\backslash$n1742-4682 (Linking)},
issn = {1742-4682},
journal = {Theoretical biology {\&} medical modelling},
pages = {4},
pmid = {18304361},
title = {{Sampling and sensitivity analyses tools (SaSAT) for computational modelling.}},
volume = {5},
year = {2008}
}
@article{Hartmann2008,
abstract = {For the simulation of structural collapse using controlled explosives, the quantification of structural parameters has to be accomplished on the basis of only few data, which may additionally be characterized by vagueness, e.g. due to uncertain measurements or changing reproduction conditions. To ensure a reliable prediction of a structural collapse, next to a close to reality simulation of the complex dynamic process, this uncertainty has to be taken into account. With regard to very high computation associated with the simulation of collapses of real world structures based on conventional finite element models, this paper addresses an efficient approach for the simulation of structural collapse based on multibody models, that simultaneously allow for the investigation of uncertainty. ?? 2008 Elsevier Ltd. All rights reserved.},
author = {Hartmann, Dietrich and Breidt, Michael and Nguyen, van Vinh and Stangenberg, Friedhelm and H??hler, Sebastian and Schweizerhof, Karl and Mattern, Steffen and Blankenhorn, Gunther and M??ller, Bernd and Liebscher, Martin},
doi = {10.1016/j.compstruc.2008.03.004},
file = {:home/nmlemus/Documents/Mendeley Desktop/Hartmann et al. - 2008 - Structural collapse simulation under consideration of uncertainty-Fundamental concept and results.pdf:pdf},
issn = {00457949},
journal = {Computers and Structures},
keywords = {Demolition,Explosives,Fuzziness,Multi-level simulation,Multibody dynamics},
number = {21-22},
pages = {2064--2078},
publisher = {Elsevier Ltd},
title = {{Structural collapse simulation under consideration of uncertainty-Fundamental concept and results}},
url = {http://dx.doi.org/10.1016/j.compstruc.2008.03.004},
volume = {86},
year = {2008}
}
@article{Tobergte2013,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Tobergte, David R. and Curtis, Shirley},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Journal of Chemical Information and Modeling},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{Workshop on Quantification, Communication, and Interpretation of Uncertainty in Simulation and Data Science}},
volume = {53},
year = {2013}
}
@article{Kiureghian2009,
abstract = {The sources and characters of uncertainties in engineering modeling for risk and reliability analyses are discussed. While many sources of uncertainty may exist, they are generally categorized as either aleatory or epistemic. Uncertainties are characterized as epistemic, if the modeler sees a possibility to reduce them by gathering more data or by refining models. Uncertainties are categorized as aleatory if the modeler does not foresee the possibility of reducing them. From a pragmatic standpoint, it is useful to thus categorize the uncertainties within a model, since it then becomes clear as to which uncertainties have the potential of being reduced. More importantly, epistemic uncertainties may introduce dependence among random events, which may not be properly noted if the character of uncertainties is not correctly modeled. Influences of the two types of uncertainties in reliability assessment, codified design, performance-based engineering and risk-based decision-making are discussed. Two simple examples demonstrate the influence of statistical dependence arising from epistemic uncertainties on systems and time-variant reliability problems. ?? 2008 Elsevier Ltd. All rights reserved.},
author = {Kiureghian, Armen Der and Ditlevsen, Ove},
doi = {10.1016/j.strusafe.2008.06.020},
file = {:home/nmlemus/Documents/Mendeley Desktop/Kiureghian, Ditlevsen - 2009 - Aleatory or epistemic Does it matter.pdf:pdf},
isbn = {0167-4730},
issn = {01674730},
journal = {Structural Safety},
keywords = {Aleatory,Epistemic,Ergodicity,Parameter uncertainty,Predictive models,Probability distribution choice,Statistical dependence,Systems,Time-variant reliability,Uncertainty},
month = {mar},
number = {2},
pages = {105--112},
title = {{Aleatory or epistemic? Does it matter?}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167473008000556},
volume = {31},
year = {2009}
}
@article{Oberkampf2002,
abstract = {This article develops a general framework for identifying error and uncertainty in computational simulations that deal with the numerical solution of a set of partial differential equations (PDEs). A comprehensive, new view of the general phases of modeling and simulation is proposed, consisting of the following phases: conceptual modeling of the physical system, mathematical modeling of the conceptual model, discretization and algorithm selection for the mathematical model, computer programming of the discrete model, numerical solution of the computer program model, and representation of the numerical solution. Our view incorporates the modeling and simulation phases that are recognized in the systems engineering and operations research communities, but it adds phases that are specific to the numerical solution of PDEs. In each of these phases, general sources of uncertainty, both aleatory and epistemic, and error are identified. Our general framework is applicable to any numerical discretization procedure for solving ODEs or PDEs. To demonstrate this framework, we describe a system-level example: the flight of an unguided, rocket-boosted, aircraft-launched missile. This example is discussed in detail at each of the six phases of modeling and simulation. Two alternative models of the flight dynamics are considered, along with aleatory uncertainty of the initial mass of the missile and epistemic uncertainty in the thrust of the rocket motor. We also investigate the interaction of modeling uncertainties and numerical integration error in the solution of the ordinary differential equations for the flight dynamics.},
author = {Oberkampf, William L. and DeLand, Sharon M. and Rutherford, Brian M. and Diegert, Kathleen V. and Alvin, Kenneth F.},
doi = {10.1016/S0951-8320(01)00120-X},
isbn = {0951-8320},
issn = {09518320},
journal = {Reliability Engineering {\&} System Safety},
keywords = {aleatory uncertainty,epistemic uncertainty,modeling,nondeterministic features,simulation,stochastic uncertainty,subjective uncertainty},
number = {3},
pages = {333--357},
title = {{Error and uncertainty in modeling and simulation}},
volume = {75},
year = {2002}
}
@book{Loucks2005,
abstract = {The usefulness of any model depends in part on the accuracy and reliability of its output. Yet, because all models are imperfect abstractions of reality, and because precise input data are rarely if ever available, all output values are subject to imprecision. Input data errors and modelling uncertainties are not independent of each other – they can interact in various ways. The end result is imprecision and uncertainty associated with model output. This chapter focuses on ways of identifying, quantifying, and communicating the uncertainties in model outputs.},
author = {Loucks, Daniel P. and van Beek, Eelco and Stedinger, Jery R. and Dijkman, Jozef P.M. and Villars, Monique T.},
booktitle = {Water Resources Systems Planning and Management: An Introduction to Methods, Models and Applications},
doi = {ISBN: 92-3-103998-9},
isbn = {9231039989},
pages = {254--290},
title = {{9 Model Sensitivity and Uncertainty Analysis}},
url = {https://www.utwente.nl/ctw/wem/education/afstuderen/Loucks{\_}VanBeek/09{\_}chapter09.pdf},
year = {2005}
}
